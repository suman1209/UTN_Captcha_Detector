{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f8e11b-2c1f-4bf4-96a4-09d0d14c768e",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4afb6613-bac6-4c1a-a10a-dd2e0302aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../datasets/utn_dataset_curated/part2/train\"\n",
    "val_path = \"../datasets/utn_dataset_curated/part2/val\"\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1079c556-83d1-43f6-a0e0-f403730021f6",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "This is just a simple dataset and dataloader, will be replaced by the proper dataloader later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff53952-c722-4268-b4bb-fddb9c76a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import VisionDataset\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as T\n",
    "class CaptchaDatasetUTN(VisionDataset):\n",
    "    def __init__(self, data_path: str,\n",
    "                 img_transform=None,\n",
    "                 target_transform=None,):\n",
    "        super(CaptchaDatasetUTN).__init__()\n",
    "        self.img_transform = img_transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data_path = data_path\n",
    "        self.img_paths = self._get_paths(data_path, suffix=\"/images/\")\n",
    "        self.ann_paths = self._get_paths(data_path, suffix=\"/labels/\")\n",
    "        # we hardcode the following values for the UTN dataset\n",
    "        self.img_height, self.img_width = 160, 640\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_paths(data_path, suffix):\n",
    "        img_path =  data_path + suffix\n",
    "        assert os.path.exists(img_path), f\"{img_path} does not exist!\"\n",
    "        file_list = list(Path(img_path).glob('**/*'))\n",
    "        file_names = [p for p in file_list if p.is_file()]\n",
    "        file_names = sorted(file_names, key=lambda x: int(str(x).split(\"/\")[-1].split(\".\")[0]))\n",
    "        return file_names\n",
    "\n",
    "    def __getitem__(self, idx: int) -> list[tuple[torch.Tensor, torch.Tensor]]:\n",
    "        if idx >= len(self) or idx < 0:\n",
    "            # needed for iterator stop condition\n",
    "            raise IndexError(f\"{self.data_path} has {len(self)} files!\")\n",
    "        factor = 4\n",
    "        img_path = str(self.img_paths[idx])\n",
    "        mu, std = (137.71044921875, 60.335174560546875)\n",
    "        # for some reason, the provided dataset has four channels\n",
    "        img = read_image(f'{img_path}')[:3, :, :].to(dtype=torch.float32)\n",
    "        img = T.Normalize((-mu), (1/std))(img)\n",
    "        img = T.Grayscale()(img)\n",
    "        h, w = img.shape[-2: ]\n",
    "        size=(h//factor, w//factor)\n",
    "        img = T.Resize(size, antialias=False)(img)\n",
    "        img = img\n",
    "        \n",
    "        assert img.size(0) == 1, f\"invalid channels in gray image: {img.shape}\"\n",
    "        if self.img_transform:\n",
    "            img = self.img_transform(img)\n",
    "        assert isinstance(img, torch.Tensor), f\"got transformed img of type{type(img)}!\"\n",
    "        with open(f\"{self.ann_paths[idx]}\", \"r\") as fo:\n",
    "            ann = []\n",
    "            for line in fo:\n",
    "                ann.append(line.split())\n",
    "        count = 0\n",
    "        for obj in ann:\n",
    "            count += 1\n",
    "        # transform bounding boxes\n",
    "        if self.target_transform is not None:\n",
    "            bboxes = self.target_transform(bboxes)\n",
    "        return img, count\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)\n",
    "        \n",
    "    def show(self, imgs, title=\"\"):\n",
    "        if not isinstance(imgs, list):\n",
    "            imgs = [imgs]\n",
    "        fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "        plt.title(title)\n",
    "        for i, img in enumerate(imgs):\n",
    "            # img = img.detach()\n",
    "            img = F.to_pil_image(img)\n",
    "            axs[0, i].imshow(np.asarray(img), cmap='gray', vmin=0, vmax=255)\n",
    "            axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "753d3c59-6135-4f09-9193-1b863b4a3a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_img_batch.shape = torch.Size([2, 1, 40, 160])\n",
      "sample_count_batch[img_num] = tensor(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset\n",
    "train_set = CaptchaDatasetUTN(data_path=train_path, img_transform=None, target_transform=None)\n",
    "val_set = CaptchaDatasetUTN(data_path=val_path, img_transform=None, target_transform=None)\n",
    "\n",
    "#dataloader\n",
    "train_loader = data.DataLoader(train_set, batch_size=batch_size)\n",
    "val_loader = data.DataLoader(val_set, batch_size=batch_size)\n",
    "sample_img_batch,  sample_count_batch = next(iter(train_loader))\n",
    "print(f\"{sample_img_batch.shape = }\")\n",
    "img_num = 0\n",
    "train_set.show(sample_img_batch[img_num])\n",
    "print(f\"{sample_count_batch[img_num] = }\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b118afac-3324-4b1f-93a2-253b85796756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CountBackbone(nn.Module):\n",
    "    def __init__(self, input_channels=1, nr_filters=16, kernel_size=3):\n",
    "        super(CountBackbone, self).__init__()\n",
    "        # single conv layer for now\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, \n",
    "                        out_channels=nr_filters, \n",
    "                        kernel_size=kernel_size, \n",
    "                        stride=1, \n",
    "                        padding=1)\n",
    "\n",
    "        # fully connected layer ( for regression ) outputs character count\n",
    "        self.fc = nn.Linear(nr_filters, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply convolution\n",
    "        x = self.conv1(x) \n",
    "        # activation function\n",
    "        x = F.relu(x)\n",
    "        # global avg pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        # flatten for the fuly  connected layer\n",
    "        x = torch.flatten(x, 1)\n",
    "        # output \n",
    "        x = self.fc(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d27646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountBackbone(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CountBackbone()\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
