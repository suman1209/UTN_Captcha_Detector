{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f8e11b-2c1f-4bf4-96a4-09d0d14c768e",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4afb6613-bac6-4c1a-a10a-dd2e0302aed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "## other standard packages\n",
    "import os\n",
    "import sys\n",
    "## Env variables and preparation stuffs\n",
    "sys.path.insert(0, \"../\")\n",
    "from src_code.task_utils.config_parser import ConfigParser\n",
    "configs_dict = {\n",
    "    \"data_configs\": {\n",
    "        \"train_path\": \"../datasets/utn_dataset_curated/part2/train\",\n",
    "        \"val_path\": \"../datasets/utn_dataset_curated/part2/val\",\n",
    "        \"test_path\": \"../datasets/utn_dataset_curated/part2/test\",\n",
    "        \"preprocessing_related\": {\n",
    "            \"mean\": 0.5,  # for raw_image normalisation\n",
    "            \"std\": 0.5,  # for raw_image normalisation\n",
    "            \"downscale_factor\": 4,\n",
    "        },\n",
    "        \"dataset_related\": {\n",
    "            \"preprocessed_dir\": \"../datasets/utn_dataset_curated/part2/train/preprocessed\",\n",
    "            \"labels_dir\": \"../datasets/utn_dataset_curated/part2/train/labels\",\n",
    "            \"augment\": True,\n",
    "            \"shuffle\": False,\n",
    "        },\n",
    "        \"augmentation_related\": {\n",
    "            \"flip_prob\": 0.5,\n",
    "            \"scale_range\": (0.8, 1.2),\n",
    "        },\n",
    "    },\n",
    "    \"model_configs\": {\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 32,\n",
    "        \"device\": \"cuda\",  # either \"cpu\" or \"cuda\"\n",
    "        \"checkpoint\": None,\n",
    "        \"backbone\": {\n",
    "            \"name\": \"VGG16\",\n",
    "            \"num_stages\": 6,\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"alpha\": 1,  # loss = alpha*loc_loss + cls_loss\n",
    "            \"pos_box_threshold\": 0.5,  # a default box is marked positive if it has (> pos_box_threshold) IoU score with any of the groundtruth boxes\n",
    "            \"hard_neg_pos\": 3,  # num of negative boxes = hard_neg_pos * num_positive_boxes\n",
    "        },\n",
    "        \"optim\": {\n",
    "            \"name\": \"SGD\",\n",
    "            \"lr\": 0.001,\n",
    "            \"momentum\": 0.9,\n",
    "            \"weight_decay\": 0.0005,\n",
    "        },\n",
    "        \"scheduler\": {\n",
    "            \"name\": \"MultiStepLR\",\n",
    "            \"milestones\": [155, 195],\n",
    "            \"gamma\": 0.1,\n",
    "        },\n",
    "    },\n",
    "    \"task_configs\": {\n",
    "        \"img_height\": 160,  # original image height\n",
    "        \"img_width\": 640,  # original image width\n",
    "        \"debug\": True,  # if True will display a lot of intermediate information for debugging purposes\n",
    "        \"log_expt\": False,  # whether to log the experiment online or not\n",
    "        \"num_classes\": 37,  # A-Z(26), 0-9(10), background(1)\n",
    "        \"min_cls_score\": 0.01,  # if the cls score for a bounding box is less than this, it is considered as background\n",
    "        \"nms_iou_score\": 0.1,  # if the iou between two bounding boxes is less than this, it is suppressed\n",
    "    },\n",
    "}\n",
    "# hyperparameters\n",
    "preprocessed_dir = \"../datasets/utn_dataset_curated/part2/train/preprocessed\"\n",
    "labels_dir = \"../datasets/utn_dataset_curated/part2/train/labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd95d090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config = {'data_configs': {'train_path': '../datasets/utn_dataset_curated/part2/train', 'val_path': '../datasets/utn_dataset_curated/part2/val', 'test_path': '../datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/preprocessed', 'labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0.5, 'scale_range': (0.8, 1.2)}}, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'checkpoint': None, 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}\n",
      "{'config_dict': {'data_configs': {'train_path': '../datasets/utn_dataset_curated/part2/train', 'val_path': '../datasets/utn_dataset_curated/part2/val', 'test_path': '../datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/preprocessed', 'labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0.5, 'scale_range': (0.8, 1.2)}}, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'checkpoint': None, 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}, 'train_path': '../datasets/utn_dataset_curated/part2/train', 'val_path': '../datasets/utn_dataset_curated/part2/val', 'downscale_factor': 4, 'preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/preprocessed', 'labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'augment': True, 'shuffle': False, 'flip_prob': 0.5, 'scale_range': (0.8, 1.2), 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'checkpoint': None, 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'checkpoint': None, 'device': 'cuda', 'print_freq': None, 'batch_size': 32, 'epochs': 1, 'pos_box_threshold': 0.5, 'neg_pos_hard_mining': 3, 'alpha': 1, 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005, 'debug': True, 'num_classes': 37, 'img_height': 160, 'img_width': 640}\n",
      "config.pos_box_threshold = 0.5\n"
     ]
    }
   ],
   "source": [
    "config = ConfigParser(configs_dict).get_parser()\n",
    "print(config.__dict__)  # Check all attributes in config\n",
    "\n",
    "# this object can be used as follows:\n",
    "print(f\"{config.pos_box_threshold = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1079c556-83d1-43f6-a0e0-f403730021f6",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "This is just a simple dataset and dataloader, will be replaced by the proper dataloader later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ff53952-c722-4268-b4bb-fddb9c76a154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "## other standard packages\n",
    "import os\n",
    "import sys\n",
    "## Env variables and preparation stuffs\n",
    "sys.path.insert(0, \"../\")\n",
    "from torchvision.datasets import VisionDataset\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76232dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../', '../', '../', '../', '../', '../', '../', '../', '../', '../', '../', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/var/lit2425/humanize/Computer Vision 2025 Project/captcha_environment/lib/python3.10/site-packages', '/tmp/tmpsu98z9om']\n",
      "Image Shape: torch.Size([1, 40, 160])\n",
      "Bounding Boxes: tensor([[ 0.1430,  0.4818,  0.2001,  0.8585],\n",
      "        [ 0.3307, -0.0112,  0.4611,  0.5103],\n",
      "        [ 0.4181,  0.2645,  0.5456,  0.7704],\n",
      "        [ 0.7742,  0.3091,  0.8777,  0.8153]])\n",
      "Labels: tensor([21,  3, 24,  4])\n",
      "Batch Image Shape: torch.Size([5, 1, 40, 160])\n",
      "Bounding Boxes (First Image): tensor([[ 0.1430,  0.4818,  0.2001,  0.8585],\n",
      "        [ 0.3307, -0.0112,  0.4611,  0.5103],\n",
      "        [ 0.4181,  0.2645,  0.5456,  0.7704],\n",
      "        [ 0.7742,  0.3091,  0.8777,  0.8153]])\n",
      "Labels (First Image): tensor([21,  3, 24,  4])\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)\n",
    "from src_code.data_utils.dataset_utils import CaptchaDataset, get_dataloader\n",
    "from src_code.data_utils.augmentation import Augmentations\n",
    "# if the preprocessed dataset is not available, run create it using src_code/data_utils/preprocessing.py\n",
    "\n",
    "# Create dataset\n",
    "train_set = CaptchaDataset(config)\n",
    "# Load a sample\n",
    "image, bboxes, labels = dataset[0]\n",
    "\n",
    "print(\"Image Shape:\", image.shape)\n",
    "print(\"Bounding Boxes:\", bboxes)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = get_dataloader(dataset, config)\n",
    "\n",
    "# Load a single batch\n",
    "images, bboxes, labels = next(iter(dataloader))\n",
    "\n",
    "# Print batch info\n",
    "print(f\"Batch Image Shape: {images.shape}\")\n",
    "print(f\"Bounding Boxes (First Image): {bboxes[0]}\")\n",
    "print(f\"Labels (First Image): {labels[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "753d3c59-6135-4f09-9193-1b863b4a3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset\n",
    "# train_set = CaptchaDatasetUTN(data_path=config.train_path, img_transform=None, target_transform=None)\n",
    "# val_set = CaptchaDatasetUTN(data_path=config.val_path, img_transform=None, target_transform=None)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11396292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_code.data_utils.dataset_utils import get_dataloader\n",
    "\n",
    "# dataloader using the new function\n",
    "train_loader = get_dataloader(train_set, config)\n",
    "val_loader = get_dataloader(val_set, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d36e5b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, bboxes, labels = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b118afac-3324-4b1f-93a2-253b85796756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class CountBackbone(nn.Module):\n",
    "    def __init__(self, input_channels=1, nr_filters=16, kernel_size=3):\n",
    "        super(CountBackbone, self).__init__()\n",
    "        # single conv layer for now\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, \n",
    "                        out_channels=nr_filters, \n",
    "                        kernel_size=kernel_size, \n",
    "                        stride=1, \n",
    "                        padding=1)\n",
    "\n",
    "        # fully connected layer ( for regression ) outputs character count\n",
    "        self.fc = nn.Linear(nr_filters, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply convolution\n",
    "        x = self.conv1(x) \n",
    "        # activation function\n",
    "        x = F.relu(x)\n",
    "        # global avg pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        # flatten for the fuly  connected layer\n",
    "        x = torch.flatten(x, 1)\n",
    "        # output \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d27646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountBackbone(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CountBackbone()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6369b6d-3b80-411a-9a31-b713b8512826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = 1e-3)\n",
    "\n",
    "    def backbone_train(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for image, bboxes, labels in self.train_loader:\n",
    "            targets = torch.tensor([len(bb) for bb in bboxes])\n",
    "            image, targets = image.to(self.device), targets.to(self.device, dtype=torch.float32).unsqueeze(1)\n",
    "            # forward pass\n",
    "            predictions = self.model(image)\n",
    "            loss = self.loss_function(predictions, targets)\n",
    "            # backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(self.train_loader)\n",
    "        print(f\"Train Loss: {average_loss:}\")\n",
    "\n",
    "\n",
    "    def backbone_validation(self):\n",
    "        self.model.eval() \n",
    "        total_loss = 0\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for image, targets in self.val_loader:\n",
    "                image, targets = image.to(self.device), targets.to(self.device, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = self.model(image)\n",
    "                loss = self.loss_function(predictions, targets)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(self.val_loader)\n",
    "        print(f\"Validation Loss: {average_loss:}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34d95ee6-6cc5-4d4e-beae-c62c54bf3368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "images, bboxes, labels = next(iter(dataloader))\n",
    "\n",
    "for image, bboxes, labels in dataloader:\n",
    "    print(len(bboxes))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e143e426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13.738372802734375\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, train_loader, val_loader)\n",
    "trainer.backbone_train()\n",
    "# trainer.backbone_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7150175-f7cb-42bf-9609-b12345bc26b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
