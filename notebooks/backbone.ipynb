{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f8e11b-2c1f-4bf4-96a4-09d0d14c768e",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4afb6613-bac6-4c1a-a10a-dd2e0302aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "## other standard packages\n",
    "import os\n",
    "import sys\n",
    "## Env variables and preparation stuffs\n",
    "sys.path.insert(0, \"../\")\n",
    "from src_code.task_utils.config_parser import ConfigParser\n",
    "configs_dict = {\n",
    "    \"data_configs\": {\n",
    "        \"train_path\": \"../datasets/utn_dataset_curated/part2/train\",\n",
    "        \"val_path\": \"../datasets/utn_dataset_curated/part2/val\",\n",
    "        \"test_path\": \"../datasets/utn_dataset_curated/part2/test\",\n",
    "        \"preprocessing_related\": {\n",
    "            \"mean\": 0.5,  # for raw_image normalisation\n",
    "            \"std\": 0.5,  # for raw_image normalisation\n",
    "            \"downscale_factor\": 4,\n",
    "        },\n",
    "        \"dataset_related\": {\n",
    "            \"preprocessed_dir\": \"../datasets/utn_dataset_curated/part2/train/preprocessed\",\n",
    "            \"labels_dir\": \"../datasets/utn_dataset_curated/part2/train/labels\",\n",
    "            \"augment\": True,\n",
    "            \"shuffle\": False,\n",
    "        },\n",
    "        \"augmentation_related\": {\n",
    "            \"flip_prob\": 0.5,\n",
    "            \"scale_range\": (0.8, 1.2),\n",
    "        },\n",
    "    },\n",
    "    \"model_configs\": {\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 32,\n",
    "        \"device\": \"cuda\",  # either \"cpu\" or \"cuda\"\n",
    "        \"checkpoint\": None,\n",
    "        \"backbone\": {\n",
    "            \"name\": \"VGG16\",\n",
    "            \"num_stages\": 6,\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"alpha\": 1,  # loss = alpha*loc_loss + cls_loss\n",
    "            \"pos_box_threshold\": 0.5,  # a default box is marked positive if it has (> pos_box_threshold) IoU score with any of the groundtruth boxes\n",
    "            \"hard_neg_pos\": 3,  # num of negative boxes = hard_neg_pos * num_positive_boxes\n",
    "        },\n",
    "        \"optim\": {\n",
    "            \"name\": \"SGD\",\n",
    "            \"lr\": 0.001,\n",
    "            \"momentum\": 0.9,\n",
    "            \"weight_decay\": 0.0005,\n",
    "        },\n",
    "        \"scheduler\": {\n",
    "            \"name\": \"MultiStepLR\",\n",
    "            \"milestones\": [155, 195],\n",
    "            \"gamma\": 0.1,\n",
    "        },\n",
    "    },\n",
    "    \"task_configs\": {\n",
    "        \"img_height\": 160,  # original image height\n",
    "        \"img_width\": 640,  # original image width\n",
    "        \"debug\": True,  # if True will display a lot of intermediate information for debugging purposes\n",
    "        \"log_expt\": False,  # whether to log the experiment online or not\n",
    "        \"num_classes\": 37,  # A-Z(26), 0-9(10), background(1)\n",
    "        \"min_cls_score\": 0.01,  # if the cls score for a bounding box is less than this, it is considered as background\n",
    "        \"nms_iou_score\": 0.1,  # if the iou between two bounding boxes is less than this, it is suppressed\n",
    "    },\n",
    "}\n",
    "# hyperparameters\n",
    "preprocessed_dir = \"../datasets/utn_dataset_curated/part2/train/preprocessed\"\n",
    "labels_dir = \"../datasets/utn_dataset_curated/part2/train/labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf434e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigParser(configs_dict).get_parser()\n",
    "print(config.__dict__)  # Check all attributes in config\n",
    "\n",
    "# this object can be used as follows:\n",
    "print(f\"{config.pos_box_threshold = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1079c556-83d1-43f6-a0e0-f403730021f6",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "This is just a simple dataset and dataloader, will be replaced by the proper dataloader later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ff53952-c722-4268-b4bb-fddb9c76a154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "## other standard packages\n",
    "import os\n",
    "import sys\n",
    "## Env variables and preparation stuffs\n",
    "sys.path.insert(0, \"../\")\n",
    "from torchvision.datasets import VisionDataset\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "753d3c59-6135-4f09-9193-1b863b4a3a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_img_batch.shape = torch.Size([2, 1, 40, 160])\n",
      "sample_count_batch[img_num] = tensor(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(sys.path)\n",
    "from src_code.data_utils.dataset_utils import CaptchaDataset, get_dataloader\n",
    "from src_code.data_utils.augmentation import Augmentations\n",
    "# if the preprocessed dataset is not available, run create it using src_code/data_utils/preprocessing.py\n",
    "\n",
    "# Create dataset\n",
    "train_set = CaptchaDataset(config)\n",
    "# Load a sample\n",
    "image, bboxes, labels = dataset[0]\n",
    "\n",
    "print(\"Image Shape:\", image.shape)\n",
    "print(\"Bounding Boxes:\", bboxes)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = get_dataloader(dataset, config)\n",
    "\n",
    "# Load a single batch\n",
    "images, bboxes, labels = next(iter(dataloader))\n",
    "\n",
    "# Print batch info\n",
    "print(f\"Batch Image Shape: {images.shape}\")\n",
    "print(f\"Bounding Boxes (First Image): {bboxes[0]}\")\n",
    "print(f\"Labels (First Image): {labels[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11396292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_code.data_utils.dataset_utils import get_dataloader\n",
    "\n",
    "# dataloader using the new function\n",
    "train_loader = get_dataloader(train_set, config)\n",
    "val_loader = get_dataloader(val_set, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d36e5b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, bboxes, labels = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b118afac-3324-4b1f-93a2-253b85796756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class CountBackbone(nn.Module):\n",
    "    def __init__(self, input_channels=1, nr_filters=16, kernel_size=3):\n",
    "        super(CountBackbone, self).__init__()\n",
    "        # single conv layer for now\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, \n",
    "                        out_channels=nr_filters, \n",
    "                        kernel_size=kernel_size, \n",
    "                        stride=1, \n",
    "                        padding=1)\n",
    "\n",
    "        # fully connected layer ( for regression ) outputs character count\n",
    "        self.fc = nn.Linear(nr_filters, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply convolution\n",
    "        x = self.conv1(x) \n",
    "        # activation function\n",
    "        x = F.relu(x)\n",
    "        # global avg pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        # flatten for the fuly  connected layer\n",
    "        x = torch.flatten(x, 1)\n",
    "        # output \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d27646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountBackbone(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CountBackbone()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6369b6d-3b80-411a-9a31-b713b8512826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = 1e-3)\n",
    "\n",
    "    def backbone_train(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for image, bboxes, labels in self.train_loader:\n",
    "            targets = torch.tensor([len(bb) for bb in bboxes])\n",
    "            image, targets = image.to(self.device), targets.to(self.device, dtype=torch.float32).unsqueeze(1)\n",
    "            # forward pass\n",
    "            predictions = self.model(image)\n",
    "            loss = self.loss_function(predictions, targets)\n",
    "            # backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(self.train_loader)\n",
    "        print(f\"Train Loss: {average_loss:}\")\n",
    "\n",
    "\n",
    "    def backbone_validation(self):\n",
    "        self.model.eval() \n",
    "        total_loss = 0\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for image, targets in self.val_loader:\n",
    "                image, targets = image.to(self.device), targets.to(self.device, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = self.model(image)\n",
    "                loss = self.loss_function(predictions, targets)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(self.val_loader)\n",
    "        print(f\"Validation Loss: {average_loss:}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e143e426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2306209.5833333335\n",
      "Validation Loss: 1842241.125\n"
     ]
    }
   ],
   "source": [
    "images, bboxes, labels = next(iter(dataloader))\n",
    "\n",
    "for image, bboxes, labels in dataloader:\n",
    "    print(len(bboxes))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7150175-f7cb-42bf-9609-b12345bc26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_loader, val_loader)\n",
    "trainer.backbone_train()\n",
    "# trainer.backbone_validation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
