{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from src_code.task_utils.config_parser import ConfigParser\n",
    "from src_code.data_utils.dataset_utils import CaptchaDataset\n",
    "from src_code.data_utils.preprocessing import get_img_transform, get_rectangle_img_transform\n",
    "from src_code.model_utils.mnist_ssd import SSD, BaseConv, pretty_print_module_list, AuxConv\n",
    "import src_code.model_utils.utils_mnist_ssd as utils_mnist_ssd\n",
    "from src_code.model_utils.mnist_ssd import SSD\n",
    "from torch import nn\n",
    "import yaml\n",
    "from torchvision.utils import draw_bounding_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Config Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config = {'task': 'train', 'data_configs': {'train_path': 'datasets/utn_dataset_curated/part2/train', 'val_path': 'datasets/utn_dataset_curated/part2/val', 'test_path': 'datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'color': False, 'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'train_preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/images', 'val_preprocessed_dir': '../datasets/utn_dataset_curated/part2/val/images', 'test_preprocessed_dir': '../datasets/utn_dataset_curated/part2/test/images', 'train_labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'val_labels_dir': '../datasets/utn_dataset_curated/part2/val/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0.5, 'scale_range': '(0.8, 1.2)', 'zoom_prob': 0.3, 'saturation_prob': 0, 'rotation_prob': 0}}, 'model_configs': {'name': 'ssd_mnist', 'save_checkpoint': True, 'log_gradients': False, 'checkpoint': None, 'print_freq': 500, 'epochs': 10, 'batch_size': 2, 'device': 'cuda', 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 0.25, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005, 'clip_grad': None}, 'scheduler': {'name': 'LinearLR', 'milestones': [10, 20], 'gamma': 0.1, 'start_factor': 0.5, 'total_iter': 4}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the base config\n",
    "base_config_path = \"../configs/configs_common_notebook.yaml\"\n",
    "if not Path(base_config_path).exists():\n",
    "    raise FileNotFoundError(f\"Base config file not found: {base_config_path}\")\n",
    "\n",
    "with open(base_config_path, \"r\") as file:\n",
    "    base_config_dict = yaml.safe_load(file)\n",
    "\n",
    "configs = ConfigParser(base_config_dict).get_parser()\n",
    "\n",
    "# Load the SSD-specific config\n",
    "ssd_config_path = \"../configs/default_ssd_configs.yaml\"\n",
    "if not Path(ssd_config_path).exists():\n",
    "    raise FileNotFoundError(f\"SSD config file not found: {ssd_config_path}\")\n",
    "\n",
    "with open(ssd_config_path, \"r\") as file:\n",
    "    ssd_config_dict = yaml.safe_load(file)\n",
    "\n",
    "configs.update(ssd_config_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CaptchaDataset\n",
      "    Number of datapoints: 5\n",
      "    Root location: ../datasets/utn_dataset_curated/part2/test/images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset = CaptchaDataset(\n",
    "    configs.test_preprocessed_dir,\n",
    "    labels_dir=None,\n",
    "    augment=False,\n",
    "    config=configs,\n",
    "    img_transform=get_rectangle_img_transform(configs)\n",
    ")\n",
    "\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_conv = BaseConv(configs.base_conv_conv_layers, \n",
    "                    configs.base_conv_input_size, chosen_fm=[-2, -1],\n",
    "                    norm=nn.BatchNorm2d, act_fn=nn.ReLU(), spectral=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12780 priors in this model\n",
      "Done initialization\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SSD(\n",
       "  (base_conv): BaseConv(\n",
       "    (module_list): ModuleList(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): ReLU()\n",
       "      (10): Conv(\n",
       "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): ReLU()\n",
       "      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (14): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (16): ReLU()\n",
       "      (17): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (19): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (aux_conv): AuxConv(\n",
       "    (module_list): ModuleList(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv(\n",
       "        (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pred_conv): PredictionConv(\n",
       "    (loc_module_list): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (1): Conv(\n",
       "          (conv): Conv2d(32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1-3): 3 x Sequential(\n",
       "        (0): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (1): Conv(\n",
       "          (conv): Conv2d(64, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cla_module_list): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (1): Conv(\n",
       "          (conv): Conv2d(32, 222, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1-3): 3 x Sequential(\n",
       "        (0): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (1): Conv(\n",
       "          (conv): Conv2d(64, 222, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_h = configs.img_height // configs.downscale_factor\n",
    "new_w = configs.img_width // configs.downscale_factor\n",
    "setattr(configs, \"base_conv_input_size\", [new_h, new_w])\n",
    "\n",
    "base_size = pretty_print_module_list(base_conv.module_list, torch.zeros([1,1,configs.base_conv_input_size[0], configs.base_conv_input_size[1]]))\n",
    "\n",
    "aux_conv = AuxConv(configs.aux_conv_conv_layers, \n",
    "                configs.aux_conv_input_size, norm=nn.BatchNorm2d, act_fn=nn.ReLU(), spectral=False)\n",
    "aux_size = pretty_print_module_list(aux_conv.module_list, torch.zeros(base_size[-1]))\n",
    "\n",
    "setattr(configs, 'fm_channels', [base_size[i][1] for i in base_conv.fm_id] + [aux_size[i][1] for i in aux_conv.fm_id])\n",
    "setattr(configs, 'fm_size', [base_size[i][-2:] for i in base_conv.fm_id] + [aux_size[i][-2:] for i in aux_conv.fm_id])\n",
    "setattr(configs, 'n_fm', len(configs.fm_channels))\n",
    "setattr(configs,'fm_prior_aspect_ratio', configs.fm_prior_aspect_ratio[:configs.n_fm])\n",
    "setattr(configs,'fm_prior_scale', np.linspace(0.1, 0.9, configs.n_fm)) #[0.2, 0.375, 0.55, 0.725, 0.9] # [0.1, 0.2, 0.375, 0.55, 0.725, 0.9] \n",
    "assert len(configs.fm_prior_scale) == len(configs.fm_prior_aspect_ratio)\n",
    "setattr(configs, 'n_prior_per_pixel', [len(i)+1 for i in configs.fm_prior_aspect_ratio]) #in fm1, each pixel has 4 priors\n",
    "setattr(configs, 'multistep_milestones', list(range(10, configs.epochs, 5)))\n",
    "utils_mnist_ssd.img_size = base_size[0][-1]\n",
    "\n",
    "model = SSD(configs, base_conv, aux_conv).to(configs.device)\n",
    "\n",
    "checkpoint = torch.load(\"../docs_and_results/demo/model_checkpoint.pth\", weights_only=False, map_location=configs.device)\n",
    "\n",
    "# Load model state\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model = model.to(configs.device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 40, 160])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/lit2425/jenga/suman/pjf/computer_vision/UTN_Captcha_Detector/notebooks/../src_code/model_utils/mnist_ssd.py:288: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /pytorch/aten/src/ATen/native/IndexingUtils.h:29.)\n",
      "  image_boxes.append(decoded_locs[above_min_score_index][sorted_index][keep])\n",
      "/var/lit2425/jenga/suman/pjf/computer_vision/UTN_Captcha_Detector/notebooks/../src_code/model_utils/mnist_ssd.py:290: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /pytorch/aten/src/ATen/native/IndexingUtils.h:29.)\n",
      "  image_scores.append(sorted_score[keep])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample image\n",
    "image = test_dataset[0]\n",
    "image = image.unsqueeze(0).to(configs.device)\n",
    "print(image.shape)\n",
    "# Model prediction\n",
    "with torch.no_grad():\n",
    "    loc_preds, cls_preds, _ = model(image)\n",
    "    boxes, labels, scores = model.detect_object(loc_preds, cls_preds, min_score=0.25, max_overlap=0.5,top_k=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert Labels\n",
    "# category_id_labels = {\n",
    "#     0: \"0\",\n",
    "#     1: \"1\",\n",
    "#     2: \"2\",\n",
    "#     3: \"3\",\n",
    "#     4: \"4\",\n",
    "#     5: \"5\",\n",
    "#     6: \"6\",\n",
    "#     7: \"7\",\n",
    "#     8: \"8\",\n",
    "#     9: \"9\",\n",
    "#     10: \"A\",\n",
    "#     11: \"B\",\n",
    "#     12: \"C\",\n",
    "#     13: \"D\",\n",
    "#     14: \"E\",\n",
    "#     15: \"F\",\n",
    "#     16: \"G\",\n",
    "#     17: \"H\",\n",
    "#     18: \"I\",\n",
    "#     19: \"J\",\n",
    "#     20: \"K\",\n",
    "#     21: \"L\",\n",
    "#     22: \"M\",\n",
    "#     23: \"N\",\n",
    "#     24: \"O\",\n",
    "#     25: \"P\",\n",
    "#     26: \"Q\",\n",
    "#     27: \"R\",\n",
    "#     28: \"S\",\n",
    "#     29: \"T\",\n",
    "#     30: \"U\",\n",
    "#     31: \"V\",\n",
    "#     32: \"W\",\n",
    "#     33: \"X\",\n",
    "#     34: \"Y\",\n",
    "#     35: \"Z\"\n",
    "# }\n",
    "\n",
    "#labels = [category_id_labels[label.item()] for label in labels[0]]\n",
    "\n",
    "def plot_image_with_bboxes(image, bboxes, labels, title=\"Image with Bounding Boxes\"):\n",
    "    img_height, img_width = image.shape[1], image.shape[2] \n",
    "    \n",
    "    # Scale normalized bboxes to absolute pixel values for visualization\n",
    "    bboxes[:, [0, 2]] *= img_width\n",
    "    bboxes[:, [1, 3]] *= img_height\n",
    "\n",
    "    # Convert to integer values for plotting\n",
    "    bboxes_abs = bboxes.to(torch.int)\n",
    "\n",
    "    # Ensure labels are strings\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.tolist()\n",
    "    labels = [str(l) for l in labels]\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    image_with_boxes = draw_bounding_boxes(image, bboxes_abs, labels=labels, colors=\"red\", width=1)\n",
    "\n",
    "    # Image tensor to NumPy for visualization\n",
    "    img = image_with_boxes.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_image_with_bboxes(image.squeeze(0), boxes[0], labels[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
