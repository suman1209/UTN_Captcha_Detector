{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "sys.path.insert(0, \"../\")\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from src_code.task_utils.config_parser import ConfigParser\n",
    "from src_code.data_utils.dataset_utils import CaptchaDataset, get_dataloader\n",
    "from src_code.data_utils.preprocessing import get_img_transform, get_rectangle_img_transform\n",
    "from src_code.model_utils.mnist_ssd import SSD, BaseConv, pretty_print_module_list, AuxConv\n",
    "import src_code.model_utils.utils_mnist_ssd as utils_mnist_ssd\n",
    "from src_code.model_utils.mnist_ssd import SSD\n",
    "from torch import nn\n",
    "import yaml\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from src_code.task_utils.evaluation import edit_score\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Config Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config = {'task': 'train', 'data_configs': {'train_path': 'datasets/utn_dataset_curated/part2/train', 'val_path': 'datasets/utn_dataset_curated/part2/val', 'test_path': 'datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'color': False, 'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'train_preprocessed_dir': '../datasets/utn_dataset/part2/train/images', 'val_preprocessed_dir': '../datasets/utn_dataset/part2/val/images', 'test_preprocessed_dir': '../datasets/utn_dataset/part2/test/images', 'train_labels_dir': '../datasets/utn_dataset/part2/train/labels', 'val_labels_dir': '../datasets/utn_dataset/part2/val/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0.5, 'zoom_prob': 0.3, 'rotation_prob': 0.2, 'line_prob': 0.1, 'salt_pepper_prob': 0.2}}, 'model_configs': {'name': 'ssd_mnist', 'save_checkpoint': True, 'log_gradients': False, 'checkpoint': None, 'print_freq': 500, 'epochs': 10, 'batch_size': 100, 'device': 'cuda', 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 0.25, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005, 'clip_grad': None}, 'scheduler': {'name': 'LinearLR', 'milestones': [10, 20], 'gamma': 0.1, 'start_factor': 0.5, 'total_iter': 4}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}\n"
     ]
    }
   ],
   "source": [
    "# Load the base config\n",
    "base_config_path = \"../configs/configs_common_notebook.yaml\"\n",
    "if not Path(base_config_path).exists():\n",
    "    raise FileNotFoundError(f\"Base config file not found: {base_config_path}\")\n",
    "\n",
    "with open(base_config_path, \"r\") as file:\n",
    "    base_config_dict = yaml.safe_load(file)\n",
    "\n",
    "configs = ConfigParser(base_config_dict).get_parser()\n",
    "\n",
    "# Load the SSD-specific config\n",
    "ssd_config_path = \"../configs/default_ssd_configs.yaml\"\n",
    "if not Path(ssd_config_path).exists():\n",
    "    raise FileNotFoundError(f\"SSD config file not found: {ssd_config_path}\")\n",
    "\n",
    "with open(ssd_config_path, \"r\") as file:\n",
    "    ssd_config_dict = yaml.safe_load(file)\n",
    "\n",
    "configs.update(ssd_config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all required dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  60000\n",
      "val:  20000\n",
      "part2:  20000\n",
      "part3:  20000\n",
      "part4:  20000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CaptchaDataset(\n",
    "    configs.train_preprocessed_dir,\n",
    "    labels_dir=configs.train_labels_dir,\n",
    "    augment=False,\n",
    "    config=configs,\n",
    "    img_transform=get_rectangle_img_transform(configs)\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset = CaptchaDataset(\n",
    "    configs.val_preprocessed_dir,\n",
    "    labels_dir=configs.val_labels_dir,\n",
    "    augment=False,\n",
    "    config=configs,\n",
    "    img_transform=get_rectangle_img_transform(configs)\n",
    ")\n",
    "\n",
    "part2_test_dataset = CaptchaDataset(\n",
    "    '../datasets/utn_dataset/part2/test/images',\n",
    "    labels_dir=None,\n",
    "    augment=False,\n",
    "    config=configs,\n",
    "    img_transform=get_rectangle_img_transform(configs)\n",
    ")\n",
    "part3_test_dataset = CaptchaDataset(\n",
    "    '../datasets/utn_dataset/part3/test/images',\n",
    "    labels_dir=None,\n",
    "    augment=False,\n",
    "    config=configs,\n",
    "    img_transform=get_rectangle_img_transform(configs)\n",
    ")\n",
    "part4_test_dataset = CaptchaDataset(\n",
    "    '../datasets/utn_dataset/part4/test/images',\n",
    "    labels_dir=None,\n",
    "    augment=False,\n",
    "    config=configs,\n",
    "    img_transform=get_rectangle_img_transform(configs)\n",
    ")\n",
    "\n",
    "train_loader = get_dataloader(train_dataset, configs)\n",
    "val_loader = get_dataloader(val_dataset, configs)\n",
    "part2_test_loader = get_dataloader(part2_test_dataset, configs)\n",
    "part3_test_loader = get_dataloader(part3_test_dataset, configs)\n",
    "part4_test_loader = get_dataloader(part4_test_dataset, configs)\n",
    "\n",
    "print('train: ', len(train_loader.dataset))\n",
    "print('val: ', len(val_loader.dataset))\n",
    "print('part2: ', len(part2_test_loader.dataset))\n",
    "print('part3: ', len(part3_test_loader.dataset))\n",
    "print('part4: ', len(part4_test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"../docs_and_results/demo/model_checkpoint_40.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_conv = BaseConv(configs.base_conv_conv_layers, \n",
    "                    configs.base_conv_input_size, chosen_fm=[-2, -1],\n",
    "                    norm=nn.BatchNorm2d, act_fn=nn.ReLU(), spectral=False).to(configs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12780 priors in this model\n",
      "Done initialization\n"
     ]
    }
   ],
   "source": [
    "new_h = configs.img_height // configs.downscale_factor\n",
    "new_w = configs.img_width // configs.downscale_factor\n",
    "setattr(configs, \"base_conv_input_size\", [new_h, new_w])\n",
    "test_img = torch.zeros([1,1,configs.base_conv_input_size[0], configs.base_conv_input_size[1]]).to(configs.device)\n",
    "base_size = pretty_print_module_list(base_conv.module_list, test_img)\n",
    "\n",
    "aux_conv = AuxConv(configs.aux_conv_conv_layers, \n",
    "                configs.aux_conv_input_size, norm=nn.BatchNorm2d, act_fn=nn.ReLU(), spectral=False)\n",
    "aux_size = pretty_print_module_list(aux_conv.module_list, torch.zeros(base_size[-1]))\n",
    "\n",
    "setattr(configs, 'fm_channels', [base_size[i][1] for i in base_conv.fm_id] + [aux_size[i][1] for i in aux_conv.fm_id])\n",
    "setattr(configs, 'fm_size', [base_size[i][-2:] for i in base_conv.fm_id] + [aux_size[i][-2:] for i in aux_conv.fm_id])\n",
    "setattr(configs, 'n_fm', len(configs.fm_channels))\n",
    "setattr(configs,'fm_prior_aspect_ratio', configs.fm_prior_aspect_ratio[:configs.n_fm])\n",
    "setattr(configs,'fm_prior_scale', np.linspace(0.1, 0.9, configs.n_fm)) #[0.2, 0.375, 0.55, 0.725, 0.9] # [0.1, 0.2, 0.375, 0.55, 0.725, 0.9] \n",
    "assert len(configs.fm_prior_scale) == len(configs.fm_prior_aspect_ratio)\n",
    "setattr(configs, 'n_prior_per_pixel', [len(i)+1 for i in configs.fm_prior_aspect_ratio]) #in fm1, each pixel has 4 priors\n",
    "setattr(configs, 'multistep_milestones', list(range(10, configs.epochs, 5)))\n",
    "utils_mnist_ssd.img_size = base_size[0][-1]\n",
    "\n",
    "model = SSD(configs, base_conv, aux_conv).to(configs.device)\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=False, map_location=configs.device)\n",
    "# Load model state\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model = model.to(configs.device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate final JSON predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/lit2425/jenga/suman/pjf/computer_vision/UTN_Captcha_Detector/notebooks/../src_code/model_utils/mnist_ssd.py:295: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /pytorch/aten/src/ATen/native/IndexingUtils.h:29.)\n",
      "  image_boxes.append(decoded_locs[above_min_score_index][sorted_index][keep])\n",
      "/var/lit2425/jenga/suman/pjf/computer_vision/UTN_Captcha_Detector/notebooks/../src_code/model_utils/mnist_ssd.py:297: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /pytorch/aten/src/ATen/native/IndexingUtils.h:29.)\n",
      "  image_scores.append(sorted_score[keep])\n",
      "200it [03:34,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../docs_and_results/prediction_jsons/part2.json\n",
      "214.29063320159912sec taken for generating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [02:45,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../docs_and_results/prediction_jsons/part3.json\n",
      "165.75085592269897sec taken for generating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [03:32,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../docs_and_results/prediction_jsons/part4.json\n",
      "212.48611330986023sec taken for generating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src_code.task_utils.evaluation import test_generate_captchas_submission\n",
    "for item in ['part2', 'part3', 'part4']:\n",
    "    st_time = time.time()\n",
    "    test_generate_captchas_submission(model, part4_test_loader, configs, test_path = f\"../datasets/utn_dataset/{item}/test/images\", output_file = f\"../docs_and_results/prediction_jsons/{item}.json\")\n",
    "    et_time = time.time()\n",
    "    time_taken = et_time - st_time\n",
    "    print(f\"{time_taken}sec taken for generating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predicted bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 40, 160])\n"
     ]
    }
   ],
   "source": [
    "# Sample image\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "image = part2_test_dataset[0]\n",
    "image = image.unsqueeze(0).to(configs.device)\n",
    "print(image.shape)\n",
    "# Model prediction\n",
    "with torch.no_grad():\n",
    "    loc_preds, cls_preds, _ = model(image)\n",
    "    boxes, labels, scores = model.detect_object(loc_preds, cls_preds, min_score=0.25, max_overlap=0.5,top_k=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAACmCAYAAABHlYwjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIMFJREFUeJzt3Xt8jvX/B/DXvY3NvdnGHMZoFhobUcvIbE7LDEkIyQMjkuSQhKIlzLHyLX4MlcPX8i3KIYeiHCalRJrTHMZqlGab0WYz9z6/Pzz29rl3YJvNcL2ej0ePx2vXrvu6PrvuW3vv87k+n8uklFIgIiIiw7Ip6wYQERFR2WIxQEREZHAsBoiIiAyOxQAREZHBsRggIiIyOBYDREREBsdigIiIyOBYDBARERkciwEiIiKDYzFAVAreeecdmEymIu178eLFUm7VvWnnzp0wmUzYuXOnbBs4cCDq1KlTZm0iMhoWA1Rsy5Ytg8lkwv79+8u6KfeFiIgIrFu3rsSPO3DgQJhMJvnPzs4OtWvXRp8+fXD06NESP9+Drk2bNlbXs3z58vDy8sLQoUPx559/lnXziEqFXVk3gOhBNGnSJEyYMMFqW0REBHr27Ilu3bqV+Pns7e2xdOlSAMD169dx+vRpLFq0CFu3bsXRo0dRs2bNEj9naVqyZAmys7PL7Py1atXCjBkzAADXrl3D0aNHsWjRInzzzTc4duwYzGZzmbWNqDSwGCAqBXZ2drCzu3v/vOzs7NCvXz+rbS1atECXLl2wadMmDBky5K61pSSUK1euTM/v4uKS53p6eXlhxIgR+OGHH/DUU0+VUcuISgeHCahEDRw4EE5OTvjjjz/QpUsXODk5wcPDAwsWLAAAxMTEoF27dnB0dISnpyeioqKsXp+cnIzXX38djRs3hpOTE5ydnREaGopDhw7lOVd8fDy6du0KR0dHVKtWDWPGjME333yTZ/wZAPbt24eOHTvCxcUFZrMZrVu3xg8//HDLn0UphSpVquC1116TbdnZ2XB1dYWtrS0uXbok22fNmgU7Ozv8+++/APLeM2AymZCWlobly5dL9/PAgQOtznfp0iUMHDgQrq6ucHFxQVhYGNLT02/Zxltxd3cHgDxFSVxcHJ577jlUrlwZZrMZLVq0wKZNm6z2yRkCOnv2rNX2/Mb327Rpg0aNGuHo0aNo27YtzGYzPDw8MHv27DxtSkhIQLdu3azes8zMzDz75b5n4OzZszCZTJg7dy4WL16MunXrwt7eHs2aNcMvv/yS5/VffPEFfHx84ODggEaNGuGrr7664/sQCrqeBw8eRGhoKJydneHk5IT27dvjp59+ku9///33sLGxwdtvv231uqioKJhMJixcuFC2nTt3DoMGDUL16tVhb28PX19ffPLJJ3na8tFHH8HX1xdmsxmVKlXCE088keffElFRsGeASpzFYkFoaCiCgoIwe/ZsrFq1CiNGjICjoyPeeustvPDCC+jevTsWLVqE/v3748knn4SXlxeAG7+o1q1bh+eeew5eXl64cOECIiMj0bp1a6vu7rS0NLRr1w5//fUXRo0aBXd3d0RFRWHHjh152vP9998jNDQUfn5+CA8Ph42NDT799FO0a9cO0dHR8Pf3z/fnMJlMCAgIwO7du2Xb77//jtTUVNjY2OCHH35A586dAQDR0dF47LHH4OTklO+xVq5ciRdffBH+/v4YOnQoAKBu3bpW+/Tq1QteXl6YMWMGDhw4gKVLl6JatWqYNWtWoa57zg2IFosFcXFxGD9+PNzc3NClSxfZ58KFC2jZsiXS09MxcuRIuLm5Yfny5ejatSvWrFmDZ599tlDnyi0lJQUdO3ZE9+7d0atXL6xZswbjx49H48aNERoaCgC4evUq2rdvjz/++AMjR45EzZo1sXLlSnz//feFPk9UVBSuXLmCl156CSaTCbNnz0b37t0RFxcnvQmbNm1C79690bhxY8yYMQMpKSkYPHgwPDw8Cn0ei8Ui1zMrKwvHjh1DeHg46tWrh4CAANnvyJEjCAwMhLOzM9544w2UK1cOkZGRaNOmDXbt2oXmzZujXbt2GD58OGbMmIFu3brh8ccfx19//YVXX30VwcHBGDZsGIAb702LFi1gMpkwYsQIVK1aFVu2bMHgwYNx+fJljB49GsCNIZSRI0eiZ8+eGDVqFDIyMvD7779j37596Nu3b6F/RiIriqiYPv30UwVA/fLLL7JtwIABCoCKiIiQbSkpKapChQrKZDKp1atXy/bjx48rACo8PFy2ZWRkKIvFYnWeM2fOKHt7e/Xuu+/Ktvfee08BUOvWrZNtV69eVQ0aNFAA1I4dO5RSSmVnZ6v69eurkJAQlZ2dLfump6crLy8v9dRTT93yZ5wzZ46ytbVVly9fVkop9eGHHypPT0/l7++vxo8fr5RSymKxKFdXVzVmzBh5XXh4uMr9z8vR0VENGDAgzzly9h00aJDV9meffVa5ubndsn1K3bzmuf/z8PBQv/76q9W+o0ePVgBUdHS0bLty5Yry8vJSderUkWuf896eOXPG6vU7duywur5KKdW6dWsFQK1YsUK2ZWZmKnd3d9WjRw/ZNm/ePAVAff7557ItLS1N1atXL88xBwwYoDw9PeXrM2fOKADKzc1NJScny/b169crAGrjxo2yrXHjxqpWrVrqypUrsm3nzp0KgNUxC5Lz8+T+r2HDhiouLs5q327duqny5cur06dPy7bz58+rihUrqqCgoDw/p6+vr8rIyFCdO3dWzs7OKj4+XvYZPHiwqlGjhrp48aLVOfr06aNcXFxUenq6UkqpZ555Rvn6+t725yAqCg4TUKl48cUXJbu6usLb2xuOjo7o1auXbPf29oarqyvi4uJkm729PWxsbnwsLRYLkpKS4OTkBG9vbxw4cED227p1Kzw8PNC1a1fZ5uDgkGds/LfffsPJkyfRt29fJCUl4eLFi7h48SLS0tLQvn177N69+5Y3qgUGBsJisWDv3r0AbvQABAYGIjAwENHR0QCAw4cP49KlSwgMDCzOpRI5fyHq505KSsLly5dv+1oHBwds27YN27ZtwzfffIPIyEg4OTmhU6dOOHHihOy3efNm+Pv7o1WrVrLNyckJQ4cOxdmzZ4s9+8DJyclqjL18+fLw9/e3em83b96MGjVqoGfPnrLNbDZLT0lh9O7dG5UqVZKvc655znnOnz+PmJgY9O/f36qXpnXr1mjcuHGhz1OnTh25nlu2bMG8efOQmpqK0NBQJCYmArjx+fz222/RrVs3PPzww/LaGjVqoG/fvtizZ4+8d2azGcuWLcOxY8cQFBSETZs24YMPPsBDDz0E4MaQ1Nq1a/H0009DKSWf04sXLyIkJASpqany+Xd1dUVCQkK+wyNExcVigEqcg4MDqlatarXNxcUFtWrVyjP33sXFBSkpKfJ1dnY2PvjgA9SvXx/29vaoUqUKqlatKt3zOeLj41G3bt08x6tXr57V1ydPngQADBgwAFWrVrX6b+nSpcjMzLQ6bm6PP/44zGaz/OLPKQaCgoKwf/9+ZGRkyPf0X7DFkfOLIUfOLz39+hTE1tYWwcHBCA4ORocOHTB06FBs374dqampmDhxouwXHx8Pb2/vPK9v2LChfL848ntvK1WqZNX2+Ph41KtXL89++bWnILe7Rjntz/05KGhbQRwdHeV6duzYEaNGjcKGDRsQGxuLmTNnAgASExORnp5e4PXMzs62mooYEBCAl19+GT///DNCQkIwaNAg+V5iYiIuXbqExYsX5/mchoWFAQD++ecfAMD48ePh5OQEf39/1K9fH6+88spt738huh3eM0AlztbWtkjblVKSIyIiMHnyZAwaNAhTp05F5cqVYWNjg9GjRxdrqlnOa+bMmYOmTZvmu09B4/zAjbvamzdvjt27d+PUqVP4+++/ERgYiOrVqyMrKwv79u1DdHQ0GjRokKcAKqrCXJ+iqFWrFry9va3ueSisghZMslgs+W4v6bYX5G6dJz9+fn5wcXEp1vUEgMzMTLnx8vTp00hPT5cpijmf0379+mHAgAH5vv7RRx8FcKPQiI2Nxddff42tW7di7dq1+L//+z+8/fbbmDJlSrHaRsRigO4pa9asQdu2bfHxxx9bbb906RKqVKkiX3t6euLo0aNQSln94jp16pTV63Ju0nN2dkZwcHCx2hQYGIhZs2Zh+/btqFKlCho0aACTyQRfX19ER0cjOjra6ia9ghR2RcKSdP36dZnhANy4brGxsXn2O378uHwfuPkXtz5jAih+z0HOsQ8fPpznPcuvPXdyDiDv56CgbUVlsVjkelatWhVms7nA62ljY4PatWvLtvDwcBw7dgxz587F+PHjMWHCBHz44YdyrIoVK8JisRTqc+ro6IjevXujd+/euHbtGrp3747p06dj4sSJcHBwuOOfk4yHwwR0T7G1tc3zV94XX3yBc+fOWW0LCQnBuXPnsGHDBtmWkZGBJUuWWO3n5+eHunXrYu7cuVa/FHPkjP/eSmBgIDIzMzFv3jy0atVKfpEFBgZi5cqVOH/+fKHuF3B0dMzzy7U0nThxArGxsWjSpIls69SpE37++Wf8+OOPsi0tLQ2LFy9GnTp14OPjA+BmEaX/FWyxWLB48eJit6dTp044f/481qxZI9vS09Pv6Ji51axZE40aNcKKFSus3u9du3YhJibmjo69Y8cO/Pvvv3I9bW1t0aFDB6xfv95qCuaFCxcQFRWFVq1awdnZGcCNqa1z587F6NGjMXbsWIwbNw7z58/Hrl275Fg9evTA2rVrcfjw4Tzn1j+nSUlJVt8rX748fHx8oJRCVlbWHf2MZFzsGaB7SpcuXfDuu+8iLCwMLVu2RExMDFatWmV1gxYAvPTSS5g/fz6ef/55jBo1CjVq1MCqVavkr6KcX9g2NjZYunQpQkND4evri7CwMHh4eODcuXPYsWMHnJ2dsXHjxlu26cknn4SdnR1iY2OtbnYLCgqSOeKFKQb8/Pywfft2vP/++6hZsya8vLzQvHnzIl2fgly/fh3//e9/Adzocj579iwWLVqE7OxshIeHy34TJkzAZ599htDQUIwcORKVK1fG8uXLcebMGaxdu1Zu3vT19UWLFi0wceJEJCcno3Llyli9ejWuX79e7DYOGTIE8+fPR//+/fHrr7+iRo0aWLlyZYmv5hcREYFnnnkGAQEBCAsLQ0pKCubPn49GjRrlWxDmJzU1Va7n9evXERsbi4ULF6JChQpWK0tOmzYN27ZtQ6tWrTB8+HDY2dkhMjISmZmZss5CRkYGBgwYgPr162P69OkAgClTpmDjxo0ICwtDTEwMHB0dMXPmTOzYsQPNmzfHkCFD4OPjg+TkZBw4cADbt29HcnIyAKBDhw5wd3dHQEAAqlevjmPHjmH+/Pno3LkzKlasWJKXkoykrKYx0P2voKmFjo6OefZt3bp1vtOhPD09VefOneXrjIwMNXbsWFWjRg1VoUIFFRAQoH788UfVunVr1bp1a6vXxsXFqc6dO6sKFSqoqlWrqrFjx6q1a9cqAOqnn36y2vfgwYOqe/fuys3NTdnb2ytPT0/Vq1cv9d133xXqZ23WrJkCoPbt2yfbEhISFABVu3btPPvnN7Xw+PHjKigoSFWoUEEBkGmGOfsmJiZa7V/Q9L7c8pta6OzsrNq3b6+2b9+eZ//Tp0+rnj17KldXV+Xg4KD8/f3V119/ne9+wcHByt7eXlWvXl29+eabatu2bflOLczvvc09PVAppeLj41XXrl2V2WxWVapUUaNGjVJbt24t9NTCOXPm5DkPck1PVUqp1atXqwYNGih7e3vVqFEjtWHDBtWjRw/VoEGD/C+iJvfUQpPJpCpXrqy6du2aZ6qmUkodOHBAhYSEKCcnJ2U2m1Xbtm3V3r175ftjxoxRtra2Vp8dpZTav3+/srOzUy+//LJsu3DhgnrllVdU7dq1Vbly5ZS7u7tq3769Wrx4sewTGRmpgoKC5LNct25dNW7cOJWamnrbn42oICal7sKdN0R3ybx58zBmzBgkJCQUaZEZevA1bdoUVatWxbZt28q6KUT3HN4zQPetq1evWn2dkZGByMhI1K9fn4WAgWVlZeUZzti5cycOHTqENm3alE2jiO5xvGeA7lvdu3fHQw89hKZNm8oY7/Hjx7Fq1aqybhqVoXPnziE4OBj9+vVDzZo1cfz4cSxatAju7u55FnYiohtYDNB9KyQkBEuXLsWqVatgsVjg4+OD1atXo3fv3mXdNCpDlSpVgp+fH5YuXYrExEQ4Ojqic+fOmDlzJtzc3Mq6eUT3JN4zQEREZHC8Z4CIiMjgWAwQEREZHIsBIiIigyv0DYSTJ08uzXYQERFRKZg6dept92HPABERkcGxGCAiIjI4FgNEREQGx2KAiIjI4FgMEBERGRyLASIiIoNjMUBERGRwLAaIiIgMjsUAERGRwbEYICIiMrhCL0dMREQFmzptWlk34ZYmT5pU1k24L9zr72NuJfW+smeAiIjI4FgMEBERGRyLASIiIoPjPQNERCWsMOO4Dg4OkgMDAyVnZGRIPn36tOS0tDTJqamphWrH/Tb+fa+50/H4Rx55RPLIkSMlf/7555J//PFHyVlZWYU6bmm8r+wZICIiMjgWA0RERAbHYQIiolJkMpkkP/zww5InTJggOTk5WfL+/fslu7u7S46KipI8ffp0q3MUtnuZ7i57e3vJLVu2lPzdd99JXrBggeSFCxdKPnnypOTs7OzSaqJgzwAREZHBsRggIiIyOA4TEBGVovbt20tetGiR5M6dO0t+7bXXJP/999+Sly1bJnnEiBGSu3XrZnWOPXv2lERTqYTFxMRI7tChg+ShQ4dKvnTpkuS9e/dKnjFjhuQVK1aUUgtvYs8AERGRwbEYICIiMjgOExARlTAbm5t/ZzVu3FjyI1p3/qtOTpKv+/lJPh4SIvk/n34qeWZEhOSkF16wOt9w7VjXlCpus+kWen35pdXXFbVFoHS2Fotkt6QkyTPGjpU8c+ZMyQEBAZLnzZsnWR8a+DLXuV8tZJuLgj0DREREBsdigIiIyOA4TEBEVMIqVqwoObhJE8mfXrkiee2bb0rO1O46f15bt37JkiWSHVeulOzRoIHV+ZT2mr2jR0v2/OOPojadCvB59+6F2s/v4EHJrpcv57tPpUqVJHfq1Emy/pyCAwcOSP5UGy4CAHh5FaotRcGeASIiIoNjMUBERGRwHCYgIiph+myCWhUqSI5t0UJy2ODBkudOmSLZUXuEsY+Pj+Tmzs6SJ1SubHW+nmfPSj7l7V3MVlNxmbTcXHu2xKr+/SW30N77CG1mSGJiouSNGzfmu0/uYYIpKHnsGSAiIjI4FgNEREQGx2ECIqISlpKSIjls1izJGzZskPzt8OGSX9EWDTL37i35c22WwKOHDkn2y/VI22bjxkluUaVKcZtNxeR94oTklHr1JI/WuvpHa7M8atSoIfkFbQEps9ks+YR2zLuBPQNEREQGx2KAiIjI4DhMQER33dRp08q6CXlMnjSpVI7722+/Sf7oo48kf/LWW5KdIyMlH1i3TnJ/bdaAzYIFkrceO2Z1jpVBQZI9du2S/HHxmkyF4ODgILnryZOSx7m4SK44caJkfUbA+++/L3n16tWSVRk+V4I9A0RERAbHYoCIiMjgWAwQEREZHO8ZIKIypY/Vm0w313Lr2bOnZP3BP5999pnkq1ev3tG578a9C9naNMD/LlokOeSTTyTv6NdP8oB27SQfOXJE8r+OjpLXWSxW55j//POS/9buH4B2XwKVrKerVZMcl5ws+U9t2uAk7eFGztoKkgkJCaXcuqJjzwAREZHBsRggIiIyOA4TENE9Q59apQ8N/Oc//5EcpHWDH9SeHX+v0h9iE/SxNtlv7FiJa/ftkxyhPWjo6aeflhxQrpzkja+9ZnWO706dkuylPbSISo+99plMGjZMckdtOGfz5s13tU13gj0DREREBsdigIiIyOBKbZjgXlxhLLfSWnGMiO7cnj17JKenp0tu2bKl5FsNE1TRHtjTpEkTyTExMSXVxEJ5THvAUL3TpyX/tXy55CW2tjdfoN11/of20KKDrq6Sm2gzKgBg0LVrkrNt+DdeaaiszRgAAPOlS5L/m5Z28xt6vo/wU0NERGRwLAaIiIgMjrMJiOieUS4rS7Lf7NmSfePjJSevXCn5Ua0bHQBc+vaVPGfKFMlB2kN+hrq55Xtub+2O/FjtmfTFUZhhUs9C3PVfKyBAcstb7Eelo7DD3ffDsPjtsGeAiIjI4FgMEBERGVyZDBOUxl38+prmblo34KjRo0v8XERUOlrs3y/5T3d3yf67d0ueqy3WM0y7Ix8A5mrPku9/+bLk6l26SP5AW1N+wk8/Se6yZYvk2FdfLXLbOTvpwWDU95E9A0RERAbHYoCIiMjg7uvZBPrQQLNmzSRPnDhR8qi72iIiuhP7H3tMsq22+E5TbVGdx+3tJTtqC/EAwNKlSyU/MWOG5CUXLki+XsCiMNn6wj9EBsOeASIiIoNjMUBERGRwZT5M4JCRIbnTt99K9omNlTxt3DjJ+tCAvka5/ojTQ9pa4LAr8x+RHmD322Ij9+Kd0jbaWvq1H31U8pgxYyR3+N//JD9z8qTkV2rXtjrW5BdflHwiMVE/icSe69fn2471oaFFaDXRg4U9A0RERAbHYoCIiMjgyrwPvZ/W/Rf7yCOS7Y4ckTxpzhzJO7Uuvg8++EDy11qX4t8PPyzZ4xbnrp2QIPnPWrUK32iifNyqC95Wu1PdwcFBcnZ2tuQMbchMKVVi7brXhzLCwsIkv/XWW5L7as8ZaDJvnuSBlSpJHpbrWL9rswvSCpg1sOaZZ24eV3uc8WO//y45zsvrtu0mepCwZ4CIiMjgWAwQEREZHIsBIiIigyvzewZW9+ghuc/atZKztSmEttr0wA8//FDyeu3+gTTtoSb/69dP8mvz5xd47me//vrmcYflHn18sNzNceN7cfpaWfD09JQ8Tpse+8QTT0jOzMyUvHXrVslrtX8LJ7WpdCV5L8G9YuTIkZJf0B4c5PLJJ5KnaZ/fgWvWSG7atavVsSy+vpIf016/+amnJGfb5P83kPepU0VpNtEDhT0DREREBsdigIiIyODKfJjgXycnyf/r2VPya4sXS9a7UtdoXYTvvfee5IHadC3z1auFOnf5rKyiNfYBMWvmTMnDtOGRxo0b57t/UlKS5PnasEtcXJzke3362t1Qvnx5q6/Dw8MlP/vss5KjoqIk16xZU7K+4p4+Le7111+XvGnTJsn6VMT7WadOnSQ3OndOcsyyZZKzGjaUnLpli+Rz2v8/AKBt27aSf9WGDBpqK5oe0Y6lu+jmVoRWEz1Y2DNARERkcCwGiIiIDK7Mhwn0Bw81aNNGcoY2a8BsNkuO0IYG9OGDDVpX44ta9+KtbOzYsShNfWDod7ZPnDhRci1tFcar2lBLSEiI5I8//liyvkIcARUrVrT6ukPz5pI/1oZa3qlWTfJVe3vJR598UvLb2ud8mfZ5njFjhuT3339f8v08ZHBOGxrYEhAguceGDZJb/PKL5GxtdlGPf/+1Opb+Wf21WTPJbbVrqB9L99XTTxeh1UQPFvYMEBERGRyLASIiIoMrk2EC/aEtrVq1kqx3WXfRhgauXbsmWR8a0HXcvl3yGu3O7T7a7IPcfI4flxxbv/7tmv3A0Lv3N27cKFl/SMz169clt2jRQrJ+N/vjjz9eWk28L+VeEOiZw4clv3H6tGSztijTk3v3Sq6yZInkIUOGSB4/frxkfcaCPhNk+fLlVufWH4B0r7uTmSgHc2/YvDn/XAiJnE1ABsaeASIiIoNjMUBERGRwZTJM4OPjI3n69OmSV61aJTmrSZMiHbP6P/9IPurtXajXNDhxokjneFBYLBbJ5xMSJB/T7sT2OH9eso3WBT0xOVlycHBwaTXxvnTlyhWrr33OnpX8TZcukh/RZm3EaAvj9NcWI/o2MVHyu+++K3nBggWSU1JSJLdr187q3EePHi1K0+86Pr+C6N7CngEiIiKDYzFARERkcGUyTHBW6z4dPny45M8++0zy+CI+qlVfV/yhP/8s1GsuuboW6RwPiiztmQxz69WTPExbdCV+3jzJlXftkvyV9mjdT379VTKXa7G+rgBQp0IFyRW8vCSPGjVK8qQ335TspD2PQHf58mXJU6dOlbxLe1/6aY/tBoDJkycXttlEROwZICIiMjoWA0RERAZXJsMEb2iLqBRVYRYoeXHFikIda512h7eR6Heaz9ceAfsfrTv6r4ULJZu1teADtQWg2rdvf/OgU6aUdDPve/rCTREREZIt2hDY4MGDJZu0R0sXRB9i27lzp2R9TX4AcHR0LEpTicjg2DNARERkcCwGiIiIDK7Uhgm4qMi96x9tgaaK6emSn9Mep+uxb59ki7Oz5FdtbtaPmVrX95YSb+X971+tq36j9kjiL7/8UnJ1bShhYp06kqtpjzlO1BYg0p/TceTIEcmDBg2yOreDg0MxW01ERsSeASIiIoNjMUBERGRwLAaIiIgMrkymFlLZ0sedy2nT3BJsbSVv6N1bsu/x45K/SkqS3L9KldJq4gPhhLa6o0d0tOQxY8ZI/kebZrvzscckb9ly8y6MKO0BRn9qq2v21t6jBO2BUwDgqz0AiYjodtgzQEREZHAsBoiIiAyOwwQGpA8TpGoP0/nTz+/mTsnJEmMbNpScpD0ox3bAgFJq4YNhV6tWkruvXy/ZR3sIVEb9+pKTtWubMG6c5Ndff11yuXLlJF+8eFHyhAkTrM596NChYraaiIyIPQNEREQGx2KAiIjI4DhMYEDR2p3tcUOHSj6gdV9/cuaM5I7ayoS9PTzyPQ7llaFdt6hevW7/gj17JPbp00eyvhphBW1Yp542W4HDAkR0J9gzQEREZHAsBoiIiAyOwwQG9O2330p+ITJS8svdukk+Wr265DmVKkkeZjJJfuONN24edNiwEm6lsV29elVyfHx8GbaEiIyAPQNEREQGx2KAiIjI4DhMYEDh77xz8wst18uzZ1479S84NGBl6rRpZd0EIqJiYc8AERGRwbEYICIiMjgOExjE5EmTyroJDyReVyJ6ELBngIiIyOBYDBARERkciwEiIiKDYzFARERkcCwGiIiIDI7FABERkcGxGCAiIjI4FgNEREQGx2KAiIjI4FgMEBERGRyLASIiIoNjMUBERGRwLAaIiIgMjsUAERGRwbEYICIiMjgWA0RERAbHYoCIiMjgWAwQEREZnEkppcq6EURERFR22DNARERkcCwGiIiIDI7FABERkcGxGCAiIjI4FgNEREQGx2KAiIjI4FgMEBERGRyLASIiIoNjMUBERGRw/w+xv9iPKfpvbwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert Labels\n",
    "\n",
    "#labels = [category_id_labels[label.item()] for label in labels[0]]\n",
    "\n",
    "def plot_image_with_bboxes(image, bboxes, labels, title=\"Image with Bounding Boxes\"):\n",
    "    img_height, img_width = image.shape[1], image.shape[2] \n",
    "    \n",
    "    # Scale normalized bboxes to absolute pixel values for visualization\n",
    "    bboxes[:, [0, 2]] *= img_width\n",
    "    bboxes[:, [1, 3]] *= img_height\n",
    "\n",
    "    # Convert to integer values for plotting\n",
    "    bboxes_abs = bboxes.to(torch.int)\n",
    "\n",
    "    # Ensure labels are strings\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.tolist()\n",
    "    labels = [str(l) for l in labels]\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    image_with_boxes = draw_bounding_boxes(image, bboxes_abs, labels=labels, colors=\"red\", width=1)\n",
    "\n",
    "    # Image tensor to NumPy for visualization\n",
    "    img = image_with_boxes.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_image_with_bboxes(image.squeeze(0), boxes[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_code.task_utils.evaluation import generate_edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|██████████████████████████████████████████████▏             | 154/200 [01:43<00:30,  1.52it/s]"
     ]
    }
   ],
   "source": [
    "# mean_edit_distance_train_loader, train_captcha_count = generate_edit_distance(model, train_loader, configs)\n",
    "mean_edit_distance_val_loader, val_captcha_count = generate_edit_distance(model, val_loader, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{mean_edit_distance_val_loader = }\")\n",
    "print(f\"{mean_edit_distance_train_loader = }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
