{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from src_code.task_utils.config_parser import ConfigParser\n",
    "from src_code.data_utils.dataset_utils import CaptchaDataset, get_dataloader\n",
    "from src_code.data_utils.preprocessing import get_img_transform, get_rectangle_img_transform\n",
    "from src_code.model_utils.mnist_ssd import SSD, BaseConv, pretty_print_module_list, AuxConv\n",
    "import src_code.model_utils.utils_mnist_ssd as utils_mnist_ssd\n",
    "from src_code.model_utils.mnist_ssd import SSD\n",
    "from torch import nn\n",
    "import yaml\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from src_code.task_utils.evaluation import edit_score\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Config Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config = {'task': 'train', 'data_configs': {'train_path': 'datasets/utn_dataset_curated/part2/train', 'val_path': 'datasets/utn_dataset_curated/part2/val', 'test_path': 'datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'color': False, 'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'train_preprocessed_dir': '../datasets/utn_dataset/part2/train/images', 'val_preprocessed_dir': '../datasets/utn_dataset/part2/val/images', 'test_preprocessed_dir': '../datasets/utn_dataset/part2/test/images', 'train_labels_dir': '../datasets/utn_dataset/part2/train/labels', 'val_labels_dir': '../datasets/utn_dataset/part2/val/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0.5, 'zoom_prob': 0.3, 'rotation_prob': 0.2, 'line_prob': 0.1, 'salt_pepper_prob': 0.2}}, 'model_configs': {'name': 'ssd_mnist', 'save_checkpoint': True, 'log_gradients': False, 'checkpoint': None, 'print_freq': 500, 'epochs': 10, 'batch_size': 2, 'device': 'cuda', 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 0.25, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005, 'clip_grad': None}, 'scheduler': {'name': 'LinearLR', 'milestones': [10, 20], 'gamma': 0.1, 'start_factor': 0.5, 'total_iter': 4}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the base config\n",
    "base_config_path = \"../configs/configs_common_notebook.yaml\"\n",
    "if not Path(base_config_path).exists():\n",
    "    raise FileNotFoundError(f\"Base config file not found: {base_config_path}\")\n",
    "\n",
    "with open(base_config_path, \"r\") as file:\n",
    "    base_config_dict = yaml.safe_load(file)\n",
    "\n",
    "configs = ConfigParser(base_config_dict).get_parser()\n",
    "\n",
    "# Load the SSD-specific config\n",
    "ssd_config_path = \"../configs/default_ssd_configs.yaml\"\n",
    "if not Path(ssd_config_path).exists():\n",
    "    raise FileNotFoundError(f\"SSD config file not found: {ssd_config_path}\")\n",
    "\n",
    "with open(ssd_config_path, \"r\") as file:\n",
    "    ssd_config_dict = yaml.safe_load(file)\n",
    "\n",
    "configs.update(ssd_config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CaptchaDataset\n",
      "    Number of datapoints: 20000\n",
      "    Root location: ../datasets/utn_dataset/part2/test/images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset = CaptchaDataset(\n",
    "    configs.test_preprocessed_dir,\n",
    "    labels_dir=None,\n",
    "    augment=False,\n",
    "    config=configs,\n",
    "    img_transform=get_rectangle_img_transform(configs)\n",
    ")\n",
    "\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CaptchaDataset\n",
      "    Number of datapoints: 20000\n",
      "    Root location: ../datasets/utn_dataset/part2/val/images\n"
     ]
    }
   ],
   "source": [
    "val_dataset = CaptchaDataset(\n",
    "    configs.val_preprocessed_dir,\n",
    "    labels_dir=configs.val_labels_dir,\n",
    "    augment=False,\n",
    "    config=configs,\n",
    "    img_transform=get_rectangle_img_transform(configs)\n",
    ")\n",
    "\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CaptchaDataset\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ../datasets/utn_dataset/part2/train/images\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CaptchaDataset(\n",
    "    configs.train_preprocessed_dir,\n",
    "    labels_dir=configs.train_labels_dir,\n",
    "    augment=False,\n",
    "    config=configs,\n",
    "    img_transform=get_rectangle_img_transform(configs)\n",
    ")\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = get_dataloader(val_dataset, configs)\n",
    "train_loader = get_dataloader(train_dataset, configs)\n",
    "test_loader = get_dataloader(test_dataset, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_conv = BaseConv(configs.base_conv_conv_layers, \n",
    "                    configs.base_conv_input_size, chosen_fm=[-2, -1],\n",
    "                    norm=nn.BatchNorm2d, act_fn=nn.ReLU(), spectral=False).to(configs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12780 priors in this model\n",
      "Done initialization\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SSD(\n",
       "  (base_conv): BaseConv(\n",
       "    (module_list): ModuleList(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): ReLU()\n",
       "      (10): Conv(\n",
       "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): ReLU()\n",
       "      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (14): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (16): ReLU()\n",
       "      (17): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (19): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (aux_conv): AuxConv(\n",
       "    (module_list): ModuleList(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv(\n",
       "        (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pred_conv): PredictionConv(\n",
       "    (loc_module_list): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (1): Conv(\n",
       "          (conv): Conv2d(32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1-3): 3 x Sequential(\n",
       "        (0): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (1): Conv(\n",
       "          (conv): Conv2d(64, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cla_module_list): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (1): Conv(\n",
       "          (conv): Conv2d(32, 222, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1-3): 3 x Sequential(\n",
       "        (0): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (1): Conv(\n",
       "          (conv): Conv2d(64, 222, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_h = configs.img_height // configs.downscale_factor\n",
    "new_w = configs.img_width // configs.downscale_factor\n",
    "setattr(configs, \"base_conv_input_size\", [new_h, new_w])\n",
    "test_img = torch.zeros([1,1,configs.base_conv_input_size[0], configs.base_conv_input_size[1]]).to(configs.device)\n",
    "base_size = pretty_print_module_list(base_conv.module_list, test_img)\n",
    "\n",
    "aux_conv = AuxConv(configs.aux_conv_conv_layers, \n",
    "                configs.aux_conv_input_size, norm=nn.BatchNorm2d, act_fn=nn.ReLU(), spectral=False)\n",
    "aux_size = pretty_print_module_list(aux_conv.module_list, torch.zeros(base_size[-1]))\n",
    "\n",
    "setattr(configs, 'fm_channels', [base_size[i][1] for i in base_conv.fm_id] + [aux_size[i][1] for i in aux_conv.fm_id])\n",
    "setattr(configs, 'fm_size', [base_size[i][-2:] for i in base_conv.fm_id] + [aux_size[i][-2:] for i in aux_conv.fm_id])\n",
    "setattr(configs, 'n_fm', len(configs.fm_channels))\n",
    "setattr(configs,'fm_prior_aspect_ratio', configs.fm_prior_aspect_ratio[:configs.n_fm])\n",
    "setattr(configs,'fm_prior_scale', np.linspace(0.1, 0.9, configs.n_fm)) #[0.2, 0.375, 0.55, 0.725, 0.9] # [0.1, 0.2, 0.375, 0.55, 0.725, 0.9] \n",
    "assert len(configs.fm_prior_scale) == len(configs.fm_prior_aspect_ratio)\n",
    "setattr(configs, 'n_prior_per_pixel', [len(i)+1 for i in configs.fm_prior_aspect_ratio]) #in fm1, each pixel has 4 priors\n",
    "setattr(configs, 'multistep_milestones', list(range(10, configs.epochs, 5)))\n",
    "utils_mnist_ssd.img_size = base_size[0][-1]\n",
    "\n",
    "model = SSD(configs, base_conv, aux_conv).to(configs.device)\n",
    "\n",
    "checkpoint = torch.load(\"../docs_and_results/demo/model_checkpoint.pth\", weights_only=False, map_location=configs.device)\n",
    "# Load model state\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model = model.to(configs.device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Save for Submission Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, gt_truth, labels = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_string = 'CKG2' predicted_captcha = 'C3KG2'\n",
      "gt_string = 'M28D5G' predicted_captcha = 'M2B85G'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/lit2425/jenga/suman/pjf/computer_vision/UTN_Captcha_Detector/notebooks/../src_code/model_utils/mnist_ssd.py:295: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /pytorch/aten/src/ATen/native/IndexingUtils.h:29.)\n",
      "  image_boxes.append(decoded_locs[above_min_score_index][sorted_index][keep])\n",
      "/var/lit2425/jenga/suman/pjf/computer_vision/UTN_Captcha_Detector/notebooks/../src_code/model_utils/mnist_ssd.py:297: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /pytorch/aten/src/ATen/native/IndexingUtils.h:29.)\n",
      "  image_scores.append(sorted_score[keep])\n"
     ]
    }
   ],
   "source": [
    "# For Val Dataset\n",
    "\n",
    "def val_generate_captchas_submission(model, val_loader, configs, output_file: str = \"val_captchas_submission.json\"):\n",
    "    \"\"\"\n",
    "    Generates captchas for the set using the pre-loaded model and saves the results in a JSON file.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    with torch.no_grad():\n",
    "        for images, gt_truth, labels_gt in val_loader:\n",
    "            # print(img, gt_truth, labels)\n",
    "            images = images.to(configs.device)\n",
    "            for idx, image in enumerate(images):\n",
    "                # print(image.unsqueeze(0))\n",
    "                loc_preds, cls_preds, _ = model(image.unsqueeze(0))\n",
    "                boxes, labels, scores = model.detect_object(loc_preds, cls_preds, min_score=0.25, max_overlap=0.5,top_k=20)\n",
    "                \n",
    "                list_boxes = boxes[0].tolist()\n",
    "                assert len(list_boxes) == len(labels[0])\n",
    "                for i, label_idx in enumerate(labels[0].tolist()):\n",
    "                    list_boxes[i].append(label_idx)\n",
    "                list_boxes = sorted(list_boxes, key=lambda x: x[0])\n",
    "                predicted_captcha = \"\".join([configs.category_id_labels[i[-1]] for i in list_boxes])\n",
    "                gt_string = \"\".join([configs.category_id_labels[i] for i in labels_gt[idx].tolist()])\n",
    "                print(f\"{gt_string = }\", f\"{predicted_captcha = }\")\n",
    "            # remove the break statement to run fully\n",
    "            break\n",
    "val_generate_captchas_submission(model, val_loader, configs, output_file = \"val_captchas_submission.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, gt_truth, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m                 gt_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([configs\u001b[38;5;241m.\u001b[39mcategory_id_labels[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m labels_gt[idx]\u001b[38;5;241m.\u001b[39mtolist()])\n\u001b[1;32m     24\u001b[0m                 \u001b[38;5;66;03m# print(f\"{gt_string = }\", f\"{predicted_captcha = }\")\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrain_generate_captchas_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_captchas_submission.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m, in \u001b[0;36mtrain_generate_captchas_submission\u001b[0;34m(model, train_loader, configs, output_file)\u001b[0m\n\u001b[1;32m      7\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, gt_truth, labels_gt \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# print(img, gt_truth, labels)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(configs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(images):\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;66;03m# print(image.unsqueeze(0))\u001b[39;00m\n",
      "File \u001b[0;32m/var/lit2425/jenga/suman/pjf/computer_vision/captcha_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/var/lit2425/jenga/suman/pjf/computer_vision/captcha_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/var/lit2425/jenga/suman/pjf/computer_vision/captcha_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/var/lit2425/jenga/suman/pjf/computer_vision/captcha_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/var/lit2425/jenga/suman/pjf/computer_vision/UTN_Captcha_Detector/notebooks/../src_code/data_utils/dataset_utils.py:117\u001b[0m, in \u001b[0;36mCaptchaDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    114\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(preprocessed_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Convert RGBA to RGB\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Load bounding boxes and labels\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_dir:\n",
      "File \u001b[0;32m/var/lit2425/jenga/suman/pjf/computer_vision/captcha_env/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/var/lit2425/jenga/suman/pjf/computer_vision/captcha_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/lit2425/jenga/suman/pjf/computer_vision/captcha_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/var/lit2425/jenga/suman/pjf/computer_vision/captcha_env/lib/python3.10/site-packages/torchvision/transforms/transforms.py:1573\u001b[0m, in \u001b[0;36mGrayscale.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m   1566\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1567\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be converted to grayscale.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Grayscaled image.\u001b[39;00m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrgb_to_grayscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_output_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_output_channels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/lit2425/jenga/suman/pjf/computer_vision/captcha_env/lib/python3.10/site-packages/torchvision/transforms/functional.py:1291\u001b[0m, in \u001b[0;36mrgb_to_grayscale\u001b[0;34m(img, num_output_channels)\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mto_grayscale(img, num_output_channels)\n\u001b[0;32m-> 1291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrgb_to_grayscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_output_channels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/lit2425/jenga/suman/pjf/computer_vision/captcha_env/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:160\u001b[0m, in \u001b[0;36mrgb_to_grayscale\u001b[0;34m(img, num_output_channels)\u001b[0m\n\u001b[1;32m    157\u001b[0m     r, g, b \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39munbind(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# This implementation closely follows the TF one:\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/ops/image_ops_impl.py#L2105-L2138\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     l_img \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;241;43m0.2989\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.587\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.114\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     l_img \u001b[38;5;241m=\u001b[39m l_img\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For Train Dataset\n",
    "\n",
    "def train_generate_captchas_submission(model, train_loader, configs, output_file: str = \"train_captchas_submission.json\"):\n",
    "    \"\"\"\n",
    "    Generates captchas for the set using the pre-loaded model and saves the results in a JSON file.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    with torch.no_grad():\n",
    "        for images, gt_truth, labels_gt in train_loader:\n",
    "            # print(img, gt_truth, labels)\n",
    "            images = images.to(configs.device)\n",
    "            for idx, image in enumerate(images):\n",
    "                # print(image.unsqueeze(0))\n",
    "                loc_preds, cls_preds, _ = model(image.unsqueeze(0))\n",
    "                boxes, labels, scores = model.detect_object(loc_preds, cls_preds, min_score=0.25, max_overlap=0.5,top_k=20)\n",
    "                \n",
    "                list_boxes = boxes[0].tolist()\n",
    "                assert len(list_boxes) == len(labels[0])\n",
    "                for i, label_idx in enumerate(labels[0].tolist()):\n",
    "                    list_boxes[i].append(label_idx)\n",
    "                list_boxes = sorted(list_boxes, key=lambda x: x[0])\n",
    "                predicted_captcha = \"\".join([configs.category_id_labels[i[-1]] for i in list_boxes])\n",
    "                gt_string = \"\".join([configs.category_id_labels[i] for i in labels_gt[idx].tolist()])\n",
    "                # print(f\"{gt_string = }\", f\"{predicted_captcha = }\")\n",
    "\n",
    "train_generate_captchas_submission(model, train_loader, configs, output_file = \"train_captchas_submission.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Test Data\n",
    "\n",
    "def test_generate_captchas_submission(model, test_loader, configs, test_path, output_file: str = \"pjf/computer_vision/UTN_Captcha_Detector/docs_and_results/predicted_jsons/part2_preds.json\"):\n",
    "    \"\"\"\n",
    "    Generates captchas for the test set using the pre-loaded model and saves the results in a JSON file.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Get sorted list fo filenames\n",
    "    filenames = sorted([f for f in os.listdir(test_path) if os.path.isfile(os.path.join(test_path, f))])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, images in enumerate(test_loader):  # No image_paths\n",
    "            images = images.to(configs.device)\n",
    "\n",
    "            for idx, image in enumerate(images):\n",
    "                img_index = batch_idx * len(images) + idx\n",
    "\n",
    "                if batch_idx * len(images) + idx < len(filenames):\n",
    "                    filename = filenames[img_index]\n",
    "                    image_id = os.path.splitext(filename)[0]  # Remove extension\n",
    "                else:\n",
    "                    print(f\"Warning: Missing filename for batch {batch_idx}, index {idx}\")\n",
    "                    continue\n",
    "\n",
    "                # If image is 2D (H, W), add a channel dimension\n",
    "                if image.dim() == 2:\n",
    "                    image = image.unsqueeze(0)  # Make it (1, H, W)\n",
    "\n",
    "                # If image is 3D (C, H, W), add batch dimension\n",
    "                if image.dim() == 3:\n",
    "                    image = image.unsqueeze(0)  # Make it (1, C, H, W)\n",
    "\n",
    "                # Forward pass\n",
    "                loc_preds, cls_preds, _ = model(image)\n",
    "                boxes, labels, scores = model.detect_object(loc_preds, cls_preds, min_score=0.25, max_overlap=0.5, top_k=20)\n",
    "\n",
    "                list_boxes = boxes[0].tolist()\n",
    "                assert len(list_boxes) == len(labels[0])\n",
    "\n",
    "                for i, label_idx in enumerate(labels[0].tolist()):\n",
    "                    list_boxes[i].append(label_idx)\n",
    "\n",
    "                list_boxes = sorted(list_boxes, key=lambda x: x[0])\n",
    "                predicted_captcha = \"\".join([configs.category_id_labels[i[-1]] for i in list_boxes])\n",
    "\n",
    "                results.append({\n",
    "                    \"image_id\": image_id,\n",
    "                    \"captcha_string\": predicted_captcha\n",
    "                })\n",
    "\n",
    "    # Save results to JSON\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "test_generate_captchas_submission(model, test_loader, configs, test_path = \"../datasets/utn_dataset_curated/part2/test/images\", output_file = \"pjf/computer_vision/UTN_Captcha_Detector/docs_and_results/predicted_jsons/part2_preds.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Processed images: ['000001', '000002', '000003', '000004', '000003']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "⚠ Not all images were processed!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 80\u001b[0m\n\u001b[1;32m     75\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(results, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m \u001b[43mtest_generate_captchas_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../datasets/utn_dataset/part3/test/images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpart3_preds.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 71\u001b[0m, in \u001b[0;36mtest_generate_captchas_submission\u001b[0;34m(model, test_loader, configs, test_path, output_file)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Debugging output to verify all images are included\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Processed images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mr\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mresults]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(filenames), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠ Not all images were processed!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Save results to JSON\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mAssertionError\u001b[0m: ⚠ Not all images were processed!"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "\n",
    "def test_generate_captchas_submission(model, test_loader, configs, test_path, output_file=\"test_captchas_submission.json\"):\n",
    "    \"\"\"\n",
    "    Generates captchas for the test set using the pre-loaded model and saves the results in a JSON file.\n",
    "    Image IDs are now taken directly from the filename (without the file extension).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Ensure filenames are sorted numerically\n",
    "    filenames = sorted(\n",
    "        [f for f in os.listdir(test_path) if os.path.isfile(os.path.join(test_path, f))],\n",
    "        key=lambda x: int(os.path.splitext(x)[0])  # Sort based on numeric value\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, images in enumerate(test_loader):\n",
    "            images = images.to(configs.device)\n",
    "\n",
    "            for idx, image in enumerate(images):\n",
    "                # Stop if there are no more filenames left\n",
    "                if idx + batch_idx * len(images) >= len(filenames):\n",
    "                    print(f\"All images processed up to index {idx + batch_idx * len(images)}. Stopping.\")\n",
    "                    break\n",
    "\n",
    "                filename = filenames[idx + batch_idx * len(images)]  # Get file based on current index\n",
    "                file_number = os.path.splitext(filename)[0]  # Extract number without file extension\n",
    "                image_id = file_number  # Use file number as image_id\n",
    "\n",
    "                # print(f\"Processing: {file_number} → Image ID: {image_id}\")  # Debugging output\n",
    "\n",
    "                # Ensure image has correct dimensions\n",
    "                if image.dim() == 2:\n",
    "                    image = image.unsqueeze(0)  # Convert to (1, H, W)\n",
    "                if image.dim() == 3:\n",
    "                    image = image.unsqueeze(0)  # Convert to (1, C, H, W)\n",
    "\n",
    "                # Forward pass through model\n",
    "                loc_preds, cls_preds, _ = model(image)\n",
    "                boxes, labels, scores = model.detect_object(\n",
    "                    loc_preds, cls_preds, min_score=0.15, max_overlap=0.5, top_k=20\n",
    "                )\n",
    "\n",
    "                list_boxes = boxes[0].tolist()\n",
    "                \n",
    "                if len(list_boxes) == 0:\n",
    "                    print(f\"⚠ Warning: No predictions for {image_id}, skipping.\")\n",
    "                    continue  # Skip images with no predictions\n",
    "\n",
    "                assert len(list_boxes) == len(labels[0])\n",
    "\n",
    "                # Append label to box data\n",
    "                for i, label_idx in enumerate(labels[0].tolist()):\n",
    "                    list_boxes[i].append(label_idx)\n",
    "\n",
    "                # Sort boxes left-to-right\n",
    "                list_boxes = sorted(list_boxes, key=lambda x: x[0])\n",
    "\n",
    "                # Convert labels to string\n",
    "                predicted_captcha = \"\".join([configs.category_id_labels[i[-1]] for i in list_boxes])\n",
    "\n",
    "                results.append({\n",
    "                    \"image_id\": image_id,  # Now using the file number directly\n",
    "                    \"captcha_string\": predicted_captcha\n",
    "                })\n",
    "\n",
    "    # Debugging output to verify all images are included\n",
    "    print(f\"📌 Processed images: {[r['image_id'] for r in results]}\")\n",
    "    assert len(results) == len(filenames), \"⚠ Not all images were processed!\"\n",
    "\n",
    "    # Save results to JSON\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "\n",
    "test_generate_captchas_submission(model, test_loader, configs, test_path = \"../datasets/utn_dataset/part3/test/images\", output_file = \"part3_preds.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_generate_captchas_submission' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_generate_captchas_submission\u001b[49m(\n\u001b[1;32m      2\u001b[0m     model, \n\u001b[1;32m      3\u001b[0m     test_loader, \n\u001b[1;32m      4\u001b[0m     configs, \n\u001b[1;32m      5\u001b[0m     test_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../datasets/utn_dataset/part4/test/images\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      6\u001b[0m     output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart4_preds.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_generate_captchas_submission' is not defined"
     ]
    }
   ],
   "source": [
    "test_generate_captchas_submission(\n",
    "    model, \n",
    "    test_loader, \n",
    "    configs, \n",
    "    test_path=\"../datasets/utn_dataset/part4/test/images\", \n",
    "    output_file=\"part4_preds.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 40, 160])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample image\n",
    "image = test_dataset[0]\n",
    "image = image.unsqueeze(0).to(configs.device)\n",
    "print(image.shape)\n",
    "# Model prediction\n",
    "with torch.no_grad():\n",
    "    loc_preds, cls_preds, _ = model(image)\n",
    "    boxes, labels, scores = model.detect_object(loc_preds, cls_preds, min_score=0.25, max_overlap=0.5,top_k=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAACmCAYAAABHlYwjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIKJJREFUeJzt3Xtcj+f/B/DXp6KjikQ5peGblb7b2GJSoUgyazHM/JDTNrMwM2zMMDnMNt+Nr8Nsc5jmu/H9Osww5hQ2ZsyaQw7RFpsViZXCp+v3h0dv10dlaaVyv56Ph8fj9bm7P/d9dfeht+u6r+s2KaUUiIiIyLCsyrsBREREVL5YDBARERkciwEiIiKDYzFARERkcCwGiIiIDI7FABERkcGxGCAiIjI4FgNEREQGx2KAiIjI4FgMEJWBN998EyaT6a72TU9PL+NWVUzbt2+HyWTC9u3bZVv//v3RsGHDcmsTkdGwGKASW7x4MUwmE/bv31/eTakU4uLisHr16lI/bv/+/WEymeSPjY0N6tevj169euHIkSOlfr77Xdu2bS2uZ9WqVeHt7Y0hQ4bg119/Le/mEZUJm/JuANH9aPz48Rg7dqzFtri4OHTv3h1RUVGlfj5bW1ssWrQIAHDjxg2cOnUK8+fPx8aNG3HkyBHUqVOn1M9Zlj788EPk5eWV2/nr1auHadOmAQCuXbuGI0eOYP78+di0aROOHj0KBweHcmsbUVlgMUBUBmxsbGBjc+/+etnY2KBPnz4W21q1aoUuXbpg/fr1GDx48D1rS2moUqVKuZ7fxcWlwPX09vbGsGHDsHv3bnTo0KGcWkZUNjhMQKWqf//+cHJywi+//IIuXbrAyckJdevWxdy5cwEAiYmJaN++PRwdHeHl5YX4+HiL91+8eBGvvPIK/P394eTkBGdnZ0RERODQoUMFzpWSkoKuXbvC0dERtWrVwsiRI7Fp06YC488AsHfvXnTq1AkuLi5wcHBASEgIdu/efcfvRSmFmjVr4uWXX5ZteXl5cHV1hbW1NS5duiTbZ8yYARsbG/z5558ACt4zYDKZkJWVhSVLlkj3c//+/S3Od+nSJfTv3x+urq5wcXFBTEwMsrOz79jGO/Hw8ACAAkVJcnIynn76adSoUQMODg5o1aoV1q9fb7FP/hDQmTNnLLYXNr7ftm1bNGvWDEeOHEG7du3g4OCAunXrYubMmQXalJqaiqioKIufWW5uboH9br9n4MyZMzCZTJg1axYWLlyIRo0awdbWFo899hi+//77Au//4osv4OvrCzs7OzRr1gz/+9///vZ9CEVdz4MHDyIiIgLOzs5wcnJCaGgovvvuO/n61q1bYWVlhTfeeMPiffHx8TCZTJg3b55sO3v2LAYMGIDatWvD1tYWfn5++Pjjjwu05YMPPoCfnx8cHBxQvXp1PProowX+LhHdDfYMUKkzm82IiIhAcHAwZs6cieXLl2PYsGFwdHTE66+/jmeffRbR0dGYP38++vbti8cffxze3t4Abv6iWr16NZ5++ml4e3vj/PnzWLBgAUJCQiy6u7OystC+fXv89ttvGD58ODw8PBAfH49t27YVaM/WrVsRERGBFi1aYOLEibCyssInn3yC9u3bIyEhAQEBAYV+HyaTCYGBgdi5c6ds++mnn5CZmQkrKyvs3r0bkZGRAICEhAQ88sgjcHJyKvRYy5Ytw6BBgxAQEIAhQ4YAABo1amSxT48ePeDt7Y1p06bhwIEDWLRoEWrVqoUZM2YU67rn34BoNpuRnJyMMWPGwM3NDV26dJF9zp8/j9atWyM7OxuxsbFwc3PDkiVL0LVrV6xcuRJPPfVUsc51u4yMDHTq1AnR0dHo0aMHVq5ciTFjxsDf3x8REREAgKtXryI0NBS//PILYmNjUadOHSxbtgxbt24t9nni4+Nx5coVPPfcczCZTJg5cyaio6ORnJwsvQnr169Hz5494e/vj2nTpiEjIwMDBw5E3bp1i30es9ks1/P69es4evQoJk6ciMaNGyMwMFD2O3z4MIKCguDs7IxXX30VVapUwYIFC9C2bVvs2LEDLVu2RPv27TF06FBMmzYNUVFRaN68OX777Te89NJLCAsLw/PPPw/g5s+mVatWMJlMGDZsGNzd3bFhwwYMHDgQly9fxogRIwDcHEKJjY1F9+7dMXz4cOTk5OCnn37C3r170bt372J/j0QWFFEJffLJJwqA+v7772Vbv379FAAVFxcn2zIyMpS9vb0ymUxqxYoVsv3YsWMKgJo4caJsy8nJUWaz2eI8p0+fVra2tmry5Mmy7Z133lEA1OrVq2Xb1atXVdOmTRUAtW3bNqWUUnl5eapJkyYqPDxc5eXlyb7Z2dnK29tbdejQ4Y7f49tvv62sra3V5cuXlVJKvf/++8rLy0sFBASoMWPGKKWUMpvNytXVVY0cOVLeN3HiRHX7Xy9HR0fVr1+/AufI33fAgAEW25966inl5uZ2x/Ypdeua3/6nbt266ocffrDYd8SIEQqASkhIkG1XrlxR3t7eqmHDhnLt83+2p0+ftnj/tm3bLK6vUkqFhIQoAGrp0qWyLTc3V3l4eKhu3brJttmzZysA6vPPP5dtWVlZqnHjxgWO2a9fP+Xl5SWvT58+rQAoNzc3dfHiRdm+Zs0aBUCtW7dOtvn7+6t69eqpK1euyLbt27crABbHLEr+93P7nwcffFAlJydb7BsVFaWqVq2qTp06JdvOnTunqlWrpoKDgwt8n35+fionJ0dFRkYqZ2dnlZKSIvsMHDhQeXp6qvT0dItz9OrVS7m4uKjs7GyllFJPPvmk8vPz+8vvg+hucJiAysSgQYMku7q6wsfHB46OjujRo4ds9/HxgaurK5KTk2Wbra0trKxufizNZjMuXLgAJycn+Pj44MCBA7Lfxo0bUbduXXTt2lW22dnZFRgb//HHH3HixAn07t0bFy5cQHp6OtLT05GVlYXQ0FDs3LnzjjeqBQUFwWw2Y8+ePQBu9gAEBQUhKCgICQkJAICff/4Zly5dQlBQUEkulcj/H6J+7gsXLuDy5ct/+V47Ozts3rwZmzdvxqZNm7BgwQI4OTmhc+fOOH78uOz31VdfISAgAG3atJFtTk5OGDJkCM6cOVPi2QdOTk4WY+xVq1ZFQECAxc/2q6++gqenJ7p37y7bHBwcpKekOHr27Inq1avL6/xrnn+ec+fOITExEX379rXopQkJCYG/v3+xz9OwYUO5nhs2bMDs2bORmZmJiIgIpKWlAbj5+fz6668RFRWFBx54QN7r6emJ3r17Y9euXfKzc3BwwOLFi3H06FEEBwdj/fr1eO+999CgQQMAN4ekVq1ahSeeeAJKKfmcpqenIzw8HJmZmfL5d3V1RWpqaqHDI0QlxWKASp2dnR3c3d0ttrm4uKBevXoF5t67uLggIyNDXufl5eG9995DkyZNYGtri5o1a8Ld3V265/OlpKSgUaNGBY7XuHFji9cnTpwAAPTr1w/u7u4WfxYtWoTc3FyL496uefPmcHBwkF/8+cVAcHAw9u/fj5ycHPma/gu2JPJ/MeTL/6WnX5+iWFtbIywsDGFhYejYsSOGDBmCLVu2IDMzE+PGjZP9UlJS4OPjU+D9Dz74oHy9JAr72VavXt2i7SkpKWjcuHGB/QprT1H+6hrlt//2z0FR24ri6Ogo17NTp04YPnw41q5di6SkJEyfPh0AkJaWhuzs7CKvZ15ensVUxMDAQLzwwgvYt28fwsPDMWDAAPlaWloaLl26hIULFxb4nMbExAAA/vjjDwDAmDFj4OTkhICAADRp0gQvvvjiX97/QvRXeM8AlTpra+u72q6UkhwXF4cJEyZgwIABmDJlCmrUqAErKyuMGDGiRFPN8t/z9ttv4+GHHy50n6LG+YGbd7W3bNkSO3fuxMmTJ/H7778jKCgItWvXxvXr17F3714kJCSgadOmBQqgu1Wc63M36tWrBx8fH4t7HoqrqAWTzGZzodtLu+1FuVfnKUyLFi3g4uJSousJALm5uXLj5alTp5CdnS1TFPM/p3369EG/fv0Kff8///lPADcLjaSkJHz55ZfYuHEjVq1ahX//+9944403MGnSpBK1jYjFAFUoK1euRLt27fDRRx9ZbL906RJq1qwpr728vHDkyBEopSx+cZ08edLiffk36Tk7OyMsLKxEbQoKCsKMGTOwZcsW1KxZE02bNoXJZIKfnx8SEhKQkJBgcZNeUYq7ImFpunHjhsxwAG5et6SkpAL7HTt2TL4O3Poftz5jAih5z0H+sX/++ecCP7PC2vN3zgEU/BwUte1umc1muZ7u7u5wcHAo8npaWVmhfv36sm3ixIk4evQoZs2ahTFjxmDs2LF4//335VjVqlWD2Wwu1ufU0dERPXv2RM+ePXHt2jVER0dj6tSpGDduHOzs7P7290nGw2ECqlCsra0L/C/viy++wNmzZy22hYeH4+zZs1i7dq1sy8nJwYcffmixX4sWLdCoUSPMmjXL4pdivvzx3zsJCgpCbm4uZs+ejTZt2sgvsqCgICxbtgznzp0r1v0Cjo6OBX65lqXjx48jKSkJDz30kGzr3Lkz9u3bh2+//Va2ZWVlYeHChWjYsCF8fX0B3Cqi9P8Fm81mLFy4sMTt6dy5M86dO4eVK1fKtuzs7L91zNvVqVMHzZo1w9KlSy1+3jt27EBiYuLfOva2bdvw559/yvW0trZGx44dsWbNGospmOfPn0d8fDzatGkDZ2dnADents6aNQsjRozAqFGjMHr0aMyZMwc7duyQY3Xr1g2rVq3Czz//XODc+uf0woULFl+rWrUqfH19oZTC9evX/9b3SMbFngGqULp06YLJkycjJiYGrVu3RmJiIpYvX25xgxYAPPfcc5gzZw6eeeYZDB8+HJ6enli+fLn8ryj/F7aVlRUWLVqEiIgI+Pn5ISYmBnXr1sXZs2exbds2ODs7Y926dXds0+OPPw4bGxskJSVZ3OwWHBwsc8SLUwy0aNECW7Zswbvvvos6derA29sbLVu2vKvrU5QbN27g008/BXCzy/nMmTOYP38+8vLyMHHiRNlv7Nix+OyzzxAREYHY2FjUqFEDS5YswenTp7Fq1Sq5edPPzw+tWrXCuHHjcPHiRdSoUQMrVqzAjRs3StzGwYMHY86cOejbty9++OEHeHp6YtmyZaW+ml9cXByefPJJBAYGIiYmBhkZGZgzZw6aNWtWaEFYmMzMTLmeN27cQFJSEubNmwd7e3uLlSXfeustbN68GW3atMHQoUNhY2ODBQsWIDc3V9ZZyMnJQb9+/dCkSRNMnToVADBp0iSsW7cOMTExSExMhKOjI6ZPn45t27ahZcuWGDx4MHx9fXHx4kUcOHAAW7ZswcWLFwEAHTt2hIeHBwIDA1G7dm0cPXoUc+bMQWRkJKpVq1aal5KMpLymMVDlV9TUQkdHxwL7hoSEFDodysvLS0VGRsrrnJwcNWrUKOXp6ans7e1VYGCg+vbbb1VISIgKCQmxeG9ycrKKjIxU9vb2yt3dXY0aNUqtWrVKAVDfffedxb4HDx5U0dHRys3NTdna2iovLy/Vo0cP9c033xTre33ssccUALV3717ZlpqaqgCo+vXrF9i/sKmFx44dU8HBwcre3l4BkGmG+fumpaVZ7F/U9L7bFTa10NnZWYWGhqotW7YU2P/UqVOqe/fuytXVVdnZ2amAgAD15ZdfFrpfWFiYsrW1VbVr11avvfaa2rx5c6FTCwv72d4+PVAppVJSUlTXrl2Vg4ODqlmzpho+fLjauHFjsacWvv322wXOg9umpyql1IoVK1TTpk2Vra2tatasmVq7dq3q1q2batq0aeEXUXP71EKTyaRq1KihunbtWmCqplJKHThwQIWHhysnJyfl4OCg2rVrp/bs2SNfHzlypLK2trb47Cil1P79+5WNjY164YUXZNv58+fViy++qOrXr6+qVKmiPDw8VGhoqFq4cKHss2DBAhUcHCyf5UaNGqnRo0erzMzMv/zeiIpiUuoe3HlDdI/Mnj0bI0eORGpq6l0tMkP3v4cffhju7u7YvHlzeTeFqMLhPQNUaV29etXidU5ODhYsWIAmTZqwEDCw69evFxjO2L59Ow4dOoS2bduWT6OIKjjeM0CVVnR0NBo0aICHH35YxniPHTuG5cuXl3fTqBydPXsWYWFh6NOnD+rUqYNjx45h/vz58PDwKLCwExHdxGKAKq3w8HAsWrQIy5cvh9lshq+vL1asWIGePXuWd9OoHFWvXh0tWrTAokWLkJaWBkdHR0RGRmL69Olwc3Mr7+YRVUi8Z4CIiMjgeM8AERGRwbEYICIiMjgWA0RERAZX7BsIJ0yYUJbtICIiojIwZcqUv9yHPQNEREQGx2KAiIjI4FgMEBERGRyLASIiIoNjMUBERGRwLAaIiIgMjsUAERGRwbEYICIiMjgWA0RERAbHYoCIiMjgir0cMRERFW3KW2+VdxPuaML48eXdhEqnov5My+JnyZ4BIiIig2MxQEREZHAsBoiIiAyO9wwQEZWy4ozp2tnZSQ4KCpKck5Mj+dSpU5KzsrIkZ2ZmFqsdFXXMuzIqyTj9P/7xD8mxsbGSP//8c8nffvut5OvXrxd6nHvxc2TPABERkcGxGCAiIjI4DhMQEZUhk8kk+YEHHpA8duxYyRcvXpS8f/9+yR4eHpLj4+MlT5061eIcRXUvU/mytbWV3Lp1a8nffPON5Llz50qeN2+e5BMnTpRx6yyxZ4CIiMjgWAwQEREZHIcJiIjKUGhoqOT58+dLjoyMlPzyyy9L/v333yUvXrxY8rBhwyRHRUVZnGPXrl2l0VQqZYmJiZI7duwoeciQIZIvXbokec+ePZKnTZt260CTJ5dNAzXsGSAiIjI4FgNEREQGx2ECIqJSZmV16/9Z/v7+kv+hdee/5OQk+UaLFpKPhYdL/tcnn0ieHhcn+cKzz1qcb6h2rGtKlbTZdBd6/Pe/kqtpC0LprM1myW4XLkiers0eCQwMlDx79mzJS5culfzSbcf18vKSnJKSUuw23wl7BoiIiAyOxQAREZHBcZiAiKiUVatWTXLYQw9J/uTKFcmrXntNcq521/kz2rr1H374oWTHZcsk123a1OJ8SnvPnhEjJHv98svdNp2K6fPo6L/cp8XBg5JdL1+WXL16dcmdO3eWrD+n4MCBA0UeNz09vdjtLC72DBARERkciwEiIiKD4zABEVEp02cT1LO3l5zUqpXkmIEDJc+aNEmyo/YIY19fX8ktnZ0lj61Rw+J83c+ckXzSx6eErabSYNJyqx9+kJw4a5bkL/79b8lpaWmS161bJzlOmz0CbfEpwPJx1qWFPQNEREQGx2KAiIjI4DhMQERUyjIyMiTHzJghee3atZK/HjpU8ovaokEOPXtK/lybJfDPQ4ckt8jLszjfY6NHS25Vs2ZJm02loJXW7V9Le/7EGm2mgKenp+RntQWkHBwcJB8/frysmlgo9gwQEREZHIsBIiIig+MwARFVSNbW1pLN2hrvlc2PP/4o+YMPPpD88euvS3ZesEDygdWrJffVZg1YzZ0reePRoxbnWBYcLLnujh2SPypZk6kY7OzsJHfo0EGyQ0SE5Fnao6Y/0Z4z8e6770pesWKFZFWOz5VgzwAREZHBsRggIiIyOBYDREREBsd7Bojonpvy1lvl3YQCJowfXybHzdOmAX46f77k8I8/lrytTx/J/dq3l3z48GHJfzo6Sl592z0Uc555RvLv2v0D0O5LoNLVtm1byVumTpUcWqWK5KBBgySPbdhQcmpqapm2rSTYM0BERGRwLAaIiIgMjsMERFSuiuqeHzBggOR//etfkoO1bvCD2vPiS+JeDFfoD64J/kib7DdqlMRVe/dKjtMeNPTEE09IDtS6n9e9/LLFOb45eVKyt/bQIio7O3fulByjrTr4ba9ekg9+9dU9bdPfwZ4BIiIig2MxQEREZHAcJiCiCmnXrl2Ss7OzJbdu3VrynYYJamoP7HnooYckJyYmllYTi+UR7QFDjU+dkvzbkiWSP9RWW4Szs8RftIcWHXR1lfzQZ59ZnGPAtWuS86z4f7yyUNwhpWjtgUTRZdWYMsBPDRERkcGxGCAiIjK4ezJMUBEXGAHKbpERIvr7UlJSJOsP+2mvLcrTt29fi/c0b95c8pQpUySf0rrnR48efesNsbGl0dQCivNvnlcx7vqvFxgoufUd9qOyYaTfEewZICIiMjgWA0RERAZXLrMJyqLrxWS6tbSHm5ub5KtXr1rsN3bcuFI/NxGVjirXr0uO+u9/JY+tXl3yb+npkl+Ni7N4/28xMZLra/8OTE5Kknw5JKTQc/toC/ckNW58N80GYKwuZbr/sGeAiIjI4FgMEBERGVylXnRIHxp47LHHJI/ThgL0Nc2JSkNFnR1THBW9K7vV/v2SzzdoINl98WLJG7THAM977z2L99ddtEjytogIyXu1xwjvevZZyfrPssuGDZKTXnrpbptOVKmxZ4CIiMjgWAwQEREZXKUbJtCHBvQ1yvXhgEPaWuAzZ860eH9oGbaNjKeid7sDFX9Yw0pbSz+je3fJI7Vr+3/vvis54PJlyVdyciyONeHNNyVvuXBB8jXt8b9FydOfD0BkMOwZICIiMjgWA0RERAZXoYYJ7LQuv85ffy3ZV1swZPuaNZLf0+4k/nLkSMm/P/CA5F7asMLt6qemSv61Xr0StJioeKy1Lmg7OzvJedpd7jna518pdW8aVgHEaAsFvf7665K79+4tefrZs5Jz4uMlf9Chg8WxXLXH/J7Nyir0fN21f0N0a7TZB0RGw54BIiIig2MxQEREZHAsBoiIiAyuQt0z0Oc//5F82M9Psr/2LPL3339f8hpt7C/Lw0Pyf/r0kZyhjSECltOsnvryy1vHff75Era6criX08sqw3S7e8HLy0vy6NGjJT/66KOSc3NzJW/cuFHyqlWrJJ84cULy/XgvQWxsrORU7T6eqVOnSp7y/feSoyZNktz+s88sjnVV+3fjrHafgW7lk09KfigxUfIjP/0kOdnbu1htJ7pfsGeAiIjI4FgMEBERGVyFGib4j7b6mG/79pJzt2yRvHLlSsnvvPOO5P7adC2Hq1cl3z5MoKuqPTvdSGZMny75eW14xN/fv9D9L2gruc2ZM0dycnKy5Iq+yt29UE2bJggA9bWH3exzdpZ8bOtWyePr15c8Upsem6VNi3vllVckr1+/XnLObavvVVadO3eWHLBsmeQ1u3ZJztNWKdxVq5bkvtoQIgC4tmsneU+LFpLXh4UVeiydz8mTd9NsovsKewaIiIgMjsUAERGRwZX7MIH+4KGmbdtKnqTdMdzZ3l5ynDY0oN+JvVbrahykPfv8Qo0aRZ57XadOd93e+4F+Z/u4ceMk19NWYbyqDbWEh4dL/uijjyT31laIIyD0u+8sXh9p2lRyhx9+kHxaW+lunnadU0NvPUZrypQpkhdrn+dp06ZJfld7eE9lHjJ4/oUXCt3efM+euz7WDf3Fvn0SA7RclHQ3t7s+H9H9gj0DREREBsdigIiIyODKZZhAf2hLmzZtJOtd1kuWLJF8rXlzyfrQgK6TPuPgqackH9a6agHLu959jx2TnNSkSbHafj/Qu/fXrVsnWX9IzI0btzpcW7VqJVm/m7259nMhoOGRIxav5z3yiOQBtraSp2uzDPp8+qnkDdoCOIMHD5Y8ZswYyVWrVpWszwTR/74Alg9AqoiKWpjKJTNTcre1ayWbtMWWzNq/H9/37Wvx/n/26HHra9rCTcHa8FZRx1pv0GFDIoA9A0RERIbHYoCIiMjgymWYwNfXV7K+/vjy5cslf6atOX6rk7Rotf/4Q/IRH59itaPp8ePF2u9+YzabJf+hXfO8//s/yfqCTPqMj89sbn1k0rSFXAiwv3zZ4vXX2lr33bp1k+yiDV05aYsL6dLS0iRPnjxZ8ty5cyVnZGRIbq8t0gUAR24bsqgsMl1cJH+sfR6LpH2WAeDwbc8qyHeyOMciMjD2DBARERkciwEiIiKDK5dhgjNnzkgeOnSoZH1oQL+bvTj0BUMa/Pqr5JQGDYp8z6U7PLfgfnZdGwK4rnVf52izMBJfflly7f/9T/JC7dkEU7WFdJ4o9VZWfgsXLpS8afNmySNGjJDcu4h18nWXteEHfTGiHTt2SO6jPbYbACZMmHBXbSUiY2PPABERkcGxGCAiIjK4chkmeHVMceYHFK44j8odtHRpsY61ukuXErejMtPvNHfUZgcs1R5V/P28eZJ9L12SHKvdvR2qraUP7VkSRvWno6PF6yStG19/pkA1rdu/qrb+vq22MFFRi2vpQ2zbt2+XrD8/AgAcb2sLEdGdsGeAiIjI4FgMEBERGdw9GSYoah1yKh9/aAs05Wpr2L+0c6dkszZ8YK3NPpjasKHk3XFxkjeUdiMroeONG1u89jl4ULI+s6CKtmb+bm0BIv3R0vr++gJE165dk3z48GHJAwYMsDi3nZ3dXbWdiIyNPQNEREQGx2KAiIjI4FgMEBERGVy5TC2k8qWPOztoUwX3tWkj+cuQEMmdtmyRPD4lRfL/1axZVk2slHZo1w8Aoteskew7a5bk6/XrS27944+S+7z2muSjR49Kjo+Pl/yrtrpmz549Jaemplqc28/P726aTkQGx54BIiIig2MxQEREZHAcJjAgfZhAty8q6taLjAyJ34SFSW6trQBpbW1d6m2rzHK0FQQBIL5Hj798z3B7e8nBwcGSN2sPNnrllVckV6lSRXJ6errksWPHWhz30KFDxWgxEdFN7BkgIiIyOBYDREREBsdhAgNKSEiQnKUNAVyZOVPypHPnJL9StarkXK2bWj8OlczVq1clb9q0SXKvXr0k16pVS7K9NqzQWFvxkMMCRPR3sGeAiIjI4FgMEBERGRyHCQzo66+/llxv3jzJLw0aJPmq9qCbyVa3asZhNWpIfvXVV28d9PnnS7uZlcIUbXYFEVFlxZ4BIiIig2MxQEREZHAcJjCgiW++WeL3Ljp//tYLgw4NTBg/vrybQERUqtgzQEREZHAsBoiIiAyOwwQGwa5tIiIqCnsGiIiIDI7FABERkcGxGCAiIjI4FgNEREQGx2KAiIjI4FgMEBERGRyLASIiIoNjMUBERGRwLAaIiIgMjsUAERGRwbEYICIiMjgWA0RERAbHYoCIiMjgWAwQEREZHIsBIiIig2MxQEREZHAsBoiIiAyOxQAREZHBmZRSqrwbQUREROWHPQNEREQGx2KAiIjI4FgMEBERGRyLASIiIoNjMUBERGRwLAaIiIgMjsUAERGRwbEYICIiMjgWA0RERAb3/48+4awZ/e/8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert Labels\n",
    "\n",
    "#labels = [category_id_labels[label.item()] for label in labels[0]]\n",
    "\n",
    "def plot_image_with_bboxes(image, bboxes, labels, title=\"Image with Bounding Boxes\"):\n",
    "    img_height, img_width = image.shape[1], image.shape[2] \n",
    "    \n",
    "    # Scale normalized bboxes to absolute pixel values for visualization\n",
    "    bboxes[:, [0, 2]] *= img_width\n",
    "    bboxes[:, [1, 3]] *= img_height\n",
    "\n",
    "    # Convert to integer values for plotting\n",
    "    bboxes_abs = bboxes.to(torch.int)\n",
    "\n",
    "    # Ensure labels are strings\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.tolist()\n",
    "    labels = [str(l) for l in labels]\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    image_with_boxes = draw_bounding_boxes(image, bboxes_abs, labels=labels, colors=\"red\", width=1)\n",
    "\n",
    "    # Image tensor to NumPy for visualization\n",
    "    img = image_with_boxes.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_image_with_bboxes(image.squeeze(0), boxes[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_code.task_utils.evaluation import levenshtein\n",
    "def generate_edit_distance(model, val_loader, configs):\n",
    "        \"\"\"\n",
    "        Generates captchas for the test set using the pre-loaded model and saves the results in a JSON file.\n",
    "        \"\"\"\n",
    "        edit_distances = []\n",
    "        with torch.no_grad():\n",
    "            for images, gt_truth, labels_gt in val_loader:\n",
    "                images = images.to(configs.device)\n",
    "                for idx, image in enumerate(images):\n",
    "                    # print(image.unsqueeze(0))\n",
    "                    loc_preds, cls_preds, _ = model(image.unsqueeze(0))\n",
    "                    boxes, labels, scores = model.detect_object(loc_preds, cls_preds, min_score=0.25, max_overlap=0.5,top_k=20)\n",
    "                    \n",
    "                    list_boxes = boxes[0].tolist()\n",
    "                    assert len(list_boxes) == len(labels[0])\n",
    "                    for i, label_idx in enumerate(labels[0].tolist()):\n",
    "                        list_boxes[i].append(label_idx)\n",
    "                    list_boxes = sorted(list_boxes, key=lambda x: x[0])\n",
    "                    predicted_captcha = \"\".join([configs.category_id_labels[i[-1]] for i in list_boxes])\n",
    "                    gt_string = \"\".join([configs.category_id_labels[i] for i in labels_gt[idx].tolist()])\n",
    "                    edit_distance = levenshtein(gt_string, predicted_captcha)\n",
    "                    edit_distances.append(edit_distance)\n",
    "        mean_edit_distance, captcha_count = np.mean(np.array(edit_distances)), len(edit_distances)\n",
    "        return mean_edit_distance.item(), captcha_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CaptchaDataset\n",
      "    Number of datapoints: 20000\n",
      "    Root location: ../datasets/utn_dataset/part2/val/images\n",
      "Dataset CaptchaDataset\n",
      "    Number of datapoints: 20000\n",
      "    Root location: ../datasets/utn_dataset/part2/test/images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset = CaptchaDataset(\n",
    "    configs.test_preprocessed_dir,\n",
    "    labels_dir=None,\n",
    "    augment=False,\n",
    "    config=configs,\n",
    "    img_transform=get_rectangle_img_transform(configs)\n",
    ")\n",
    "\n",
    "val_dataset = CaptchaDataset(\n",
    "    configs.val_preprocessed_dir,\n",
    "    labels_dir=configs.val_labels_dir,\n",
    "    augment=False,\n",
    "    config=configs,\n",
    "    img_transform=get_rectangle_img_transform(configs)\n",
    ")\n",
    "\n",
    "print(val_dataset)\n",
    "\n",
    "print(test_dataset)\n",
    "val_loader = get_dataloader(val_dataset, configs)\n",
    "train_loader = get_dataloader(train_dataset, configs)\n",
    "test_loader = get_dataloader(test_dataset, configs)\n",
    "mean_edit_distance, captcha_count = generate_edit_distance(model, val_loader, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6645 captcha_count = 20000\n"
     ]
    }
   ],
   "source": [
    "print(mean_edit_distance, f\"{captcha_count = }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
