{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ee10b8-389a-4787-82c1-4277b7b7c559",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "934d956e-e7b0-4e6f-8a46-45fe77badfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "## other standard packages\n",
    "import os\n",
    "import sys\n",
    "## Env variables and preparation stuffs\n",
    "sys.path.insert(0, \"../\")\n",
    "from src_code.task_utils.config_parser import ConfigParser\n",
    "configs_dict = {\n",
    "    \"data_configs\": {\n",
    "        \"train_path\": \"datasets/utn_dataset_curated/part2/train\",\n",
    "        \"val_path\": \"datasets/utn_dataset_curated/part2/val\",\n",
    "        \"test_path\": \"datasets/utn_dataset_curated/part2/test\",\n",
    "        \"preprocessing_related\": {\n",
    "            \"mean\": 0.5,  # for raw_image normalisation\n",
    "            \"std\": 0.5,  # for raw_image normalisation\n",
    "            \"downscale_factor\": 4,\n",
    "        },\n",
    "        \"dataset_related\": {\n",
    "            \"preprocessed_dir\": \"../datasets/utn_dataset_curated/part2/train/preprocessed\",\n",
    "            \"labels_dir\": \"../datasets/utn_dataset_curated/part2/train/labels\",\n",
    "            \"augment\": True,\n",
    "            \"shuffle\": False,\n",
    "        },\n",
    "        \"augmentation_related\": {\n",
    "            \"flip_prob\": 0,\n",
    "            \"scale_range\": (0.8, 1.2),\n",
    "            \"zoom_prob\": 1,\n",
    "            \"saturation_prob\": 0\n",
    "        },\n",
    "    },\n",
    "    \"model_configs\": {\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 32,\n",
    "        \"device\": \"cuda\",  # either \"cpu\" or \"cuda\"\n",
    "        \"checkpoint\": None,\n",
    "        \"backbone\": {\n",
    "            \"name\": \"VGG16\",\n",
    "            \"num_stages\": 6,\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"alpha\": 1,  # loss = alpha*loc_loss + cls_loss\n",
    "            \"pos_box_threshold\": 0.5,  # a default box is marked positive if it has (> pos_box_threshold) IoU score with any of the groundtruth boxes\n",
    "            \"hard_neg_pos\": 3,  # num of negative boxes = hard_neg_pos * num_positive_boxes\n",
    "        },\n",
    "        \"optim\": {\n",
    "            \"name\": \"SGD\",\n",
    "            \"lr\": 0.001,\n",
    "            \"momentum\": 0.9,\n",
    "            \"weight_decay\": 0.0005,\n",
    "        },\n",
    "        \"scheduler\": {\n",
    "            \"name\": \"MultiStepLR\",\n",
    "            \"milestones\": [155, 195],\n",
    "            \"gamma\": 0.1,\n",
    "        },\n",
    "    },\n",
    "    \"task_configs\": {\n",
    "        \"img_height\": 160,  # original image height\n",
    "        \"img_width\": 640,  # original image width\n",
    "        \"debug\": True,  # if True will display a lot of intermediate information for debugging purposes\n",
    "        \"log_expt\": False,  # whether to log the experiment online or not\n",
    "        \"num_classes\": 37,  # A-Z(26), 0-9(10), background(1)\n",
    "        \"min_cls_score\": 0.01,  # if the cls score for a bounding box is less than this, it is considered as background\n",
    "        \"nms_iou_score\": 0.1,  # if the iou between two bounding boxes is less than this, it is suppressed\n",
    "    },\n",
    "}\n",
    "# hyperparameters\n",
    "preprocessed_dir = \"../datasets/utn_dataset_curated/part2/train/preprocessed\"\n",
    "labels_dir = \"../datasets/utn_dataset_curated/part2/train/labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41000c80-0aee-4d58-9fad-11a4c2344aa8",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "16d1639a-4f80-4cbe-aa48-da4f30787063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "2234bd15-102f-427f-92c7-76aa39894f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config = {'data_configs': {'train_path': 'datasets/utn_dataset_curated/part2/train', 'val_path': 'datasets/utn_dataset_curated/part2/val', 'test_path': 'datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/preprocessed', 'labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0, 'scale_range': (0.8, 1.2), 'zoom_prob': 1, 'saturation_prob': 0}}, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'checkpoint': None, 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}\n",
      "{'config_dict': {'data_configs': {'train_path': 'datasets/utn_dataset_curated/part2/train', 'val_path': 'datasets/utn_dataset_curated/part2/val', 'test_path': 'datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/preprocessed', 'labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0, 'scale_range': (0.8, 1.2), 'zoom_prob': 1, 'saturation_prob': 0}}, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'checkpoint': None, 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}, 'train_path': 'datasets/utn_dataset_curated/part2/train', 'downscale_factor': 4, 'preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/preprocessed', 'labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'augment': True, 'shuffle': False, 'flip_prob': 0, 'scale_range': (0.8, 1.2), 'zoom_prob': 1, 'saturation_prob': 0, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'checkpoint': None, 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'checkpoint': None, 'device': 'cuda', 'print_freq': None, 'batch_size': 32, 'epochs': 1, 'pos_box_threshold': 0.5, 'neg_pos_hard_mining': 3, 'alpha': 1, 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005, 'debug': True, 'num_classes': 37, 'img_height': 160, 'img_width': 640}\n",
      "config.pos_box_threshold = 0.5\n"
     ]
    }
   ],
   "source": [
    "config = ConfigParser(configs_dict).get_parser()\n",
    "print(config.__dict__)  # Check all attributes in config\n",
    "\n",
    "# this object can be used as follows:\n",
    "print(f\"{config.pos_box_threshold = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0ffc95-8219-423d-89e4-a5fc4dbbe8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: change to relative import using a dot (.) in datautils line 4 & 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0cb7d-b2d9-466b-81d9-dec4f9ae9974",
   "metadata": {},
   "source": [
    "# 0. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "ded66662-0ef2-48a9-adda-6e2ace0d8c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../', '../', '../', '../', '../', '../', '../', '../', '../', '../', '../', '../', '../', '../', '../', '../', '../', '../', '../', '../', '../', '../', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python311.zip', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/lib-dynload', '', '/Users/lucaheller/Desktop/UTN_Captcha_Detector/.venv/lib/python3.11/site-packages', '/var/folders/ph/4ppm8f8950g15_ff9t0m53k80000gn/T/tmp6h8mv_hx']\n",
      "Image Shape: torch.Size([1, 40, 160])\n",
      "Bounding Boxes: tensor([[0.3793, 0.0688, 0.6011, 0.9485]])\n",
      "Labels: tensor([24])\n",
      "Batch Image Shape: torch.Size([5, 1, 40, 160])\n",
      "Bounding Boxes (First Image): tensor([[0.0669, 0.5308, 0.1555, 1.0000],\n",
      "        [0.3584, 0.0000, 0.5609, 0.5765],\n",
      "        [0.4941, 0.1833, 0.6922, 0.9927]])\n",
      "Labels (First Image): tensor([21,  3, 24])\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)\n",
    "from src_code.data_utils.dataset_utils import CaptchaDataset, get_dataloader\n",
    "from src_code.data_utils.augmentation import Augmentations\n",
    "# if the preprocessed dataset is not available, run create it using src_code/data_utils/preprocessing.py\n",
    "\n",
    "# Create dataset\n",
    "dataset = CaptchaDataset(config)\n",
    "\n",
    "# Load a sample\n",
    "image, bboxes, labels = dataset[0]\n",
    "\n",
    "print(\"Image Shape:\", image.shape)\n",
    "print(\"Bounding Boxes:\", bboxes)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = get_dataloader(dataset, config)\n",
    "\n",
    "# Load a single batch\n",
    "images, bboxes, labels = next(iter(dataloader))\n",
    "\n",
    "# Print batch info\n",
    "print(f\"Batch Image Shape: {images.shape}\")\n",
    "print(f\"Bounding Boxes (First Image): {bboxes[0]}\")\n",
    "print(f\"Labels (First Image): {labels[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "1a528098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: torch.Size([1, 40, 160])\n",
      "Bounding Boxes: tensor([[0.1459, 0.1422, 0.3600, 1.0000],\n",
      "        [0.7948, 0.0000, 1.0000, 0.8455]])\n",
      "Labels: tensor([21,  0])\n",
      "BBoxes for Visualization: tensor([[ 23,   5,  57,  40],\n",
      "        [127,   0, 160,  33]], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# dataset = CaptchaDataset(preprocessed_dir, labels_dir, downscale_factor=False, augment=True)\n",
    "# Load one sample for visualization\n",
    "image, bboxes, labels = dataset[0]\n",
    "\n",
    "print(\"Image Shape:\", image.shape)\n",
    "print(\"Bounding Boxes:\", bboxes)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "\n",
    "def plot_image_with_bboxes(image, bboxes, labels, title=\"Image with Bounding Boxes\"):\n",
    "    img_height, img_width = image.shape[1], image.shape[2] \n",
    "    \n",
    "    # Scale normalized bboxes to absolute pixel values for visualization\n",
    "    # TODO: --> * 4 used for non flipped images: works\n",
    "    # Issue with flipped ones\n",
    "    # How to test: set flip prob to one and you will see :)\n",
    "    bboxes[:, [0, 2]] *= img_width\n",
    "    bboxes[:, [1, 3]] *= img_height\n",
    "\n",
    "    # Convert to integer values for plotting\n",
    "    bboxes_abs = bboxes.to(torch.int)\n",
    "\n",
    "    print(\"BBoxes for Visualization:\", bboxes_abs)\n",
    "\n",
    "    # Ensure labels are strings\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.tolist()\n",
    "    labels = [str(l) for l in labels]\n",
    "\n",
    "    # TODO: Image to RGB\n",
    "\n",
    "    # Draw bboxes\n",
    "    image_with_boxes = draw_bounding_boxes(image, bboxes_abs, labels=labels, colors=\"red\", width=2)\n",
    "\n",
    "    # image tensor to NumPy for visualization\n",
    "    img = image_with_boxes.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_image_with_bboxes(image, bboxes, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ac567-c663-4d04-a301-c4e2b1ff9f8b",
   "metadata": {},
   "source": [
    "# 1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8609000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irene\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\irene\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD(\n",
      "  (backbone): VGG16Backbone(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "    )\n",
      "    (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (auxiliary_convs): Sequential(\n",
      "    (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (loc_head): Conv2d(1024, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (cls_head): Conv2d(1024, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from src_code.model_utils.backbone import VGG16Backbone\n",
    "from src_code.model_utils.ssd import SSD\n",
    "\n",
    "# Initialize the SSD model\n",
    "model = SSD(num_classes=config.num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d19ab-526e-4e66-a4f7-7d0eb73c92fb",
   "metadata": {},
   "source": [
    "## 1.1 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab12065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16Backbone(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (5): ReLU()\n",
      "  (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (7): ReLU()\n",
      ")\n",
      "Conv2d(1024, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(1024, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "print(model.backbone)\n",
    "print(model.auxiliary_convs)\n",
    "print(model.loc_head)\n",
    "print(model.cls_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d6389-fe7a-4df7-82bd-9e98ead66501",
   "metadata": {},
   "source": [
    "## 1.2 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f044187b-ba69-4376-b2a3-3caf57388979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irene\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "from src_code.model_utils.loss import MultiBoxLoss\n",
    "default_boxes = torch.Tensor([[0.5000, 0.1250, 0.5000, 0.1250],[0.5000, 0.3750, 0.5000, 0.1250],\n",
    "        [0.5000, 0.6250, 0.5000, 0.1250],[0.5000, 0.8750, 0.5000, 0.1250],\n",
    "        [0.5000, 0.1250, 0.5000, 0.0625],[0.5000, 0.3750, 0.5000, 0.0625],\n",
    "        [0.5000, 0.6250, 0.5000, 0.0625],[0.5000, 0.8750, 0.5000, 0.0625]])\n",
    "# (1, 8, 4)\n",
    "locs_pred = torch.Tensor([[[0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.]]])\n",
    "\n",
    "# (1, 8, 36)\n",
    "cls_pred = torch.Tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
    "boxes = torch.Tensor([[[ 61,  36, 139, 115], [297,  10, 386,  98], [509,  26, 572,  90]]])\n",
    "labels = torch.Tensor([[21,  0, 33]])\n",
    "\n",
    "# calculate loss\n",
    "mbl = MultiBoxLoss(default_boxes, config)\n",
    "loss, debug_info = mbl(locs_pred, cls_pred, boxes, labels)\n",
    "expected_loss = 10.8635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29716e21-60a9-4382-b9ae-8fbab55fe563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(291.2195)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bd0c178-b35c-4d61-ad64-0e103d358d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overlap_gt_def_boxes': [tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]])],\n",
       " 'db_for_each_obj': [tensor([0, 0, 0])],\n",
       " 'db_indices_for_each_obj': [tensor([0, 0, 0])],\n",
       " 'overlap_value_for_each_db': tensor([1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'self.label_each_db': [tensor([33., 36., 36., 36., 36., 36., 36., 36.])],\n",
       " 'match': [tensor([ True, False, False, False, False, False, False, False])],\n",
       " 'gt_locs': [tensor([[  19.0000, 1071.0000,    5.8158,   12.6685],\n",
       "          [  27.7500,  170.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  150.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  130.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  380.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  340.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  300.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  260.0000,    6.8686,   17.2021]]),\n",
       "  tensor([[  19.0000, 1071.0000,    5.8158,   12.6685],\n",
       "          [  27.7500,  170.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  150.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  130.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  380.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  340.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  300.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  260.0000,    6.8686,   17.2021]])],\n",
       " 'num_images': 1,\n",
       " 'loc_loss': tensor(276.6211),\n",
       " 'n_positive': tensor([1]),\n",
       " 'n_hard_negatives': tensor([3]),\n",
       " 'gt_label_each_default_box': tensor([33, 36, 36, 36, 36, 36, 36, 36]),\n",
       " 'conf_loss_for_each_default_box': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'confidence_pos_loss': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'hard_negatives': tensor([[ True,  True,  True, False, False, False, False, False]]),\n",
       " 'conf_neg_loss': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'ce_loss': tensor(14.5984),\n",
       " 'ce_hard_neg_loss': tensor(10.9488),\n",
       " 'ce_pos_loss': tensor(3.6496),\n",
       " 'loss': tensor(291.2195)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44dd71d-fcde-495d-bfda-a326f44eba56",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f831262d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config = {'data_configs': {'train_path': 'datasets/utn_dataset_curated/part2/train', 'val_path': 'datasets/utn_dataset_curated/part2/val', 'test_path': 'datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/preprocessed', 'labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0.5, 'scale_range': (0.8, 1.2)}}, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'checkpoint': None, 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}\n",
      "Using device: cpu\n",
      "Default boxes shape: torch.Size([8, 4])\n",
      "Number of batches in train_loader: 1\n",
      "config attributes: ['_ConfigParser__verify__argparse', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'alpha', 'augment', 'batch_size', 'config_dict', 'debug', 'device', 'downscale_factor', 'epochs', 'flip_prob', 'get_config', 'get_parser', 'img_height', 'img_width', 'labels_dir', 'lr', 'momentum', 'neg_pos_hard_mining', 'num_classes', 'pos_box_threshold', 'preprocessed_dir', 'print_freq', 'scale_range', 'shuffle', 'train_path', 'weight_decay']\n",
      "config.pos_box_threshold: 0.5\n",
      "Epoch 1/1:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [5, 8] at index 1 does not match the shape of the indexed tensor [5, 80, 4] at index 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\irene\\Documents\\UTN\\CV\\groupAssignment\\code\\UTN_Captcha_Detector\\notebooks\\..\\src_code\\model_utils\\train_utils.py:65\u001b[0m, in \u001b[0;36mCaptchaTrainer.train_step\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     63\u001b[0m loc_pred, cls_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(images)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m loss, debug_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     68\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\irene\\Documents\\UTN\\CV\\groupAssignment\\code\\UTN_Captcha_Detector\\notebooks\\..\\src_code\\model_utils\\loss.py:144\u001b[0m, in \u001b[0;36mMultiBoxLoss.forward\u001b[1;34m(self, locs_pred, cls_pred, boxes, labels, downscale_factor)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Localization loss is computed only over positive default boxes\u001b[39;00m\n\u001b[0;32m    143\u001b[0m smooth_L1_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSmoothL1Loss()\n\u001b[1;32m--> 144\u001b[0m loc_loss \u001b[38;5;241m=\u001b[39m smooth_L1_loss(\u001b[43mlocs_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos_db\u001b[49m\u001b[43m]\u001b[49m, gt_locs[pos_db])\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m    146\u001b[0m     debug_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m loc_loss\n",
      "\u001b[1;31mIndexError\u001b[0m: The shape of the mask [5, 8] at index 1 does not match the shape of the indexed tensor [5, 80, 4] at index 1"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from src_code.model_utils.train_utils import CaptchaTrainer\n",
    "\n",
    "config = ConfigParser(configs_dict)\n",
    "config = config.get_parser()\n",
    "\n",
    "# Automatically detect if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ensure `config.device` is updated correctly\n",
    "config.device = device\n",
    "\n",
    "# Move model to the correct device\n",
    "model.to(config.device)\n",
    "print(f\"Using device: {config.device}\")\n",
    "\n",
    "# Ensure default boxes are defined\n",
    "print(f\"Default boxes shape: {default_boxes.shape}\")  # Check before passing\n",
    "\n",
    "# Ensure dataloader has data\n",
    "assert len(dataloader) > 0, \"Error: Training dataloader is empty!\"\n",
    "print(f\"Number of batches in train_loader: {len(dataloader)}\")\n",
    "\n",
    "print(f\"config attributes: {dir(config)}\")  # Check attributes exist\n",
    "print(f\"config.pos_box_threshold: {config.pos_box_threshold}\")  # Ensure it's defined\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = SGD(\n",
    "    model.parameters(),\n",
    "    lr=config.lr,  # Use object attribute instead of dictionary key\n",
    "    momentum=config.momentum,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = MultiBoxLoss(default_boxes, config)\n",
    "\n",
    "# Create trainer\n",
    "trainer = CaptchaTrainer(\n",
    "    model=model,\n",
    "    train_loader=dataloader,\n",
    "    val_loader=None,  # You can set a validation loader if needed\n",
    "    test_loader=None,  # You can set a test loader if needed\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "num_epochs = config.epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    trainer.train_step(epoch)\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddccfc6-5820-4c8c-9e14-5262389d34ac",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
