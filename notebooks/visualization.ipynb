{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ee10b8-389a-4787-82c1-4277b7b7c559",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "934d956e-e7b0-4e6f-8a46-45fe77badfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "## other standard packages\n",
    "import os\n",
    "import sys\n",
    "## Env variables and preparation stuffs\n",
    "sys.path.insert(0, \"../\")\n",
    "from src_code.task_utils.config_parser import ConfigParser\n",
    "configs_dict = {\n",
    "    \"task\": \"train\",  # 'train', 'evaluate' or 'sweep'\n",
    "    \"data_configs\": {\n",
    "        \"train_path\": \"../datasets/utn_dataset_curated/part2/train\",\n",
    "        \"val_path\": \"../datasets/utn_dataset_curated/part2/val\",\n",
    "        \"test_path\": \"../datasets/utn_dataset_curated/part2/test\",\n",
    "        \"preprocessing_related\": {\n",
    "            \"mean\": 0.5,  # for raw_image normalisation\n",
    "            \"std\": 0.5,  # for raw_image normalisation\n",
    "            \"downscale_factor\": 4,\n",
    "        },\n",
    "        \"dataset_related\": {\n",
    "            \"train_preprocessed_dir\": \"../datasets/utn_dataset_curated/part2/train/images/\",\n",
    "            \"val_preprocessed_dir\": \"../datasets/utn_dataset_curated/part2/val/images\",\n",
    "            \"test_preprocessed_dir\": \"../datasets/utn_dataset_curated/part2/test/images\",\n",
    "            \"train_labels_dir\": \"../datasets/utn_dataset_curated/part2/train/labels\",\n",
    "            \"val_labels_dir\": \"../datasets/utn_dataset_curated/part2/val/labels\",\n",
    "            \"augment\": True,\n",
    "            \"shuffle\": False,\n",
    "        },\n",
    "        \"augmentation_related\": {\n",
    "            \"flip_prob\": 0,\n",
    "            \"zoom_prob\": 0,\n",
    "            \"rotation_prob\": 0,\n",
    "            \"line_prob\": 0,\n",
    "            \"salt_pepper_prob\": 1,\n",
    "        },\n",
    "    },\n",
    "    \"model_configs\": {\n",
    "        \"name\": \"ssd_mnist\",  # \"ssd_mnist\" or \"ssd_captcha\"\n",
    "        \"save_checkpoint\": False,\n",
    "        \"log_gradients\": False,\n",
    "        \"checkpoint\": None,  # Training from Scratch\n",
    "        \"print_freq\": 500,\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 2,\n",
    "        \"device\": \"cuda\",  # either \"cpu\" or \"cuda\"\n",
    "        \"backbone\": {\n",
    "            \"name\": \"VGG16\",\n",
    "            \"num_stages\": 6,\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"alpha\": 0.25,  # loss = alpha*loc_loss + cls_loss\n",
    "            \"pos_box_threshold\": 0.5,  # a default box is marked positive if it has (> pos_box_threshold) IoU score with any of the groundtruth boxes\n",
    "            \"hard_neg_pos\": 3,  # num of negative boxes = hard_neg_pos * num_positive_boxes\n",
    "        },\n",
    "        \"optim\": {\n",
    "            \"name\": \"SGD\",\n",
    "            \"lr\": 0.001,\n",
    "            \"momentum\": 0.9,\n",
    "            \"weight_decay\": 0.0005,\n",
    "            \"clip_grad\": None,\n",
    "        },\n",
    "        \"scheduler\": {\n",
    "            \"name\": \"LinearLR\",  # 'MultiStepLR' or 'LinearLR'\n",
    "            \"milestones\": [10, 20],  # lr reduction whenever training hits the given epoch milestone\n",
    "            \"gamma\": 0.1,  # the factor by which the learning rate decreases\n",
    "            \"start_factor\": 0.5,  # lr multiplicative factor increases from (start_factor to 1) in total_iter steps\n",
    "            \"total_iter\": 4,\n",
    "        },\n",
    "    },\n",
    "    \"task_configs\": {\n",
    "        \"img_height\": 160,  # original image height\n",
    "        \"img_width\": 640,  # original image width\n",
    "        \"debug\": True,  # if True will display a lot of intermediate information for debugging purposes\n",
    "        \"log_expt\": False,  # whether to log the experiment online or not\n",
    "        \"num_classes\": 37,  # A-Z(26), 0-9(10), background(1)\n",
    "        \"min_cls_score\": 0.01,  # if the cls score for a bounding box is less than this, it is considered as background\n",
    "        \"nms_iou_score\": 0.1,  # if the iou between two bounding boxes is less than this, it is suppressed\n",
    "    },\n",
    "}\n",
    "\n",
    "# hyperparameters\n",
    "preprocessed_dir = \"../datasets/utn_dataset_curated/part2/train/preprocessed\"\n",
    "labels_dir = \"../datasets/utn_dataset_curated/part2/train/labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce50ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41000c80-0aee-4d58-9fad-11a4c2344aa8",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "16d1639a-4f80-4cbe-aa48-da4f30787063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2234bd15-102f-427f-92c7-76aa39894f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config = {'task': 'train', 'data_configs': {'train_path': '../datasets/utn_dataset_curated/part2/train', 'val_path': '../datasets/utn_dataset_curated/part2/val', 'test_path': '../datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'train_preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/images/', 'val_preprocessed_dir': '../datasets/utn_dataset_curated/part2/val/images', 'test_preprocessed_dir': '../datasets/utn_dataset_curated/part2/test/images', 'train_labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'val_labels_dir': '../datasets/utn_dataset_curated/part2/val/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0, 'zoom_prob': 0, 'rotation_prob': 0, 'line_prob': 0, 'salt_pepper_prob': 1}}, 'model_configs': {'name': 'ssd_mnist', 'save_checkpoint': False, 'log_gradients': False, 'checkpoint': None, 'print_freq': 500, 'epochs': 10, 'batch_size': 2, 'device': 'cuda', 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 0.25, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005, 'clip_grad': None}, 'scheduler': {'name': 'LinearLR', 'milestones': [10, 20], 'gamma': 0.1, 'start_factor': 0.5, 'total_iter': 4}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}\n",
      "{'config_dict': {'task': 'train', 'data_configs': {'train_path': '../datasets/utn_dataset_curated/part2/train', 'val_path': '../datasets/utn_dataset_curated/part2/val', 'test_path': '../datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'train_preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/images/', 'val_preprocessed_dir': '../datasets/utn_dataset_curated/part2/val/images', 'test_preprocessed_dir': '../datasets/utn_dataset_curated/part2/test/images', 'train_labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'val_labels_dir': '../datasets/utn_dataset_curated/part2/val/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0, 'zoom_prob': 0, 'rotation_prob': 0, 'line_prob': 0, 'salt_pepper_prob': 1}}, 'model_configs': {'name': 'ssd_mnist', 'save_checkpoint': False, 'log_gradients': False, 'checkpoint': None, 'print_freq': 500, 'epochs': 10, 'batch_size': 2, 'device': 'cuda', 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 0.25, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005, 'clip_grad': None}, 'scheduler': {'name': 'LinearLR', 'milestones': [10, 20], 'gamma': 0.1, 'start_factor': 0.5, 'total_iter': 4}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}, 'task': 'train', 'train_path': '../datasets/utn_dataset_curated/part2/train', 'downscale_factor': 4, 'color': True, 'train_preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/images/', 'val_preprocessed_dir': '../datasets/utn_dataset_curated/part2/val/images', 'test_preprocessed_dir': '../datasets/utn_dataset_curated/part2/test/images', 'train_labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'val_labels_dir': '../datasets/utn_dataset_curated/part2/val/labels', 'augment': True, 'shuffle': False, 'flip_prob': 0, 'zoom_prob': 0, 'rotation_prob': 0, 'line_prob': 0, 'salt_pepper_prob': 1, 'model_name': 'ssd_mnist', 'log_gradients': False, 'model_configs': {'name': 'ssd_mnist', 'save_checkpoint': False, 'log_gradients': False, 'checkpoint': None, 'print_freq': 500, 'epochs': 10, 'batch_size': 2, 'device': 'cuda', 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 0.25, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005, 'clip_grad': None}, 'scheduler': {'name': 'LinearLR', 'milestones': [10, 20], 'gamma': 0.1, 'start_factor': 0.5, 'total_iter': 4}}, 'checkpoint': None, 'device': 'cuda', 'print_freq': 500, 'batch_size': 2, 'epochs': 10, 'scheduler_name': 'LinearLR', 'multistep_milestones': [10, 20], 'multistep_gamma': 0.1, 'linearLR_start_factor': 0.5, 'linearLR_total_iter': 4, 'pos_box_threshold': 0.5, 'neg_pos_hard_mining': 3, 'alpha': 0.25, 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005, 'clip_grad': None, 'debug': True, 'log_expt': False, 'num_classes': 37, 'img_height': 160, 'img_width': 640}\n",
      "config.pos_box_threshold = 0.5\n"
     ]
    }
   ],
   "source": [
    "config = ConfigParser(configs_dict).get_parser()\n",
    "print(config.__dict__)  # Check all attributes in config\n",
    "\n",
    "# this object can be used as follows:\n",
    "print(f\"{config.pos_box_threshold = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0ffc95-8219-423d-89e4-a5fc4dbbe8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: change to relative import using a dot (.) in datautils line 4 & 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0cb7d-b2d9-466b-81d9-dec4f9ae9974",
   "metadata": {},
   "source": [
    "# 0. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ded66662-0ef2-48a9-adda-6e2ace0d8c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lucaheller/Desktop/UTN_Captcha_Detector/notebooks\n",
      "Image Shape: <built-in method size of Tensor object at 0x1591febd0>\n",
      "Bounding Boxes: tensor([[0.1430, 0.4818, 0.2001, 0.8585],\n",
      "        [0.3307, 0.0000, 0.4611, 0.5103],\n",
      "        [0.4181, 0.2645, 0.5456, 0.7704],\n",
      "        [0.7742, 0.3091, 0.8777, 0.8153]])\n",
      "Labels: tensor([21,  3, 24,  4])\n",
      "Batch Image Shape: torch.Size([2, 1, 40, 160])\n",
      "Bounding Boxes (First Image): tensor([[0.1430, 0.4818, 0.2001, 0.8585],\n",
      "        [0.3307, 0.0000, 0.4611, 0.5103],\n",
      "        [0.4181, 0.2645, 0.5456, 0.7704],\n",
      "        [0.7742, 0.3091, 0.8777, 0.8153]])\n",
      "Labels (First Image): tensor([21,  3, 24,  4])\n"
     ]
    }
   ],
   "source": [
    "#print(sys.path)\n",
    "from src_code.data_utils.dataset_utils import CaptchaDataset, get_dataloader\n",
    "from src_code.data_utils.preprocessing import get_img_transform, get_rectangle_img_transform\n",
    "from src_code.data_utils.augmentation import Augmentations\n",
    "# if the preprocessed dataset is not available, run create it using src_code/data_utils/preprocessing.py\n",
    "\n",
    "current_path = os.getcwd()\n",
    "print(current_path)\n",
    "\n",
    "# Create dataset\n",
    "dataset = CaptchaDataset(config.train_preprocessed_dir, config.train_labels_dir, augment=True, config=config, img_transform=get_rectangle_img_transform(config))\n",
    "\n",
    "# Load a sample\n",
    "image, bboxes, labels = dataset[0]\n",
    "\n",
    "print(\"Image Shape:\", image.size)\n",
    "print(\"Bounding Boxes:\", bboxes)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = get_dataloader(dataset, config)\n",
    "\n",
    "# Load a single batch\n",
    "images, bboxes, labels = next(iter(dataloader))\n",
    "\n",
    "# Print batch info\n",
    "print(f\"Batch Image Shape: {images.shape}\")\n",
    "print(f\"Bounding Boxes (First Image): {bboxes[0]}\")\n",
    "print(f\"Labels (First Image): {labels[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1a528098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBoxes for Visualization: tensor([[ 22,  19,  32,  34],\n",
      "        [ 52,   0,  73,  20],\n",
      "        [ 66,  10,  87,  30],\n",
      "        [123,  12, 140,  32]], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# dataset = CaptchaDataset(preprocessed_dir, labels_dir, downscale_factor=False, augment=True)\n",
    "# Load one sample for visualization\n",
    "image, bboxes, labels = dataset[0]\n",
    "\n",
    "#print(\"Image Shape:\", image.shape)\n",
    "#print(\"Bounding Boxes:\", bboxes)\n",
    "#print(\"Labels:\", labels)\n",
    "\n",
    "\n",
    "def plot_image_with_bboxes(image, bboxes, labels, title=\"Image with Bounding Boxes\"):\n",
    "    img_height, img_width = image.shape[-2], image.shape[-1] \n",
    "    \n",
    "    # Scale normalized bboxes to absolute pixel values for visualization\n",
    "    bboxes[:, [0, 2]] *= img_width\n",
    "    bboxes[:, [1, 3]] *= img_height\n",
    "\n",
    "    # Convert to integer values for plotting\n",
    "    bboxes_abs = bboxes.to(torch.int)\n",
    "\n",
    "    print(\"BBoxes for Visualization:\", bboxes_abs)\n",
    "\n",
    "    # Ensure labels are strings\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.tolist()\n",
    "    labels = [str(l) for l in labels]\n",
    "\n",
    "    # Draw bboxes\n",
    "    image_with_boxes = draw_bounding_boxes(image, bboxes_abs, labels=labels, colors=\"red\", width=2)\n",
    "\n",
    "    # image tensor to NumPy for visualization\n",
    "    img = image_with_boxes.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_image_with_bboxes(image, bboxes, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ac567-c663-4d04-a301-c4e2b1ff9f8b",
   "metadata": {},
   "source": [
    "# 1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d8609000",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SSD' from 'src_code.model_utils.ssd' (/Users/lucaheller/Desktop/UTN_Captcha_Detector/notebooks/../src_code/model_utils/ssd.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc_code\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VGG16Backbone\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc_code\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mssd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SSD\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the SSD model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m SSD(num_classes\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_classes)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SSD' from 'src_code.model_utils.ssd' (/Users/lucaheller/Desktop/UTN_Captcha_Detector/notebooks/../src_code/model_utils/ssd.py)"
     ]
    }
   ],
   "source": [
    "from src_code.model_utils.backbone import VGG16Backbone\n",
    "from src_code.model_utils.ssd import SSD\n",
    "\n",
    "# Initialize the SSD model\n",
    "model = SSD(num_classes=config.num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d19ab-526e-4e66-a4f7-7d0eb73c92fb",
   "metadata": {},
   "source": [
    "## 1.1 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab12065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16Backbone(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (5): ReLU()\n",
      "  (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (7): ReLU()\n",
      ")\n",
      "Conv2d(1024, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(1024, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "print(model.backbone)\n",
    "print(model.auxiliary_convs)\n",
    "print(model.loc_head)\n",
    "print(model.cls_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d6389-fe7a-4df7-82bd-9e98ead66501",
   "metadata": {},
   "source": [
    "## 1.2 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f044187b-ba69-4376-b2a3-3caf57388979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irene\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "from src_code.model_utils.loss import MultiBoxLoss\n",
    "default_boxes = torch.Tensor([[0.5000, 0.1250, 0.5000, 0.1250],[0.5000, 0.3750, 0.5000, 0.1250],\n",
    "        [0.5000, 0.6250, 0.5000, 0.1250],[0.5000, 0.8750, 0.5000, 0.1250],\n",
    "        [0.5000, 0.1250, 0.5000, 0.0625],[0.5000, 0.3750, 0.5000, 0.0625],\n",
    "        [0.5000, 0.6250, 0.5000, 0.0625],[0.5000, 0.8750, 0.5000, 0.0625]])\n",
    "# (1, 8, 4)\n",
    "locs_pred = torch.Tensor([[[0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.]]])\n",
    "\n",
    "# (1, 8, 36)\n",
    "cls_pred = torch.Tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
    "boxes = torch.Tensor([[[ 61,  36, 139, 115], [297,  10, 386,  98], [509,  26, 572,  90]]])\n",
    "labels = torch.Tensor([[21,  0, 33]])\n",
    "\n",
    "# calculate loss\n",
    "mbl = MultiBoxLoss(default_boxes, config)\n",
    "loss, debug_info = mbl(locs_pred, cls_pred, boxes, labels)\n",
    "expected_loss = 10.8635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29716e21-60a9-4382-b9ae-8fbab55fe563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(291.2195)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bd0c178-b35c-4d61-ad64-0e103d358d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overlap_gt_def_boxes': [tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]])],\n",
       " 'db_for_each_obj': [tensor([0, 0, 0])],\n",
       " 'db_indices_for_each_obj': [tensor([0, 0, 0])],\n",
       " 'overlap_value_for_each_db': tensor([1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'self.label_each_db': [tensor([33., 36., 36., 36., 36., 36., 36., 36.])],\n",
       " 'match': [tensor([ True, False, False, False, False, False, False, False])],\n",
       " 'gt_locs': [tensor([[  19.0000, 1071.0000,    5.8158,   12.6685],\n",
       "          [  27.7500,  170.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  150.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  130.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  380.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  340.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  300.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  260.0000,    6.8686,   17.2021]]),\n",
       "  tensor([[  19.0000, 1071.0000,    5.8158,   12.6685],\n",
       "          [  27.7500,  170.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  150.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  130.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  380.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  340.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  300.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  260.0000,    6.8686,   17.2021]])],\n",
       " 'num_images': 1,\n",
       " 'loc_loss': tensor(276.6211),\n",
       " 'n_positive': tensor([1]),\n",
       " 'n_hard_negatives': tensor([3]),\n",
       " 'gt_label_each_default_box': tensor([33, 36, 36, 36, 36, 36, 36, 36]),\n",
       " 'conf_loss_for_each_default_box': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'confidence_pos_loss': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'hard_negatives': tensor([[ True,  True,  True, False, False, False, False, False]]),\n",
       " 'conf_neg_loss': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'ce_loss': tensor(14.5984),\n",
       " 'ce_hard_neg_loss': tensor(10.9488),\n",
       " 'ce_pos_loss': tensor(3.6496),\n",
       " 'loss': tensor(291.2195)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44dd71d-fcde-495d-bfda-a326f44eba56",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f831262d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config = {'data_configs': {'train_path': 'datasets/utn_dataset_curated/part2/train', 'val_path': 'datasets/utn_dataset_curated/part2/val', 'test_path': 'datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/preprocessed', 'labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0.5, 'scale_range': (0.8, 1.2)}}, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'checkpoint': None, 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}\n",
      "Using device: cpu\n",
      "Default boxes shape: torch.Size([8, 4])\n",
      "Number of batches in train_loader: 1\n",
      "config attributes: ['_ConfigParser__verify__argparse', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'alpha', 'augment', 'batch_size', 'config_dict', 'debug', 'device', 'downscale_factor', 'epochs', 'flip_prob', 'get_config', 'get_parser', 'img_height', 'img_width', 'labels_dir', 'lr', 'momentum', 'neg_pos_hard_mining', 'num_classes', 'pos_box_threshold', 'preprocessed_dir', 'print_freq', 'scale_range', 'shuffle', 'train_path', 'weight_decay']\n",
      "config.pos_box_threshold: 0.5\n",
      "Epoch 1/1:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [5, 8] at index 1 does not match the shape of the indexed tensor [5, 80, 4] at index 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\irene\\Documents\\UTN\\CV\\groupAssignment\\code\\UTN_Captcha_Detector\\notebooks\\..\\src_code\\model_utils\\train_utils.py:65\u001b[0m, in \u001b[0;36mCaptchaTrainer.train_step\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     63\u001b[0m loc_pred, cls_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(images)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m loss, debug_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     68\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\irene\\Documents\\UTN\\CV\\groupAssignment\\code\\UTN_Captcha_Detector\\notebooks\\..\\src_code\\model_utils\\loss.py:144\u001b[0m, in \u001b[0;36mMultiBoxLoss.forward\u001b[1;34m(self, locs_pred, cls_pred, boxes, labels, downscale_factor)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Localization loss is computed only over positive default boxes\u001b[39;00m\n\u001b[0;32m    143\u001b[0m smooth_L1_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSmoothL1Loss()\n\u001b[1;32m--> 144\u001b[0m loc_loss \u001b[38;5;241m=\u001b[39m smooth_L1_loss(\u001b[43mlocs_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos_db\u001b[49m\u001b[43m]\u001b[49m, gt_locs[pos_db])\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m    146\u001b[0m     debug_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m loc_loss\n",
      "\u001b[1;31mIndexError\u001b[0m: The shape of the mask [5, 8] at index 1 does not match the shape of the indexed tensor [5, 80, 4] at index 1"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from src_code.model_utils.train_utils import CaptchaTrainer\n",
    "\n",
    "config = ConfigParser(configs_dict)\n",
    "config = config.get_parser()\n",
    "\n",
    "# Automatically detect if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ensure `config.device` is updated correctly\n",
    "config.device = device\n",
    "\n",
    "# Move model to the correct device\n",
    "model.to(config.device)\n",
    "print(f\"Using device: {config.device}\")\n",
    "\n",
    "# Ensure default boxes are defined\n",
    "print(f\"Default boxes shape: {default_boxes.shape}\")  # Check before passing\n",
    "\n",
    "# Ensure dataloader has data\n",
    "assert len(dataloader) > 0, \"Error: Training dataloader is empty!\"\n",
    "print(f\"Number of batches in train_loader: {len(dataloader)}\")\n",
    "\n",
    "print(f\"config attributes: {dir(config)}\")  # Check attributes exist\n",
    "print(f\"config.pos_box_threshold: {config.pos_box_threshold}\")  # Ensure it's defined\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = SGD(\n",
    "    model.parameters(),\n",
    "    lr=config.lr,  # Use object attribute instead of dictionary key\n",
    "    momentum=config.momentum,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = MultiBoxLoss(default_boxes, config)\n",
    "\n",
    "# Create trainer\n",
    "trainer = CaptchaTrainer(\n",
    "    model=model,\n",
    "    train_loader=dataloader,\n",
    "    val_loader=None,  # You can set a validation loader if needed\n",
    "    test_loader=None,  # You can set a test loader if needed\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "num_epochs = config.epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    trainer.train_step(epoch)\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddccfc6-5820-4c8c-9e14-5262389d34ac",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
