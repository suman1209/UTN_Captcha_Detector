{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ee10b8-389a-4787-82c1-4277b7b7c559",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "934d956e-e7b0-4e6f-8a46-45fe77badfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "## other standard packages\n",
    "import os\n",
    "import sys\n",
    "## Env variables and preparation stuffs\n",
    "sys.path.insert(0, \"../\")\n",
    "from src_code.task_utils.config_parser import ConfigParser\n",
    "configs_dict = {\n",
    "    \"data_configs\": {\n",
    "        \"train_path\": \"datasets/utn_dataset_curated/part2/train\",\n",
    "        \"val_path\": \"datasets/utn_dataset_curated/part2/val\",\n",
    "        \"test_path\": \"datasets/utn_dataset_curated/part2/test\",\n",
    "        \"preprocessing\": {\n",
    "            \"mean\": 0.5,  # for raw_image normalisation\n",
    "            \"std\": 0.5,  # for raw_image normalisation\n",
    "            \"downscale_factor\": 4,\n",
    "        },\n",
    "        \"augmentation_related\": {\n",
    "            \"place_holder\": None,  # this is just a place holder, will be filled later\n",
    "        },\n",
    "    },\n",
    "    \"model_configs\": {\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 32,\n",
    "        \"device\": \"cuda\",  # either \"cpu\" or \"cuda\"\n",
    "        \"backbone\": {\n",
    "            \"name\": \"VGG16\",\n",
    "            \"num_stages\": 6,\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"alpha\": 1,  # loss = alpha*loc_loss + cls_loss\n",
    "            \"pos_box_threshold\": 0.5,  # a default box is marked positive if it has (> pos_box_threshold) IoU score with any of the groundtruth boxes\n",
    "            \"hard_neg_pos\": 3,  # num of negative boxes = hard_neg_pos * num_positive_boxes\n",
    "        },\n",
    "        \"optim\": {\n",
    "            \"name\": \"SGD\",\n",
    "            \"lr\": 0.001,\n",
    "            \"momentum\": 0.9,\n",
    "            \"weight_decay\": 0.0005,\n",
    "        },\n",
    "        \"scheduler\": {\n",
    "            \"name\": \"MultiStepLR\",\n",
    "            \"milestones\": [155, 195],\n",
    "            \"gamma\": 0.1,\n",
    "        },\n",
    "    },\n",
    "    \"task_configs\": {\n",
    "        \"img_height\": 160,  # original image height\n",
    "        \"img_width\": 640,  # original image width\n",
    "        \"debug\": True,  # if True will display a lot of intermediate information for debugging purposes\n",
    "        \"log_expt\": False,  # whether to log the experiment online or not\n",
    "        \"num_classes\": 37,  # A-Z(26), 0-9(10), background(1)\n",
    "        \"min_cls_score\": 0.01,  # if the cls score for a bounding box is less than this, it is considered as background\n",
    "        \"nms_iou_score\": 0.1,  # if the iou between two bounding boxes is less than this, it is suppressed\n",
    "    },\n",
    "}\n",
    "# hyperparameters\n",
    "preprocessed_dir = \"../datasets/utn_dataset_curated/part2/train/preprocessed\"\n",
    "labels_dir = \"../datasets/utn_dataset_curated/part2/train/labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41000c80-0aee-4d58-9fad-11a4c2344aa8",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16d1639a-4f80-4cbe-aa48-da4f30787063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2234bd15-102f-427f-92c7-76aa39894f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config = {'data_configs': {'train_path': 'datasets/utn_dataset_curated/part2/train', 'val_path': 'datasets/utn_dataset_curated/part2/val', 'test_path': 'datasets/utn_dataset_curated/part2/test', 'preprocessing': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'augmentation_related': {'place_holder': None}}, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}\n",
      "config.pos_box_threshold = 0.5\n"
     ]
    }
   ],
   "source": [
    "config = ConfigParser(configs_dict).get_parser()\n",
    "# this object can be used as follows:\n",
    "print(f\"{config.pos_box_threshold = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0ffc95-8219-423d-89e4-a5fc4dbbe8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: change to relative import using a dot (.) in datautils line 4 & 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0cb7d-b2d9-466b-81d9-dec4f9ae9974",
   "metadata": {},
   "source": [
    "# 0. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ded66662-0ef2-48a9-adda-6e2ace0d8c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../', '../', '../', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python311.zip', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/lib-dynload', '', '/Users/lucaheller/Desktop/UTN_Captcha_Detector/.venv/lib/python3.11/site-packages', '/var/folders/ph/4ppm8f8950g15_ff9t0m53k80000gn/T/tmpvb0033_g']\n",
      "Image Shape: torch.Size([1, 40, 160])\n",
      "Bounding Boxes: tensor([[ 0.0358,  0.1204,  0.0500,  0.2146],\n",
      "        [ 0.0827, -0.0028,  0.1153,  0.1276],\n",
      "        [ 0.1045,  0.0661,  0.1364,  0.1926],\n",
      "        [ 0.1936,  0.0773,  0.2194,  0.2038]])\n",
      "Labels: tensor([21,  3, 24,  4])\n",
      "Batch Image Shape: torch.Size([4, 1, 40, 160])\n",
      "Bounding Boxes (First Image): tensor([[0.0239, 0.0571, 0.0543, 0.1802],\n",
      "        [0.1161, 0.0160, 0.1511, 0.1538],\n",
      "        [0.1989, 0.0420, 0.2237, 0.1413]])\n",
      "Labels (First Image): tensor([21,  0, 33])\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)\n",
    "from src_code.data_utils.dataset_utils import CaptchaDataset, get_dataloader\n",
    "from src_code.data_utils.augmentation import Augmentations\n",
    "# if the preprocessed dataset is not available, run create it using src_code/data_utils/preprocessing.py\n",
    "\n",
    "# Create dataset\n",
    "dataset = CaptchaDataset(preprocessed_dir, labels_dir, downscale_factor=4, augment=True)\n",
    "\n",
    "# Load a sample\n",
    "image, bboxes, labels = dataset[0]\n",
    "\n",
    "print(\"Image Shape:\", image.shape)\n",
    "print(\"Bounding Boxes:\", bboxes)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = get_dataloader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Load a single batch\n",
    "images, bboxes, labels = next(iter(dataloader))\n",
    "\n",
    "# Print batch info\n",
    "print(f\"Batch Image Shape: {images.shape}\")\n",
    "print(f\"Bounding Boxes (First Image): {bboxes[0]}\")\n",
    "print(f\"Labels (First Image): {labels[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a528098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: torch.Size([1, 40, 160])\n",
      "Bounding Boxes: tensor([[ 0.0358,  0.1204,  0.0500,  0.2146],\n",
      "        [ 0.0827, -0.0028,  0.1153,  0.1276],\n",
      "        [ 0.1045,  0.0661,  0.1364,  0.1926],\n",
      "        [ 0.1936,  0.0773,  0.2194,  0.2038]])\n",
      "Labels: tensor([21,  3, 24,  4])\n",
      "BBoxes for Visualization: tensor([[ 22,  19,  32,  34],\n",
      "        [ 52,   0,  73,  20],\n",
      "        [ 66,  10,  87,  30],\n",
      "        [123,  12, 140,  32]], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dataset = CaptchaDataset(preprocessed_dir, labels_dir, downscale_factor=4, augment=True)\n",
    "# Load one sample for visualization\n",
    "image, bboxes, labels = dataset[0]\n",
    "\n",
    "print(\"Image Shape:\", image.shape)\n",
    "print(\"Bounding Boxes:\", bboxes)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "\n",
    "def plot_image_with_bboxes(image, bboxes, labels, title=\"Image with Bounding Boxes\"):\n",
    "    img_height, img_width = image.shape[1], image.shape[2] \n",
    "    \n",
    "    # Scale normalized bboxes to absolute pixel values for visualization\n",
    "    # TODO: --> * 4 used for non flipped images: works\n",
    "    # Issue with flipped ones\n",
    "    # How to test: set flip prob to one and you will see :)\n",
    "    bboxes[:, [0, 2]] *= img_width * 4\n",
    "    bboxes[:, [1, 3]] *= img_height * 4\n",
    "\n",
    "    # Convert to integer values for plotting\n",
    "    bboxes_abs = bboxes.to(torch.int)\n",
    "\n",
    "    print(\"BBoxes for Visualization:\", bboxes_abs)\n",
    "\n",
    "    # Ensure labels are strings\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.tolist()\n",
    "    labels = [str(l) for l in labels]\n",
    "\n",
    "    # TODO: Image to RGB\n",
    "\n",
    "    # Draw bboxes\n",
    "    image_with_boxes = draw_bounding_boxes(image, bboxes_abs, labels=labels, colors=\"red\", width=2)\n",
    "\n",
    "    # image tensor to NumPy for visualization\n",
    "    img = image_with_boxes.permute(1, 2, 0).numpy()\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_image_with_bboxes(image, bboxes, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ac567-c663-4d04-a301-c4e2b1ff9f8b",
   "metadata": {},
   "source": [
    "# 1. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d19ab-526e-4e66-a4f7-7d0eb73c92fb",
   "metadata": {},
   "source": [
    "## 1.1 Model Architecture ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d6389-fe7a-4df7-82bd-9e98ead66501",
   "metadata": {},
   "source": [
    "## 1.2 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f044187b-ba69-4376-b2a3-3caf57388979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_code.model_utils.loss import MultiBoxLoss\n",
    "default_boxes = torch.Tensor([[0.5000, 0.1250, 0.5000, 0.1250],[0.5000, 0.3750, 0.5000, 0.1250],\n",
    "        [0.5000, 0.6250, 0.5000, 0.1250],[0.5000, 0.8750, 0.5000, 0.1250],\n",
    "        [0.5000, 0.1250, 0.5000, 0.0625],[0.5000, 0.3750, 0.5000, 0.0625],\n",
    "        [0.5000, 0.6250, 0.5000, 0.0625],[0.5000, 0.8750, 0.5000, 0.0625]])\n",
    "# (1, 8, 4)\n",
    "locs_pred = torch.Tensor([[[0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.]]])\n",
    "\n",
    "# (1, 8, 36)\n",
    "cls_pred = torch.Tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
    "boxes = torch.Tensor([[[ 61,  36, 139, 115], [297,  10, 386,  98], [509,  26, 572,  90]]])\n",
    "labels = torch.Tensor([[21,  0, 33]])\n",
    "\n",
    "# calculate loss\n",
    "mbl = MultiBoxLoss(default_boxes, config)\n",
    "loss, debug_info = mbl(locs_pred, cls_pred, boxes, labels)\n",
    "expected_loss = 10.8635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29716e21-60a9-4382-b9ae-8fbab55fe563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(291.2195)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bd0c178-b35c-4d61-ad64-0e103d358d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overlap_gt_def_boxes': [tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]])],\n",
       " 'db_for_each_obj': [tensor([0, 0, 0])],\n",
       " 'db_indices_for_each_obj': [tensor([0, 0, 0])],\n",
       " 'overlap_value_for_each_db': tensor([1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'self.label_each_db': [tensor([33., 36., 36., 36., 36., 36., 36., 36.])],\n",
       " 'match': [tensor([ True, False, False, False, False, False, False, False])],\n",
       " 'gt_locs': [tensor([[  19.0000, 1071.0000,    5.8158,   12.6685],\n",
       "          [  27.7500,  170.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  150.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  130.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  380.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  340.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  300.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  260.0000,    6.8686,   17.2021]]),\n",
       "  tensor([[  19.0000, 1071.0000,    5.8158,   12.6685],\n",
       "          [  27.7500,  170.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  150.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  130.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  380.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  340.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  300.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  260.0000,    6.8686,   17.2021]])],\n",
       " 'num_images': 1,\n",
       " 'loc_loss': tensor(276.6211),\n",
       " 'n_positive': tensor([1]),\n",
       " 'n_hard_negatives': tensor([3]),\n",
       " 'gt_label_each_default_box': tensor([33, 36, 36, 36, 36, 36, 36, 36]),\n",
       " 'conf_loss_for_each_default_box': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'confidence_pos_loss': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'hard_negatives': tensor([[ True,  True,  True, False, False, False, False, False]]),\n",
       " 'conf_neg_loss': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'ce_loss': tensor(14.5984),\n",
       " 'ce_hard_neg_loss': tensor(10.9488),\n",
       " 'ce_pos_loss': tensor(3.6496),\n",
       " 'loss': tensor(291.2195)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44dd71d-fcde-495d-bfda-a326f44eba56",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddccfc6-5820-4c8c-9e14-5262389d34ac",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
