{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ee10b8-389a-4787-82c1-4277b7b7c559",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934d956e-e7b0-4e6f-8a46-45fe77badfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "## other standard packages\n",
    "import os\n",
    "import sys\n",
    "## Env variables and preparation stuffs\n",
    "sys.path.insert(0, \"../\")\n",
    "from src_code.task_utils.config_parser import ConfigParser\n",
    "configs_dict = {\n",
    "    \"data_configs\": {\n",
    "        \"train_path\": \"datasets/utn_dataset_curated/part2/train\",\n",
    "        \"val_path\": \"datasets/utn_dataset_curated/part2/val\",\n",
    "        \"test_path\": \"datasets/utn_dataset_curated/part2/test\",\n",
    "        \"preprocessing\": {\n",
    "            \"mean\": 0.5,  # for raw_image normalisation\n",
    "            \"std\": 0.5,  # for raw_image normalisation\n",
    "            \"downscale_factor\": 4,\n",
    "        },\n",
    "        \"augmentation_related\": {\n",
    "            \"place_holder\": None,  # this is just a place holder, will be filled later\n",
    "        },\n",
    "    },\n",
    "    \"model_configs\": {\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 32,\n",
    "        \"device\": \"cuda\",  # either \"cpu\" or \"cuda\"\n",
    "        \"backbone\": {\n",
    "            \"name\": \"VGG16\",\n",
    "            \"num_stages\": 6,\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"alpha\": 1,  # loss = alpha*loc_loss + cls_loss\n",
    "            \"pos_box_threshold\": 0.5,  # a default box is marked positive if it has (> pos_box_threshold) IoU score with any of the groundtruth boxes\n",
    "            \"hard_neg_pos\": 3,  # num of negative boxes = hard_neg_pos * num_positive_boxes\n",
    "        },\n",
    "        \"optim\": {\n",
    "            \"name\": \"SGD\",\n",
    "            \"lr\": 0.001,\n",
    "            \"momentum\": 0.9,\n",
    "            \"weight_decay\": 0.0005,\n",
    "        },\n",
    "        \"scheduler\": {\n",
    "            \"name\": \"MultiStepLR\",\n",
    "            \"milestones\": [155, 195],\n",
    "            \"gamma\": 0.1,\n",
    "        },\n",
    "    },\n",
    "    \"task_configs\": {\n",
    "        \"img_height\": 160,  # original image height\n",
    "        \"img_width\": 640,  # original image width\n",
    "        \"debug\": True,  # if True will display a lot of intermediate information for debugging purposes\n",
    "        \"log_expt\": False,  # whether to log the experiment online or not\n",
    "        \"num_classes\": 37,  # A-Z(26), 0-9(10), background(1)\n",
    "        \"min_cls_score\": 0.01,  # if the cls score for a bounding box is less than this, it is considered as background\n",
    "        \"nms_iou_score\": 0.1,  # if the iou between two bounding boxes is less than this, it is suppressed\n",
    "    },\n",
    "}\n",
    "# hyperparameters\n",
    "preprocessed_dir = \"../datasets/utn_dataset_curated/part2/train/preprocessed\"\n",
    "labels_dir = \"../datasets/utn_dataset_curated/part2/train/labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41000c80-0aee-4d58-9fad-11a4c2344aa8",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d1639a-4f80-4cbe-aa48-da4f30787063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2234bd15-102f-427f-92c7-76aa39894f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config = {'data_configs': {'train_path': 'datasets/utn_dataset_curated/part2/train', 'val_path': 'datasets/utn_dataset_curated/part2/val', 'test_path': 'datasets/utn_dataset_curated/part2/test', 'preprocessing': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'augmentation_related': {'place_holder': None}}, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}\n",
      "config.pos_box_threshold = 0.5\n"
     ]
    }
   ],
   "source": [
    "config = ConfigParser(configs_dict).get_parser()\n",
    "# this object can be used as follows:\n",
    "print(f\"{config.pos_box_threshold = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0ffc95-8219-423d-89e4-a5fc4dbbe8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: change to relative import using a dot (.) in datautils line 4 & 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0cb7d-b2d9-466b-81d9-dec4f9ae9974",
   "metadata": {},
   "source": [
    "# 0. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded66662-0ef2-48a9-adda-6e2ace0d8c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python311.zip', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/lib-dynload', '', '/Users/lucaheller/Desktop/UTN_Captcha_Detector/.venv/lib/python3.11/site-packages', '/var/folders/ph/4ppm8f8950g15_ff9t0m53k80000gn/T/tmpgneq9_lk']\n",
      "Image Shape: torch.Size([1, 40, 160])\n",
      "Bounding Boxes: tensor([[ 0.7999,  0.4818,  0.8570,  0.8585],\n",
      "        [ 0.5389, -0.0112,  0.6693,  0.5103],\n",
      "        [ 0.4544,  0.2645,  0.5819,  0.7704],\n",
      "        [ 0.1223,  0.3091,  0.2258,  0.8153]])\n",
      "Labels: tensor([21,  3, 24,  4])\n",
      "Batch Image Shape: torch.Size([4, 1, 40, 160])\n",
      "Bounding Boxes (First Image): tensor([[0.7710, 0.2338, 0.9068, 0.7828],\n",
      "        [0.4378, 0.3101, 0.5362, 0.7893],\n",
      "        [0.1053, 0.1216, 0.2537, 0.7186]])\n",
      "Labels (First Image): tensor([34,  6, 34])\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)\n",
    "from src_code.data_utils.dataset_utils import CaptchaDataset, get_dataloader\n",
    "from src_code.data_utils.augmentation import Augmentations\n",
    "# if the preprocessed dataset is not available, run create it using src_code/data_utils/preprocessing.py\n",
    "\n",
    "# Create dataset\n",
    "dataset = CaptchaDataset(preprocessed_dir, labels_dir, downscale_factor=4, augment=True)\n",
    "\n",
    "# Load a sample\n",
    "image, bboxes, labels = dataset[0]\n",
    "\n",
    "print(\"Image Shape:\", image.shape)\n",
    "print(\"Bounding Boxes:\", bboxes)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = get_dataloader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Load a single batch\n",
    "images, bboxes, labels = next(iter(dataloader))\n",
    "\n",
    "# Print batch info\n",
    "print(f\"Batch Image Shape: {images.shape}\")\n",
    "print(f\"Bounding Boxes (First Image): {bboxes[0]}\")\n",
    "print(f\"Labels (First Image): {labels[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a528098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: torch.Size([1, 40, 160])\n",
      "Bounding Boxes: tensor([[ 0.7999,  0.4818,  0.8570,  0.8585],\n",
      "        [ 0.5389, -0.0112,  0.6693,  0.5103],\n",
      "        [ 0.4544,  0.2645,  0.5819,  0.7704],\n",
      "        [ 0.1223,  0.3091,  0.2258,  0.8153]])\n",
      "Labels: tensor([21,  3, 24,  4])\n",
      "BBoxes for Visualization: tensor([[127,  19, 137,  34],\n",
      "        [ 86,   0, 107,  20],\n",
      "        [ 72,  10,  93,  30],\n",
      "        [ 19,  12,  36,  32]], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# dataset = CaptchaDataset(preprocessed_dir, labels_dir, downscale_factor=False, augment=True)\n",
    "# Load one sample for visualization\n",
    "image, bboxes, labels = dataset[0]\n",
    "\n",
    "print(\"Image Shape:\", image.shape)\n",
    "print(\"Bounding Boxes:\", bboxes)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "\n",
    "def plot_image_with_bboxes(image, bboxes, labels, title=\"Image with Bounding Boxes\"):\n",
    "    img_height, img_width = image.shape[1], image.shape[2] \n",
    "    \n",
    "    # Scale normalized bboxes to absolute pixel values for visualization\n",
    "    # TODO: --> * 4 used for non flipped images: works\n",
    "    # Issue with flipped ones\n",
    "    # How to test: set flip prob to one and you will see :)\n",
    "    bboxes[:, [0, 2]] *= img_width\n",
    "    bboxes[:, [1, 3]] *= img_height\n",
    "\n",
    "    # Convert to integer values for plotting\n",
    "    bboxes_abs = bboxes.to(torch.int)\n",
    "\n",
    "    print(\"BBoxes for Visualization:\", bboxes_abs)\n",
    "\n",
    "    # Ensure labels are strings\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.tolist()\n",
    "    labels = [str(l) for l in labels]\n",
    "\n",
    "    # TODO: Image to RGB\n",
    "\n",
    "    # Draw bboxes\n",
    "    image_with_boxes = draw_bounding_boxes(image, bboxes_abs, labels=labels, colors=\"red\", width=2)\n",
    "\n",
    "    # image tensor to NumPy for visualization\n",
    "    img = image_with_boxes.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_image_with_bboxes(image, bboxes, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ac567-c663-4d04-a301-c4e2b1ff9f8b",
   "metadata": {},
   "source": [
    "# 1. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d19ab-526e-4e66-a4f7-7d0eb73c92fb",
   "metadata": {},
   "source": [
    "## 1.1 Model Architecture ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d6389-fe7a-4df7-82bd-9e98ead66501",
   "metadata": {},
   "source": [
    "## 1.2 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f044187b-ba69-4376-b2a3-3caf57388979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_code.model_utils.loss import MultiBoxLoss\n",
    "default_boxes = torch.Tensor([[0.5000, 0.1250, 0.5000, 0.1250],[0.5000, 0.3750, 0.5000, 0.1250],\n",
    "        [0.5000, 0.6250, 0.5000, 0.1250],[0.5000, 0.8750, 0.5000, 0.1250],\n",
    "        [0.5000, 0.1250, 0.5000, 0.0625],[0.5000, 0.3750, 0.5000, 0.0625],\n",
    "        [0.5000, 0.6250, 0.5000, 0.0625],[0.5000, 0.8750, 0.5000, 0.0625]])\n",
    "# (1, 8, 4)\n",
    "locs_pred = torch.Tensor([[[0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.]]])\n",
    "\n",
    "# (1, 8, 36)\n",
    "cls_pred = torch.Tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
    "boxes = torch.Tensor([[[ 61,  36, 139, 115], [297,  10, 386,  98], [509,  26, 572,  90]]])\n",
    "labels = torch.Tensor([[21,  0, 33]])\n",
    "\n",
    "# calculate loss\n",
    "mbl = MultiBoxLoss(default_boxes, config)\n",
    "loss, debug_info = mbl(locs_pred, cls_pred, boxes, labels)\n",
    "expected_loss = 10.8635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29716e21-60a9-4382-b9ae-8fbab55fe563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(291.2195)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bd0c178-b35c-4d61-ad64-0e103d358d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overlap_gt_def_boxes': [tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]])],\n",
       " 'db_for_each_obj': [tensor([0, 0, 0])],\n",
       " 'db_indices_for_each_obj': [tensor([0, 0, 0])],\n",
       " 'overlap_value_for_each_db': tensor([1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'self.label_each_db': [tensor([33., 36., 36., 36., 36., 36., 36., 36.])],\n",
       " 'match': [tensor([ True, False, False, False, False, False, False, False])],\n",
       " 'gt_locs': [tensor([[  19.0000, 1071.0000,    5.8158,   12.6685],\n",
       "          [  27.7500,  170.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  150.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  130.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  380.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  340.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  300.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  260.0000,    6.8686,   17.2021]]),\n",
       "  tensor([[  19.0000, 1071.0000,    5.8158,   12.6685],\n",
       "          [  27.7500,  170.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  150.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  130.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  380.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  340.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  300.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  260.0000,    6.8686,   17.2021]])],\n",
       " 'num_images': 1,\n",
       " 'loc_loss': tensor(276.6211),\n",
       " 'n_positive': tensor([1]),\n",
       " 'n_hard_negatives': tensor([3]),\n",
       " 'gt_label_each_default_box': tensor([33, 36, 36, 36, 36, 36, 36, 36]),\n",
       " 'conf_loss_for_each_default_box': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'confidence_pos_loss': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'hard_negatives': tensor([[ True,  True,  True, False, False, False, False, False]]),\n",
       " 'conf_neg_loss': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'ce_loss': tensor(14.5984),\n",
       " 'ce_hard_neg_loss': tensor(10.9488),\n",
       " 'ce_pos_loss': tensor(3.6496),\n",
       " 'loss': tensor(291.2195)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44dd71d-fcde-495d-bfda-a326f44eba56",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddccfc6-5820-4c8c-9e14-5262389d34ac",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
