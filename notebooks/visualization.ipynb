{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ee10b8-389a-4787-82c1-4277b7b7c559",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "934d956e-e7b0-4e6f-8a46-45fe77badfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "## other standard packages\n",
    "import os\n",
    "import sys\n",
    "## Env variables and preparation stuffs\n",
    "sys.path.insert(0, \"../\")\n",
    "from src_code.task_utils.config_parser import ConfigParser\n",
    "configs_dict = {\n",
    "    \"data_configs\": {\n",
    "        \"train_path\": \"datasets/utn_dataset_curated/part2/train\",\n",
    "        \"val_path\": \"datasets/utn_dataset_curated/part2/val\",\n",
    "        \"test_path\": \"datasets/utn_dataset_curated/part2/test\",\n",
    "        \"preprocessing_related\": {\n",
    "            \"mean\": 0.5,  # for raw_image normalisation\n",
    "            \"std\": 0.5,  # for raw_image normalisation\n",
    "            \"downscale_factor\": 4,\n",
    "        },\n",
    "        \"dataset_related\": {\n",
    "            \"preprocessed_dir\": \"../datasets/utn_dataset_curated/part2/train/preprocessed\",\n",
    "            \"labels_dir\": \"../datasets/utn_dataset_curated/part2/train/labels\",\n",
    "            \"augment\": True,\n",
    "            \"shuffle\": False,\n",
    "        },\n",
    "        \"augmentation_related\": {\n",
    "            \"flip_prob\": 0.5,\n",
    "            \"scale_range\": (0.8, 1.2),\n",
    "        },\n",
    "    },\n",
    "    \"model_configs\": {\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 32,\n",
    "        \"device\": \"cuda\",  # either \"cpu\" or \"cuda\"\n",
    "        \"checkpoint\": None,\n",
    "        \"backbone\": {\n",
    "            \"name\": \"VGG16\",\n",
    "            \"num_stages\": 6,\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"alpha\": 1,  # loss = alpha*loc_loss + cls_loss\n",
    "            \"pos_box_threshold\": 0.5,  # a default box is marked positive if it has (> pos_box_threshold) IoU score with any of the groundtruth boxes\n",
    "            \"hard_neg_pos\": 3,  # num of negative boxes = hard_neg_pos * num_positive_boxes\n",
    "        },\n",
    "        \"optim\": {\n",
    "            \"name\": \"SGD\",\n",
    "            \"lr\": 0.001,\n",
    "            \"momentum\": 0.9,\n",
    "            \"weight_decay\": 0.0005,\n",
    "        },\n",
    "        \"scheduler\": {\n",
    "            \"name\": \"MultiStepLR\",\n",
    "            \"milestones\": [155, 195],\n",
    "            \"gamma\": 0.1,\n",
    "        },\n",
    "    },\n",
    "    \"task_configs\": {\n",
    "        \"img_height\": 160,  # original image height\n",
    "        \"img_width\": 640,  # original image width\n",
    "        \"debug\": True,  # if True will display a lot of intermediate information for debugging purposes\n",
    "        \"log_expt\": False,  # whether to log the experiment online or not\n",
    "        \"num_classes\": 37,  # A-Z(26), 0-9(10), background(1)\n",
    "        \"min_cls_score\": 0.01,  # if the cls score for a bounding box is less than this, it is considered as background\n",
    "        \"nms_iou_score\": 0.1,  # if the iou between two bounding boxes is less than this, it is suppressed\n",
    "    },\n",
    "}\n",
    "# hyperparameters\n",
    "preprocessed_dir = \"../datasets/utn_dataset_curated/part2/train/preprocessed\"\n",
    "labels_dir = \"../datasets/utn_dataset_curated/part2/train/labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41000c80-0aee-4d58-9fad-11a4c2344aa8",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d1639a-4f80-4cbe-aa48-da4f30787063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2234bd15-102f-427f-92c7-76aa39894f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config = {'data_configs': {'train_path': 'datasets/utn_dataset_curated/part2/train', 'val_path': 'datasets/utn_dataset_curated/part2/val', 'test_path': 'datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/preprocessed', 'labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0.5, 'scale_range': (0.8, 1.2)}}, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'checkpoint': None, 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}\n",
      "{'config_dict': {'data_configs': {'train_path': 'datasets/utn_dataset_curated/part2/train', 'val_path': 'datasets/utn_dataset_curated/part2/val', 'test_path': 'datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/preprocessed', 'labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0.5, 'scale_range': (0.8, 1.2)}}, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'checkpoint': None, 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}, 'train_path': 'datasets/utn_dataset_curated/part2/train', 'downscale_factor': 4, 'preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/preprocessed', 'labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'augment': True, 'shuffle': False, 'flip_prob': 0.5, 'scale_range': (0.8, 1.2), 'device': 'cuda', 'print_freq': None, 'batch_size': 32, 'epochs': 1, 'pos_box_threshold': 0.5, 'neg_pos_hard_mining': 3, 'alpha': 1, 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005, 'debug': True, 'num_classes': 37, 'img_height': 160, 'img_width': 640}\n",
      "config.pos_box_threshold = 0.5\n"
     ]
    }
   ],
   "source": [
    "config = ConfigParser(configs_dict).get_parser()\n",
    "print(config.__dict__)  # Check all attributes in config\n",
    "\n",
    "# this object can be used as follows:\n",
    "print(f\"{config.pos_box_threshold = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0ffc95-8219-423d-89e4-a5fc4dbbe8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: change to relative import using a dot (.) in datautils line 4 & 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0cb7d-b2d9-466b-81d9-dec4f9ae9974",
   "metadata": {},
   "source": [
    "# 0. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded66662-0ef2-48a9-adda-6e2ace0d8c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\python311.zip', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\DLLs', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\Lib', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0', '', 'C:\\\\Users\\\\irene\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages', 'C:\\\\Users\\\\irene\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\win32', 'C:\\\\Users\\\\irene\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\irene\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\Pythonwin', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\Lib\\\\site-packages']\n",
      "Image Shape: torch.Size([1, 40, 160])\n",
      "Bounding Boxes: tensor([[ 0.1430,  0.4818,  0.2001,  0.8585],\n",
      "        [ 0.3307, -0.0112,  0.4611,  0.5103],\n",
      "        [ 0.4181,  0.2645,  0.5456,  0.7704],\n",
      "        [ 0.7742,  0.3091,  0.8777,  0.8153]])\n",
      "Labels: tensor([21,  3, 24,  4])\n",
      "Batch Image Shape: torch.Size([5, 1, 40, 160])\n",
      "Bounding Boxes (First Image): tensor([[ 0.7999,  0.4818,  0.8570,  0.8585],\n",
      "        [ 0.5389, -0.0112,  0.6693,  0.5103],\n",
      "        [ 0.4544,  0.2645,  0.5819,  0.7704],\n",
      "        [ 0.1223,  0.3091,  0.2258,  0.8153]])\n",
      "Labels (First Image): tensor([21,  3, 24,  4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\irene\\Documents\\UTN\\CV\\groupAssignment\\code\\UTN_Captcha_Detector\\notebooks\\..\\src_code\\data_utils\\dataset_utils.py:109: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  image = torch.load(preprocessed_path)\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)\n",
    "from src_code.data_utils.dataset_utils import CaptchaDataset, get_dataloader\n",
    "from src_code.data_utils.augmentation import Augmentations\n",
    "# if the preprocessed dataset is not available, run create it using src_code/data_utils/preprocessing.py\n",
    "\n",
    "# Create dataset\n",
    "dataset = CaptchaDataset(config)\n",
    "\n",
    "# Load a sample\n",
    "image, bboxes, labels = dataset[0]\n",
    "\n",
    "print(\"Image Shape:\", image.shape)\n",
    "print(\"Bounding Boxes:\", bboxes)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = get_dataloader(dataset, config)\n",
    "\n",
    "# Load a single batch\n",
    "images, bboxes, labels = next(iter(dataloader))\n",
    "\n",
    "# Print batch info\n",
    "print(f\"Batch Image Shape: {images.shape}\")\n",
    "print(f\"Bounding Boxes (First Image): {bboxes[0]}\")\n",
    "print(f\"Labels (First Image): {labels[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a528098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: torch.Size([1, 40, 160])\n",
      "Bounding Boxes: tensor([[ 0.7999,  0.4818,  0.8570,  0.8585],\n",
      "        [ 0.5389, -0.0112,  0.6693,  0.5103],\n",
      "        [ 0.4544,  0.2645,  0.5819,  0.7704],\n",
      "        [ 0.1223,  0.3091,  0.2258,  0.8153]])\n",
      "Labels: tensor([21,  3, 24,  4])\n",
      "BBoxes for Visualization: tensor([[127,  19, 137,  34],\n",
      "        [ 86,   0, 107,  20],\n",
      "        [ 72,  10,  93,  30],\n",
      "        [ 19,  12,  36,  32]], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# dataset = CaptchaDataset(preprocessed_dir, labels_dir, downscale_factor=False, augment=True)\n",
    "# Load one sample for visualization\n",
    "image, bboxes, labels = dataset[0]\n",
    "\n",
    "print(\"Image Shape:\", image.shape)\n",
    "print(\"Bounding Boxes:\", bboxes)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "\n",
    "def plot_image_with_bboxes(image, bboxes, labels, title=\"Image with Bounding Boxes\"):\n",
    "    img_height, img_width = image.shape[1], image.shape[2] \n",
    "    \n",
    "    # Scale normalized bboxes to absolute pixel values for visualization\n",
    "    # TODO: --> * 4 used for non flipped images: works\n",
    "    # Issue with flipped ones\n",
    "    # How to test: set flip prob to one and you will see :)\n",
    "    bboxes[:, [0, 2]] *= img_width\n",
    "    bboxes[:, [1, 3]] *= img_height\n",
    "\n",
    "    # Convert to integer values for plotting\n",
    "    bboxes_abs = bboxes.to(torch.int)\n",
    "\n",
    "    print(\"BBoxes for Visualization:\", bboxes_abs)\n",
    "\n",
    "    # Ensure labels are strings\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.tolist()\n",
    "    labels = [str(l) for l in labels]\n",
    "\n",
    "    # TODO: Image to RGB\n",
    "\n",
    "    # Draw bboxes\n",
    "    image_with_boxes = draw_bounding_boxes(image, bboxes_abs, labels=labels, colors=\"red\", width=2)\n",
    "\n",
    "    # image tensor to NumPy for visualization\n",
    "    img = image_with_boxes.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_image_with_bboxes(image, bboxes, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ac567-c663-4d04-a301-c4e2b1ff9f8b",
   "metadata": {},
   "source": [
    "# 1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8609000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irene\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\irene\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD(\n",
      "  (backbone): VGG16Backbone(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "    )\n",
      "    (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (auxiliary_convs): Sequential(\n",
      "    (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (loc_head): Conv2d(1024, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (cls_head): Conv2d(1024, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from src_code.model_utils.backbone import VGG16Backbone\n",
    "from src_code.model_utils.ssd import SSD\n",
    "\n",
    "# Initialize the SSD model\n",
    "model = SSD(num_classes=config.num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d19ab-526e-4e66-a4f7-7d0eb73c92fb",
   "metadata": {},
   "source": [
    "## 1.1 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab12065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16Backbone(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (5): ReLU()\n",
      "  (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (7): ReLU()\n",
      ")\n",
      "Conv2d(1024, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(1024, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "print(model.backbone)\n",
    "print(model.auxiliary_convs)\n",
    "print(model.loc_head)\n",
    "print(model.cls_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d6389-fe7a-4df7-82bd-9e98ead66501",
   "metadata": {},
   "source": [
    "## 1.2 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f044187b-ba69-4376-b2a3-3caf57388979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irene\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "from src_code.model_utils.loss import MultiBoxLoss\n",
    "default_boxes = torch.Tensor([[0.5000, 0.1250, 0.5000, 0.1250],[0.5000, 0.3750, 0.5000, 0.1250],\n",
    "        [0.5000, 0.6250, 0.5000, 0.1250],[0.5000, 0.8750, 0.5000, 0.1250],\n",
    "        [0.5000, 0.1250, 0.5000, 0.0625],[0.5000, 0.3750, 0.5000, 0.0625],\n",
    "        [0.5000, 0.6250, 0.5000, 0.0625],[0.5000, 0.8750, 0.5000, 0.0625]])\n",
    "# (1, 8, 4)\n",
    "locs_pred = torch.Tensor([[[0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.]]])\n",
    "\n",
    "# (1, 8, 36)\n",
    "cls_pred = torch.Tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
    "boxes = torch.Tensor([[[ 61,  36, 139, 115], [297,  10, 386,  98], [509,  26, 572,  90]]])\n",
    "labels = torch.Tensor([[21,  0, 33]])\n",
    "\n",
    "# calculate loss\n",
    "mbl = MultiBoxLoss(default_boxes, config)\n",
    "loss, debug_info = mbl(locs_pred, cls_pred, boxes, labels)\n",
    "expected_loss = 10.8635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29716e21-60a9-4382-b9ae-8fbab55fe563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(291.2195)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bd0c178-b35c-4d61-ad64-0e103d358d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overlap_gt_def_boxes': [tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]])],\n",
       " 'db_for_each_obj': [tensor([0, 0, 0])],\n",
       " 'db_indices_for_each_obj': [tensor([0, 0, 0])],\n",
       " 'overlap_value_for_each_db': tensor([1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'self.label_each_db': [tensor([33., 36., 36., 36., 36., 36., 36., 36.])],\n",
       " 'match': [tensor([ True, False, False, False, False, False, False, False])],\n",
       " 'gt_locs': [tensor([[  19.0000, 1071.0000,    5.8158,   12.6685],\n",
       "          [  27.7500,  170.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  150.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  130.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  380.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  340.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  300.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  260.0000,    6.8686,   17.2021]]),\n",
       "  tensor([[  19.0000, 1071.0000,    5.8158,   12.6685],\n",
       "          [  27.7500,  170.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  150.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  130.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  380.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  340.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  300.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  260.0000,    6.8686,   17.2021]])],\n",
       " 'num_images': 1,\n",
       " 'loc_loss': tensor(276.6211),\n",
       " 'n_positive': tensor([1]),\n",
       " 'n_hard_negatives': tensor([3]),\n",
       " 'gt_label_each_default_box': tensor([33, 36, 36, 36, 36, 36, 36, 36]),\n",
       " 'conf_loss_for_each_default_box': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'confidence_pos_loss': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'hard_negatives': tensor([[ True,  True,  True, False, False, False, False, False]]),\n",
       " 'conf_neg_loss': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'ce_loss': tensor(14.5984),\n",
       " 'ce_hard_neg_loss': tensor(10.9488),\n",
       " 'ce_pos_loss': tensor(3.6496),\n",
       " 'loss': tensor(291.2195)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44dd71d-fcde-495d-bfda-a326f44eba56",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f831262d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config = {'data_configs': {'train_path': 'datasets/utn_dataset_curated/part2/train', 'val_path': 'datasets/utn_dataset_curated/part2/val', 'test_path': 'datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/preprocessed', 'labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0.5, 'scale_range': (0.8, 1.2)}}, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'checkpoint': None, 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}\n",
      "Using device: cpu\n",
      "Default boxes shape: torch.Size([8, 4])\n",
      "Number of batches in train_loader: 1\n",
      "config attributes: ['_ConfigParser__verify__argparse', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'alpha', 'augment', 'batch_size', 'config_dict', 'debug', 'device', 'downscale_factor', 'epochs', 'flip_prob', 'get_config', 'get_parser', 'img_height', 'img_width', 'labels_dir', 'lr', 'momentum', 'neg_pos_hard_mining', 'num_classes', 'pos_box_threshold', 'preprocessed_dir', 'print_freq', 'scale_range', 'shuffle', 'train_path', 'weight_decay']\n",
      "config.pos_box_threshold: 0.5\n",
      "Epoch 1/1:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [5, 8] at index 1 does not match the shape of the indexed tensor [5, 80, 4] at index 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\irene\\Documents\\UTN\\CV\\groupAssignment\\code\\UTN_Captcha_Detector\\notebooks\\..\\src_code\\model_utils\\train_utils.py:65\u001b[0m, in \u001b[0;36mCaptchaTrainer.train_step\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     63\u001b[0m loc_pred, cls_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(images)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m loss, debug_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     68\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\irene\\Documents\\UTN\\CV\\groupAssignment\\code\\UTN_Captcha_Detector\\notebooks\\..\\src_code\\model_utils\\loss.py:144\u001b[0m, in \u001b[0;36mMultiBoxLoss.forward\u001b[1;34m(self, locs_pred, cls_pred, boxes, labels, downscale_factor)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Localization loss is computed only over positive default boxes\u001b[39;00m\n\u001b[0;32m    143\u001b[0m smooth_L1_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSmoothL1Loss()\n\u001b[1;32m--> 144\u001b[0m loc_loss \u001b[38;5;241m=\u001b[39m smooth_L1_loss(\u001b[43mlocs_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos_db\u001b[49m\u001b[43m]\u001b[49m, gt_locs[pos_db])\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m    146\u001b[0m     debug_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m loc_loss\n",
      "\u001b[1;31mIndexError\u001b[0m: The shape of the mask [5, 8] at index 1 does not match the shape of the indexed tensor [5, 80, 4] at index 1"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from src_code.model_utils.train_utils import CaptchaTrainer\n",
    "\n",
    "config = ConfigParser(configs_dict)\n",
    "config = config.get_parser()\n",
    "\n",
    "# Automatically detect if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ensure `config.device` is updated correctly\n",
    "config.device = device\n",
    "\n",
    "# Move model to the correct device\n",
    "model.to(config.device)\n",
    "print(f\"Using device: {config.device}\")\n",
    "\n",
    "# Ensure default boxes are defined\n",
    "print(f\"Default boxes shape: {default_boxes.shape}\")  # Check before passing\n",
    "\n",
    "# Ensure dataloader has data\n",
    "assert len(dataloader) > 0, \"Error: Training dataloader is empty!\"\n",
    "print(f\"Number of batches in train_loader: {len(dataloader)}\")\n",
    "\n",
    "print(f\"config attributes: {dir(config)}\")  # Check attributes exist\n",
    "print(f\"config.pos_box_threshold: {config.pos_box_threshold}\")  # Ensure it's defined\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = SGD(\n",
    "    model.parameters(),\n",
    "    lr=config.lr,  # Use object attribute instead of dictionary key\n",
    "    momentum=config.momentum,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = MultiBoxLoss(default_boxes, config)\n",
    "\n",
    "# Create trainer\n",
    "trainer = CaptchaTrainer(\n",
    "    model=model,\n",
    "    train_loader=dataloader,\n",
    "    val_loader=None,  # You can set a validation loader if needed\n",
    "    test_loader=None,  # You can set a test loader if needed\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "num_epochs = config.epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    trainer.train_step(epoch)\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddccfc6-5820-4c8c-9e14-5262389d34ac",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
