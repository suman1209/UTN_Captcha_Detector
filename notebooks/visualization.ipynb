{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ee10b8-389a-4787-82c1-4277b7b7c559",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "934d956e-e7b0-4e6f-8a46-45fe77badfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "## other standard packages\n",
    "import os\n",
    "import sys\n",
    "## Env variables and preparation stuffs\n",
    "sys.path.insert(0, \"../\")\n",
    "from src_code.task_utils.config_parser import ConfigParser\n",
    "configs_dict = {\n",
    "    \"data_configs\": {\n",
    "        \"train_path\": \"datasets/utn_dataset_curated/part2/train\",\n",
    "        \"val_path\": \"datasets/utn_dataset_curated/part2/val\",\n",
    "        \"test_path\": \"datasets/utn_dataset_curated/part2/test\",\n",
    "        \"preprocessing_related\": {\n",
    "            \"mean\": 0.5,  # for raw_image normalisation\n",
    "            \"std\": 0.5,  # for raw_image normalisation\n",
    "            \"downscale_factor\": 4,\n",
    "        },\n",
    "        \"dataset_related\": {\n",
    "            \"preprocessed_dir\": \"../datasets/utn_dataset_curated/part2/train/preprocessed\",\n",
    "            \"labels_dir\": \"../datasets/utn_dataset_curated/part2/train/labels\",\n",
    "            \"augment\": True,\n",
    "            \"shuffle\": False,\n",
    "        },\n",
    "        \"augmentation_related\": {\n",
    "            \"flip_prob\": 0,\n",
    "            \"scale_range\": (0.8, 1.2),\n",
    "            \"zoom_prob\": 0,\n",
    "            \"saturation_prob\": 0,\n",
    "            \"rotation_prob\": 1\n",
    "        },\n",
    "    },\n",
    "    \"model_configs\": {\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 32,\n",
    "        \"device\": \"cuda\",  # either \"cpu\" or \"cuda\"\n",
    "        \"checkpoint\": None,\n",
    "        \"backbone\": {\n",
    "            \"name\": \"VGG16\",\n",
    "            \"num_stages\": 6,\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"alpha\": 1,  # loss = alpha*loc_loss + cls_loss\n",
    "            \"pos_box_threshold\": 0.5,  # a default box is marked positive if it has (> pos_box_threshold) IoU score with any of the groundtruth boxes\n",
    "            \"hard_neg_pos\": 3,  # num of negative boxes = hard_neg_pos * num_positive_boxes\n",
    "        },\n",
    "        \"optim\": {\n",
    "            \"name\": \"SGD\",\n",
    "            \"lr\": 0.001,\n",
    "            \"momentum\": 0.9,\n",
    "            \"weight_decay\": 0.0005,\n",
    "        },\n",
    "        \"scheduler\": {\n",
    "            \"name\": \"MultiStepLR\",\n",
    "            \"milestones\": [155, 195],\n",
    "            \"gamma\": 0.1,\n",
    "        },\n",
    "    },\n",
    "    \"task_configs\": {\n",
    "        \"img_height\": 160,  # original image height\n",
    "        \"img_width\": 640,  # original image width\n",
    "        \"debug\": True,  # if True will display a lot of intermediate information for debugging purposes\n",
    "        \"log_expt\": False,  # whether to log the experiment online or not\n",
    "        \"num_classes\": 37,  # A-Z(26), 0-9(10), background(1)\n",
    "        \"min_cls_score\": 0.01,  # if the cls score for a bounding box is less than this, it is considered as background\n",
    "        \"nms_iou_score\": 0.1,  # if the iou between two bounding boxes is less than this, it is suppressed\n",
    "    },\n",
    "}\n",
    "# hyperparameters\n",
    "preprocessed_dir = \"../datasets/utn_dataset_curated/part2/train/preprocessed\"\n",
    "labels_dir = \"../datasets/utn_dataset_curated/part2/train/labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41000c80-0aee-4d58-9fad-11a4c2344aa8",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16d1639a-4f80-4cbe-aa48-da4f30787063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2234bd15-102f-427f-92c7-76aa39894f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config = {'data_configs': {'train_path': 'datasets/utn_dataset_curated/part2/train', 'val_path': 'datasets/utn_dataset_curated/part2/val', 'test_path': 'datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/preprocessed', 'labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0, 'scale_range': (0.8, 1.2), 'zoom_prob': 0, 'saturation_prob': 0, 'rotation_prob': 1}}, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'checkpoint': None, 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}\n",
      "{'config_dict': {'data_configs': {'train_path': 'datasets/utn_dataset_curated/part2/train', 'val_path': 'datasets/utn_dataset_curated/part2/val', 'test_path': 'datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/preprocessed', 'labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0, 'scale_range': (0.8, 1.2), 'zoom_prob': 0, 'saturation_prob': 0, 'rotation_prob': 1}}, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'checkpoint': None, 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}, 'train_path': 'datasets/utn_dataset_curated/part2/train', 'downscale_factor': 4, 'train_preprocessed_dir': None, 'val_preprocessed_dir': None, 'test_preprocessed_dir': None, 'train_labels_dir': None, 'val_labels_dir': None, 'augment': True, 'shuffle': False, 'flip_prob': 0, 'scale_range': (0.8, 1.2), 'zoom_prob': 0, 'saturation_prob': 0, 'rotation_prob': 1, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'checkpoint': None, 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'checkpoint': None, 'device': 'cuda', 'print_freq': None, 'batch_size': 32, 'epochs': 1, 'scheduler_name': 'MultiStepLR', 'multistep_milestones': [155, 195], 'multistep_gamma': 0.1, 'linearLR_start_factor': None, 'linearLR_total_iter': None, 'pos_box_threshold': 0.5, 'neg_pos_hard_mining': 3, 'alpha': 1, 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005, 'debug': True, 'num_classes': 37, 'img_height': 160, 'img_width': 640}\n",
      "config.pos_box_threshold = 0.5\n"
     ]
    }
   ],
   "source": [
    "config = ConfigParser(configs_dict).get_parser()\n",
    "print(config.__dict__)  # Check all attributes in config\n",
    "\n",
    "# this object can be used as follows:\n",
    "print(f\"{config.pos_box_threshold = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0ffc95-8219-423d-89e4-a5fc4dbbe8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: change to relative import using a dot (.) in datautils line 4 & 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0cb7d-b2d9-466b-81d9-dec4f9ae9974",
   "metadata": {},
   "source": [
    "# 0. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ded66662-0ef2-48a9-adda-6e2ace0d8c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../', '../', '../', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python311.zip', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/lib-dynload', '', '/Users/lucaheller/Desktop/UTN_Captcha_Detector/.venv/lib/python3.11/site-packages', '/var/folders/ph/4ppm8f8950g15_ff9t0m53k80000gn/T/tmpkuvddkb5']\n",
      "Image Shape: torch.Size([1, 40, 160])\n",
      "Bounding Boxes: tensor([[0.1443, 0.5975, 0.2102, 0.9945],\n",
      "        [0.3194, 0.0173, 0.4615, 0.5756],\n",
      "        [0.4128, 0.2480, 0.5519, 0.8007],\n",
      "        [0.7683, 0.1643, 0.8836, 0.7081]])\n",
      "Labels: tensor([21,  3, 24,  4])\n",
      "Batch Image Shape: torch.Size([32, 1, 40, 160])\n",
      "Bounding Boxes (First Image): tensor([[0.1327, 0.2197, 0.2061, 0.6319],\n",
      "        [0.3331, 0.0000, 0.4847, 0.4815],\n",
      "        [0.4071, 0.2083, 0.5557, 0.7993],\n",
      "        [0.7551, 0.5139, 0.8801, 1.0000]])\n",
      "Labels (First Image): tensor([21,  3, 24,  4])\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)\n",
    "from src_code.data_utils.dataset_utils import CaptchaDataset, get_dataloader\n",
    "from src_code.data_utils.augmentation import Augmentations\n",
    "# if the preprocessed dataset is not available, run create it using src_code/data_utils/preprocessing.py\n",
    "\n",
    "# Create dataset\n",
    "dataset = CaptchaDataset(preprocessed_dir, labels_dir, augment=True, config=config)\n",
    "\n",
    "# Load a sample\n",
    "image, bboxes, labels = dataset[0]\n",
    "\n",
    "print(\"Image Shape:\", image.shape)\n",
    "print(\"Bounding Boxes:\", bboxes)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = get_dataloader(dataset, config)\n",
    "\n",
    "# Load a single batch\n",
    "images, bboxes, labels = next(iter(dataloader))\n",
    "\n",
    "# Print batch info\n",
    "print(f\"Batch Image Shape: {images.shape}\")\n",
    "print(f\"Bounding Boxes (First Image): {bboxes[0]}\")\n",
    "print(f\"Labels (First Image): {labels[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a528098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: torch.Size([1, 40, 160])\n",
      "Bounding Boxes: tensor([[0.1330, 0.0309, 0.2170, 0.4605],\n",
      "        [0.3386, 0.0000, 0.5026, 0.4605],\n",
      "        [0.4009, 0.1730, 0.5619, 0.8143]])\n",
      "Labels: tensor([21,  3, 24])\n",
      "BBoxes for Visualization: tensor([[21,  1, 34, 18],\n",
      "        [54,  0, 80, 18],\n",
      "        [64,  6, 89, 32]], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# dataset = CaptchaDataset(preprocessed_dir, labels_dir, downscale_factor=False, augment=True)\n",
    "# Load one sample for visualization\n",
    "image, bboxes, labels = dataset[0]\n",
    "\n",
    "print(\"Image Shape:\", image.shape)\n",
    "print(\"Bounding Boxes:\", bboxes)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "\n",
    "def plot_image_with_bboxes(image, bboxes, labels, title=\"Image with Bounding Boxes\"):\n",
    "    img_height, img_width = image.shape[-2], image.shape[-1] \n",
    "    \n",
    "    # Scale normalized bboxes to absolute pixel values for visualization\n",
    "    bboxes[:, [0, 2]] *= img_width\n",
    "    bboxes[:, [1, 3]] *= img_height\n",
    "\n",
    "    # Convert to integer values for plotting\n",
    "    bboxes_abs = bboxes.to(torch.int)\n",
    "\n",
    "    print(\"BBoxes for Visualization:\", bboxes_abs)\n",
    "\n",
    "    # Ensure labels are strings\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.tolist()\n",
    "    labels = [str(l) for l in labels]\n",
    "\n",
    "    # Draw bboxes\n",
    "    image_with_boxes = draw_bounding_boxes(image, bboxes_abs, labels=labels, colors=\"red\", width=2)\n",
    "\n",
    "    # image tensor to NumPy for visualization\n",
    "    img = image_with_boxes.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_image_with_bboxes(image, bboxes, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ac567-c663-4d04-a301-c4e2b1ff9f8b",
   "metadata": {},
   "source": [
    "# 1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d8609000",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SSD' from 'src_code.model_utils.ssd' (/Users/lucaheller/Desktop/UTN_Captcha_Detector/notebooks/../src_code/model_utils/ssd.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc_code\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VGG16Backbone\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc_code\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mssd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SSD\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the SSD model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m SSD(num_classes\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_classes)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SSD' from 'src_code.model_utils.ssd' (/Users/lucaheller/Desktop/UTN_Captcha_Detector/notebooks/../src_code/model_utils/ssd.py)"
     ]
    }
   ],
   "source": [
    "from src_code.model_utils.backbone import VGG16Backbone\n",
    "from src_code.model_utils.ssd import SSD\n",
    "\n",
    "# Initialize the SSD model\n",
    "model = SSD(num_classes=config.num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d19ab-526e-4e66-a4f7-7d0eb73c92fb",
   "metadata": {},
   "source": [
    "## 1.1 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab12065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16Backbone(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (5): ReLU()\n",
      "  (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (7): ReLU()\n",
      ")\n",
      "Conv2d(1024, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(1024, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "print(model.backbone)\n",
    "print(model.auxiliary_convs)\n",
    "print(model.loc_head)\n",
    "print(model.cls_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d6389-fe7a-4df7-82bd-9e98ead66501",
   "metadata": {},
   "source": [
    "## 1.2 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f044187b-ba69-4376-b2a3-3caf57388979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irene\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "from src_code.model_utils.loss import MultiBoxLoss\n",
    "default_boxes = torch.Tensor([[0.5000, 0.1250, 0.5000, 0.1250],[0.5000, 0.3750, 0.5000, 0.1250],\n",
    "        [0.5000, 0.6250, 0.5000, 0.1250],[0.5000, 0.8750, 0.5000, 0.1250],\n",
    "        [0.5000, 0.1250, 0.5000, 0.0625],[0.5000, 0.3750, 0.5000, 0.0625],\n",
    "        [0.5000, 0.6250, 0.5000, 0.0625],[0.5000, 0.8750, 0.5000, 0.0625]])\n",
    "# (1, 8, 4)\n",
    "locs_pred = torch.Tensor([[[0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.],\n",
    "                     [0., 0., 0., 0.]]])\n",
    "\n",
    "# (1, 8, 36)\n",
    "cls_pred = torch.Tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "     [0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "      0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
    "boxes = torch.Tensor([[[ 61,  36, 139, 115], [297,  10, 386,  98], [509,  26, 572,  90]]])\n",
    "labels = torch.Tensor([[21,  0, 33]])\n",
    "\n",
    "# calculate loss\n",
    "mbl = MultiBoxLoss(default_boxes, config)\n",
    "loss, debug_info = mbl(locs_pred, cls_pred, boxes, labels)\n",
    "expected_loss = 10.8635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29716e21-60a9-4382-b9ae-8fbab55fe563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(291.2195)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bd0c178-b35c-4d61-ad64-0e103d358d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overlap_gt_def_boxes': [tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]])],\n",
       " 'db_for_each_obj': [tensor([0, 0, 0])],\n",
       " 'db_indices_for_each_obj': [tensor([0, 0, 0])],\n",
       " 'overlap_value_for_each_db': tensor([1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'self.label_each_db': [tensor([33., 36., 36., 36., 36., 36., 36., 36.])],\n",
       " 'match': [tensor([ True, False, False, False, False, False, False, False])],\n",
       " 'gt_locs': [tensor([[  19.0000, 1071.0000,    5.8158,   12.6685],\n",
       "          [  27.7500,  170.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  150.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  130.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  380.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  340.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  300.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  260.0000,    6.8686,   17.2021]]),\n",
       "  tensor([[  19.0000, 1071.0000,    5.8158,   12.6685],\n",
       "          [  27.7500,  170.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  150.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  130.0000,    6.8686,   13.7364],\n",
       "          [  27.7500,  380.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  340.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  300.0000,    6.8686,   17.2021],\n",
       "          [  27.7500,  260.0000,    6.8686,   17.2021]])],\n",
       " 'num_images': 1,\n",
       " 'loc_loss': tensor(276.6211),\n",
       " 'n_positive': tensor([1]),\n",
       " 'n_hard_negatives': tensor([3]),\n",
       " 'gt_label_each_default_box': tensor([33, 36, 36, 36, 36, 36, 36, 36]),\n",
       " 'conf_loss_for_each_default_box': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'confidence_pos_loss': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'hard_negatives': tensor([[ True,  True,  True, False, False, False, False, False]]),\n",
       " 'conf_neg_loss': tensor([[3.6496, 3.6109, 3.6496, 3.6496, 3.6496, 3.6109, 3.6496, 3.6496]]),\n",
       " 'ce_loss': tensor(14.5984),\n",
       " 'ce_hard_neg_loss': tensor(10.9488),\n",
       " 'ce_pos_loss': tensor(3.6496),\n",
       " 'loss': tensor(291.2195)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44dd71d-fcde-495d-bfda-a326f44eba56",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f831262d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config = {'data_configs': {'train_path': 'datasets/utn_dataset_curated/part2/train', 'val_path': 'datasets/utn_dataset_curated/part2/val', 'test_path': 'datasets/utn_dataset_curated/part2/test', 'preprocessing_related': {'mean': 0.5, 'std': 0.5, 'downscale_factor': 4}, 'dataset_related': {'preprocessed_dir': '../datasets/utn_dataset_curated/part2/train/preprocessed', 'labels_dir': '../datasets/utn_dataset_curated/part2/train/labels', 'augment': True, 'shuffle': False}, 'augmentation_related': {'flip_prob': 0.5, 'scale_range': (0.8, 1.2)}}, 'model_configs': {'epochs': 1, 'batch_size': 32, 'device': 'cuda', 'checkpoint': None, 'backbone': {'name': 'VGG16', 'num_stages': 6}, 'loss': {'alpha': 1, 'pos_box_threshold': 0.5, 'hard_neg_pos': 3}, 'optim': {'name': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler': {'name': 'MultiStepLR', 'milestones': [155, 195], 'gamma': 0.1}}, 'task_configs': {'img_height': 160, 'img_width': 640, 'debug': True, 'log_expt': False, 'num_classes': 37, 'min_cls_score': 0.01, 'nms_iou_score': 0.1}}\n",
      "Using device: cpu\n",
      "Default boxes shape: torch.Size([8, 4])\n",
      "Number of batches in train_loader: 1\n",
      "config attributes: ['_ConfigParser__verify__argparse', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'alpha', 'augment', 'batch_size', 'config_dict', 'debug', 'device', 'downscale_factor', 'epochs', 'flip_prob', 'get_config', 'get_parser', 'img_height', 'img_width', 'labels_dir', 'lr', 'momentum', 'neg_pos_hard_mining', 'num_classes', 'pos_box_threshold', 'preprocessed_dir', 'print_freq', 'scale_range', 'shuffle', 'train_path', 'weight_decay']\n",
      "config.pos_box_threshold: 0.5\n",
      "Epoch 1/1:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [5, 8] at index 1 does not match the shape of the indexed tensor [5, 80, 4] at index 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\irene\\Documents\\UTN\\CV\\groupAssignment\\code\\UTN_Captcha_Detector\\notebooks\\..\\src_code\\model_utils\\train_utils.py:65\u001b[0m, in \u001b[0;36mCaptchaTrainer.train_step\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     63\u001b[0m loc_pred, cls_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(images)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m loss, debug_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     68\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\irene\\Documents\\UTN\\CV\\groupAssignment\\code\\UTN_Captcha_Detector\\notebooks\\..\\src_code\\model_utils\\loss.py:144\u001b[0m, in \u001b[0;36mMultiBoxLoss.forward\u001b[1;34m(self, locs_pred, cls_pred, boxes, labels, downscale_factor)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Localization loss is computed only over positive default boxes\u001b[39;00m\n\u001b[0;32m    143\u001b[0m smooth_L1_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSmoothL1Loss()\n\u001b[1;32m--> 144\u001b[0m loc_loss \u001b[38;5;241m=\u001b[39m smooth_L1_loss(\u001b[43mlocs_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos_db\u001b[49m\u001b[43m]\u001b[49m, gt_locs[pos_db])\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m    146\u001b[0m     debug_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m loc_loss\n",
      "\u001b[1;31mIndexError\u001b[0m: The shape of the mask [5, 8] at index 1 does not match the shape of the indexed tensor [5, 80, 4] at index 1"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from src_code.model_utils.train_utils import CaptchaTrainer\n",
    "\n",
    "config = ConfigParser(configs_dict)\n",
    "config = config.get_parser()\n",
    "\n",
    "# Automatically detect if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ensure `config.device` is updated correctly\n",
    "config.device = device\n",
    "\n",
    "# Move model to the correct device\n",
    "model.to(config.device)\n",
    "print(f\"Using device: {config.device}\")\n",
    "\n",
    "# Ensure default boxes are defined\n",
    "print(f\"Default boxes shape: {default_boxes.shape}\")  # Check before passing\n",
    "\n",
    "# Ensure dataloader has data\n",
    "assert len(dataloader) > 0, \"Error: Training dataloader is empty!\"\n",
    "print(f\"Number of batches in train_loader: {len(dataloader)}\")\n",
    "\n",
    "print(f\"config attributes: {dir(config)}\")  # Check attributes exist\n",
    "print(f\"config.pos_box_threshold: {config.pos_box_threshold}\")  # Ensure it's defined\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = SGD(\n",
    "    model.parameters(),\n",
    "    lr=config.lr,  # Use object attribute instead of dictionary key\n",
    "    momentum=config.momentum,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = MultiBoxLoss(default_boxes, config)\n",
    "\n",
    "# Create trainer\n",
    "trainer = CaptchaTrainer(\n",
    "    model=model,\n",
    "    train_loader=dataloader,\n",
    "    val_loader=None,  # You can set a validation loader if needed\n",
    "    test_loader=None,  # You can set a test loader if needed\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "num_epochs = config.epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    trainer.train_step(epoch)\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddccfc6-5820-4c8c-9e14-5262389d34ac",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
